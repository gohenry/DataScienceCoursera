{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArtAuction_Nov3_7pm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gohenry/DataScienceCoursera/blob/master/ArtAuction_Nov3_3pm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQehzGIK_d1T",
        "colab_type": "code",
        "outputId": "f84de01b-ef8e-4229-b6f4-b62079261bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount = True)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzosBaOfNRIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd drive/My Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3bFX7bd_jIk",
        "colab_type": "code",
        "outputId": "73bdce4c-cb9d-4a81-e877-31c990b3f237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "#import the necessary packages\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxpxKtnT_wzt",
        "colab_type": "code",
        "outputId": "cabbcdd4-a072-4e75-ec42-69d4308bdeea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#load numerical data using Pandas\n",
        "!pip install -q xlrd\n",
        "\n",
        "cols = ['num_author','creation_year','height','width','estimate_low','estimate_high','auction_year','artist_birth','hammer price']\n",
        "df = pd.read_excel('result_2017_2018.xlsx', usecols=cols)\n",
        "df\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_author</th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height</th>\n",
              "      <th>width</th>\n",
              "      <th>estimate_low</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>hammer price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1983.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>80669.0</td>\n",
              "      <td>141170.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>161338.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>92678.0</td>\n",
              "      <td>139017.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1984.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>91964.0</td>\n",
              "      <td>118239.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>128750.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>131377.0</td>\n",
              "      <td>197066.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>105102.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>131378.0</td>\n",
              "      <td>197067.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>100</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>2117691.0</td>\n",
              "      <td>2911825.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>2183869.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>100</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1800000.0</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>1900000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>100</td>\n",
              "      <td>1986.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>3500000.0</td>\n",
              "      <td>4500000.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>4343000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>100</td>\n",
              "      <td>1989.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>3000000.0</td>\n",
              "      <td>5000000.0</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>3300000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>100</td>\n",
              "      <td>1978.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>1500000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1458 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      num_author  creation_year  ...  artist_birth  hammer price\n",
              "0              1         1983.0  ...        1928.0      161338.0\n",
              "1              1         1982.0  ...        1928.0           NaN\n",
              "2              1         1984.0  ...        1928.0      128750.0\n",
              "3              1         1982.0  ...        1928.0      105102.0\n",
              "4              1         1982.0  ...        1928.0           NaN\n",
              "...          ...            ...  ...           ...           ...\n",
              "1453         100         1999.0  ...        1923.0     2183869.0\n",
              "1454         100         2012.0  ...        1923.0     1900000.0\n",
              "1455         100         1986.0  ...        1923.0     4343000.0\n",
              "1456         100         1989.0  ...        1923.0     3300000.0\n",
              "1457         100         1978.0  ...        1923.0     1500000.0\n",
              "\n",
              "[1458 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cRDwRTDeeQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop rows containing missing data\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRER55fhXrt_",
        "colab_type": "code",
        "outputId": "7bc60fa3-ce2e-4907-f8bc-b34a7f4546ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "#check distribution of hammer price\n",
        "df.hist(column='hammer price', bins=100)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fc84a8d3908>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAZHElEQVR4nO3df3Bd5X3n8fcHCD9iEcv8WK3XVhEb\nPGS6uDFYg92lk0q4yZofG3tnCAPjBoe6VTNLKVnSFqf7R9ppM3XacR1wszRqTNdsXQQhYewawuIa\nNF12ayc2cWzASRGOKVYcq+AfIHDShX77x31MLkKyrq7u1fV9zuc1c0fnPM9zzn2+Aj46PLo6RxGB\nmZnl5bRGT8DMzGrP4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHuzWMpP2SfqnR8zgVSFom6YlG\nz8Py4XA3OwVExIaI+Fij52H5cLibTYKkM06Fc5iN5HC3RpsnabekY5IelHQ2gKQZkjZL+idJR9L2\n7BMHSeqX9IeS/p+kYUl/I+l8SRskvSbp25I6ysaHpP8q6QVJr0v6A0kfTMe/JukhSWeWjb9e0i5J\nR9OYnyvr2y/pLkm7gTdGC+f0fr8paZ+kVyT9iaTTUt+nJP1fSWskvQr8Xmp7uuz4/yBpi6TDkg5J\n+t3UfpqklZJelPRqmvd5tfwHYnlwuFuj3QgsBi4Gfg74VGo/DfhL4CLgZ4DjwJ+NOPYm4JPALOCD\nwN+nY84D9gKfHzH+PwHzgYXA7wC9wC8D7cBlwM0Aki4H7gN+HTgf+AqwSdJZZee6GbgOaI2It8ao\n7b8AncAVwBLgV8r6FgD7gDbgC+UHSToX+FvgceDfAZcAW1P37cBS4BdT3xHgy2O8vxWYw90a7Z6I\n+GFEHAb+BpgHEBGvRsTXI+LNiHidUgD+4ohj/zIiXoyIY8A3gRcj4m9T2H4NuHzE+D+OiNci4jng\nWeCJiNhXdvyJ8T3AVyJie0S8HRHrgZ9Q+qFQPu+XI+L4SWr7YkQcjoh/BL5E+uGR/DAi1kbEW6Oc\n43rgRxGxOiJ+HBGvR8T21Pdp4L9HxIGI+Anwe8ANXtqxkfwvhDXaj8q236R0NYqk9wNrKF3Vz0j9\n50o6PSLeTvuHyo49Psp+y4j3Gm/8v03bFwHLJd1e1n/mibklL5+kptHGvDSB49uBF8fouwh4RNK/\nlLW9Ten/AAYrmJMVhK/c7VT1WeBSYEFEfAD4SGrXFLz3y8AXIqK17PX+iHigbEwlt1NtL9v+GeCH\nFR7/MvDvT9J3zYi5nR0RDnZ7F4e7narOpXQ1fTT9wnDk+nk9/QXwaUkLVDJN0nVpLXwifjv9Yrgd\nuAN4sMLjNgMzJX1G0lmSzpW0IPX9OfAFSRcBSLpQ0pIJzssKwOFup6ovAecArwDbKP1ycUpExA7g\n1yj9AvcIMMBPf9E7ERuBncAu4FFgXYXv/zrwUeA/U1q2egHoTt13A5uAJyS9Tul7s2C081ixyQ/r\nMKs9SQHMiYiBRs/FislX7mZmGXK4m5llyMsyZmYZ8pW7mVmGTok/Yrrggguio6OjqmPfeOMNpk2b\nVtsJncKKVG+RaoVi1VukWqF+9e7cufOViLhwtL5TItw7OjrYsWNHVcf29/fT1dVV2wmdwopUb5Fq\nhWLVW6RaoX71SnpprD4vy5iZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhly\nuJuZZajpw33P4DE6Vj5Kx8pHGz0VM7NTRtOHu5mZvZfD3cwsQxWFu6T/Juk5Sc9KekDS2ZIulrRd\n0oCkByWdmcaelfYHUn9HPQswM7P3GjfcJc0CfhPojIjLgNOBm4AvAmsi4hJKDxFekQ5ZARxJ7WvS\nODMzm0KVLsucAZwj6Qzg/cBB4Grg4dS/HliatpekfVL/IkmqzXTNzKwSFT1mT9IdwBeA48ATwB3A\ntnR1jqR24JsRcZmkZ4HFEXEg9b0ILIiIV0acswfoAWhra5vf19dXVQFDh49x6Hhpe+6s6VWdo5kM\nDw/T0tLS6GlMiSLVCsWqt0i1Qv3q7e7u3hkRnaP1jfuwDkkzKF2NXwwcBb4GLJ7spCKiF+gF6Ozs\njGpvZL92w0ZW7ymVsX9ZdedoJkV6yEGRaoVi1VukWqEx9VayLPNLwA8i4p8i4v8D3wCuAlrTMg3A\nbGAwbQ8C7QCpfzrwak1nbWZmJ1VJuP8jsFDS+9Pa+SLgeeAp4IY0ZjmwMW1vSvuk/iejkrUfMzOr\nmXHDPSK2U/rF6DPAnnRML3AXcKekAeB8YF06ZB1wfmq/E1hZh3mbmdlJVPSA7Ij4PPD5Ec37gCtH\nGftj4BOTn5qZmVXLf6FqZpYhh7uZWYYc7mZmGXK4m5llyOFuZpYhh7uZWYYc7mZmGXK4m5llyOFu\nZpYhh7uZWYYc7mZmGXK4m5llyOFuZpYhh7uZWYYc7mZmGXK4m5llaNxwl3SppF1lr9ckfUbSeZK2\nSHohfZ2RxkvSPZIGJO2WdEX9yzAzs3KVPGbv+xExLyLmAfOBN4FHKD0+b2tEzAG28tPH6V0DzEmv\nHuDeekzczMzGNtFlmUXAixHxErAEWJ/a1wNL0/YS4P4o2Qa0SppZk9mamVlFFBGVD5buA56JiD+T\ndDQiWlO7gCMR0SppM7AqIp5OfVuBuyJix4hz9VC6sqetrW1+X19fVQUMHT7GoeOl7bmzpld1jmYy\nPDxMS0tLo6cxJYpUKxSr3iLVCvWrt7u7e2dEdI7WV9EDsgEknQl8HPjcyL6ICEmV/5QoHdML9AJ0\ndnZGV1fXRA5/x9oNG1m9p1TG/mXVnaOZ9Pf3U+33qtkUqVYoVr1FqhUaU+9ElmWuoXTVfijtHzqx\n3JK+DqX2QaC97LjZqc3MzKbIRML9ZuCBsv1NwPK0vRzYWNZ+S/rUzELgWEQcnPRMzcysYhUty0ia\nBnwU+PWy5lXAQ5JWAC8BN6b2x4BrgQFKn6y5tWazNTOzilQU7hHxBnD+iLZXKX16ZuTYAG6ryezM\nzKwq/gtVM7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLk\ncDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQxWFu6RWSQ9L+p6kvZJ+XtJ5krZIeiF9nZHG\nStI9kgYk7ZZ0RX1LMDOzkSq9cr8beDwiPgR8GNgLrAS2RsQcYGvah9KDtOekVw9wb01nbGZm4xo3\n3CVNBz4CrAOIiH+OiKPAEmB9GrYeWJq2lwD3R8k2oFXSzJrP3MzMxqTSI09PMkCaB/QCz1O6at8J\n3AEMRkRrGiPgSES0StoMrIqIp1PfVuCuiNgx4rw9lK7saWtrm9/X11dVAUOHj3HoeGl77qzpVZ2j\nmQwPD9PS0tLoaUyJItUKxaq3SLVC/ert7u7eGRGdo/VV8oDsM4ArgNsjYruku/npEgxQeii2pJP/\nlBghInop/dCgs7Mzurq6JnL4O9Zu2MjqPaUy9i+r7hzNpL+/n2q/V82mSLVCseotUq3QmHorWXM/\nAByIiO1p/2FKYX/oxHJL+jqU+geB9rLjZ6c2MzObIuOGe0T8CHhZ0qWpaRGlJZpNwPLUthzYmLY3\nAbekT80sBI5FxMHaTtvMzE6mkmUZgNuBDZLOBPYBt1L6wfCQpBXAS8CNaexjwLXAAPBmGmtmZlOo\nonCPiF3AaIv2i0YZG8Btk5yXmZlNgv9C1cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzN\nzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy1BF4S5pv6Q9\nknZJ2pHazpO0RdIL6euM1C5J90gakLRb0hX1LMDMzN5rIlfu3RExLyJOPJFpJbA1IuYAW9M+wDXA\nnPTqAe6t1WTNzKwyk1mWWQKsT9vrgaVl7fdHyTagVdLMSbyPmZlNkEqPPB1nkPQD4AgQwFciolfS\n0YhoTf0CjkREq6TNwKqIeDr1bQXuiogdI87ZQ+nKnra2tvl9fX1VFTB0+BiHjpe2586aXtU5msnw\n8DAtLS2NnsaUKFKtUKx6i1Qr1K/e7u7unWWrKe9S0QOygV+IiEFJ/wbYIul75Z0REZLG/ynx7mN6\ngV6Azs7O6Orqmsjh71i7YSOr95TK2L+sunM0k/7+fqr9XjWbItUKxaq3SLVCY+qtaFkmIgbT1yHg\nEeBK4NCJ5Zb0dSgNHwTayw6fndrMzGyKjBvukqZJOvfENvAx4FlgE7A8DVsObEzbm4Bb0qdmFgLH\nIuJgzWduZmZjqmRZpg14pLSszhnAX0fE45K+DTwkaQXwEnBjGv8YcC0wALwJ3FrzWZuZ2UmNG+4R\nsQ/48CjtrwKLRmkP4LaazM7MzKriv1A1M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOz\nDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwyVHG4Szpd0nck\nbU77F0vaLmlA0oOSzkztZ6X9gdTfUZ+pm5nZWCZy5X4HsLds/4vAmoi4BDgCrEjtK4AjqX1NGmdm\nZlOoonCXNBu4Dvhq2hdwNfBwGrIeWJq2l6R9Uv+iNN7MzKaISo88HWeQ9DDwR8C5wG8BnwK2patz\nJLUD34yIyyQ9CyyOiAOp70VgQUS8MuKcPUAPQFtb2/y+vr6qChg6fIxDx0vbc2dNr+oczWR4eJiW\nlpZGT2NKFKlWKFa9RaoV6ldvd3f3zojoHK1v3AdkS7oeGIqInZK6ajWpiOgFegE6Ozujq6u6U6/d\nsJHVe0pl7F9W3TmaSX9/P9V+r5pNkWqFYtVbpFqhMfWOG+7AVcDHJV0LnA18ALgbaJV0RkS8BcwG\nBtP4QaAdOCDpDGA68GrNZ25mZmMad809Ij4XEbMjogO4CXgyIpYBTwE3pGHLgY1pe1PaJ/U/GZWs\n/ZiZWc1M5nPudwF3ShoAzgfWpfZ1wPmp/U5g5eSmaGZmE1XJssw7IqIf6E/b+4ArRxnzY+ATNZib\nmZlVyX+hamaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmG\nHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZWjccJd0tqRvSfqupOck/X5qv1jSdkkDkh6U\ndGZqPyvtD6T+jvqWYGZmI1Vy5f4T4OqI+DAwD1gsaSHwRWBNRFwCHAFWpPErgCOpfU0aZ2ZmU6iS\nB2RHRAyn3felVwBXAw+n9vXA0rS9JO2T+hdJUs1mbGZm41JEjD9IOh3YCVwCfBn4E2BbujpHUjvw\nzYi4TNKzwOKIOJD6XgQWRMQrI87ZA/QAtLW1ze/r66uqgKHDxzh0vLQ9d9b0qs7RTIaHh2lpaWn0\nNKZEkWqFYtVbpFqhfvV2d3fvjIjO0foqekB2RLwNzJPUCjwCfGiyk4qIXqAXoLOzM7q6uqo6z9oN\nG1m9p1TG/mXVnaOZ9Pf3U+33qtkUqVYoVr1FqhUaU++EPi0TEUeBp4CfB1olnfjhMBsYTNuDQDtA\n6p8OvFqT2ZqZWUUq+bTMhemKHUnnAB8F9lIK+RvSsOXAxrS9Ke2T+p+MStZ+zMysZipZlpkJrE/r\n7qcBD0XEZknPA32S/hD4DrAujV8H/C9JA8Bh4KY6zNvMzE5i3HCPiN3A5aO07wOuHKX9x8AnajI7\nMzOriv9C1cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MM\nOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy1Alj9lrl/SUpOclPSfpjtR+nqQtkl5IX2ek\ndkm6R9KApN2Srqh3EWZm9m6VXLm/BXw2In4WWAjcJulngZXA1oiYA2xN+wDXAHPSqwe4t+azNjOz\nkxo33CPiYEQ8k7Zfp/Rw7FnAEmB9GrYeWJq2lwD3R8k2oFXSzJrP3MzMxjShNXdJHZSep7odaIuI\ng6nrR0Bb2p4FvFx22IHUZmZmU2TcB2SfIKkF+DrwmYh4TdI7fRERkmIibyyph9KyDW1tbfT390/k\n8He0nQOfnfsWQNXnaCbDw8OFqBOKVSsUq94i1QqNqbeicJf0PkrBviEivpGaD0maGREH07LLUGof\nBNrLDp+d2t4lInqBXoDOzs7o6uqqqoC1Gzayek+pjP3LqjtHM+nv76fa71WzKVKtUKx6i1QrNKbe\nSj4tI2AdsDci/rSsaxOwPG0vBzaWtd+SPjWzEDhWtnxjZmZToJIr96uATwJ7JO1Kbb8LrAIekrQC\neAm4MfU9BlwLDABvArfWdMZmZjauccM9Ip4GNEb3olHGB3DbJOdlZmaT4L9QNTPLkMPdzCxDDncz\nsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPd\nzCxDDnczsww53M3MMuRwNzPLUCXPUL1P0pCkZ8vazpO0RdIL6euM1C5J90gakLRb0hX1nLyZmY2u\nkiv3/wksHtG2EtgaEXOArWkf4BpgTnr1APfWZpqV6Vj56DsvM7MiGzfcI+LvgMMjmpcA69P2emBp\nWfv9UbINaJU0s1aTNTOzyqj0POtxBkkdwOaIuCztH42I1rQt4EhEtEraDKxKD9VG0lbgrojYMco5\neyhd3dPW1ja/r6+vqgKGDh/j0PH3ts+dNb2q853qhoeHaWlpafQ0pkSRaoVi1VukWqF+9XZ3d++M\niM7R+s6Y7MkjIiSN/xPivcf1Ar0AnZ2d0dXVVdX7r92wkdV73lvG/mXVne9U19/fT7Xfq2ZTpFqh\nWPUWqVZoTL3Vflrm0InllvR1KLUPAu1l42anNjMzm0LVhvsmYHnaXg5sLGu/JX1qZiFwLCIOTnKO\nZmY2QeMuy0h6AOgCLpB0APg8sAp4SNIK4CXgxjT8MeBaYAB4E7i1DnM2M7NxjBvuEXHzGF2LRhkb\nwG2TnZSZmU2O/0LVzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPL0KRvHHaq\nKr+n+/5V1zVwJmZmU89X7mZmGXK4m5llyOFuZpYhh7uZWYYc7mZmGcr20zLl/MkZMyuaQoR7ufKg\nH8nBb2a5qEu4S1oM3A2cDnw1IlbV431qbawrfF/5m1mzqXm4Szod+DLwUeAA8G1JmyLi+Vq/Vz2d\n7Aq/Fuf0Dwkzq6d6XLlfCQxExD4ASX3AEqCpwn0sY4X+WFf6lZynkv9LONH+2blv0VXxbCt/32rH\n1YJ/6FXP3zsbi0qPPa3hCaUbgMUR8atp/5PAgoj4jRHjeoCetHsp8P0q3/IC4JUqj21GRaq3SLVC\nseotUq1Qv3oviogLR+to2C9UI6IX6J3seSTtiIjOGkypKRSp3iLVCsWqt0i1QmPqrcfn3AeB9rL9\n2anNzMymSD3C/dvAHEkXSzoTuAnYVIf3MTOzMdR8WSYi3pL0G8D/pvRRyPsi4rlav0+ZSS/tNJki\n1VukWqFY9RapVmhAvTX/haqZmTWe7y1jZpYhh7uZWYaaOtwlLZb0fUkDklY2ej71JOk+SUOSnm30\nXOpNUrukpyQ9L+k5SXc0ek71JOlsSd+S9N1U7+83ek71Jul0Sd+RtLnRc6k3Sfsl7ZG0S9KOKXvf\nZl1zT7c5+AfKbnMA3NxstzmolKSPAMPA/RFxWaPnU0+SZgIzI+IZSecCO4GlGf+zFTAtIoYlvQ94\nGrgjIrY1eGp1I+lOoBP4QERc3+j51JOk/UBnREzpH20185X7O7c5iIh/Bk7c5iBLEfF3wOFGz2Mq\nRMTBiHgmbb8O7AVmNXZW9RMlw2n3fenVnFddFZA0G7gO+Gqj55KzZg73WcDLZfsHyDgAikpSB3A5\nsL2xM6mvtEyxCxgCtkREzvV+Cfgd4F8aPZEpEsATknam265MiWYOd8ucpBbg68BnIuK1Rs+nniLi\n7YiYR+kvuq+UlOXSm6TrgaGI2NnouUyhX4iIK4BrgNvSEmvdNXO4+zYHGUtrz18HNkTENxo9n6kS\nEUeBp4DFjZ5LnVwFfDytQ/cBV0v6q8ZOqb4iYjB9HQIeobSkXHfNHO6+zUGm0i8Y1wF7I+JPGz2f\nepN0oaTWtH0OpQ8JfK+xs6qPiPhcRMyOiA5K/80+GRG/3OBp1Y2kaelDAUiaBnwMmJJPvDVtuEfE\nW8CJ2xzsBR6q820OGkrSA8DfA5dKOiBpRaPnVEdXAZ+kdFW3K72ubfSk6mgm8JSk3ZQuWrZERPYf\nESyINuBpSd8FvgU8GhGPT8UbN+1HIc3MbGxNe+VuZmZjc7ibmWXI4W5mliGHu5lZhhzuZmZTbCI3\nApS0puxTY/8g6WhF7+FPy5iZTa1qbwQo6Xbg8oj4lfHG+srdzGyKjXYjQEkflPR4ugfN/5H0oVEO\nvRl4oJL3qPkzVM3MrCq9wKcj4gVJC4D/AVx9olPSRcDFwJOVnMzhbmbWYOkmef8R+Frp7hsAnDVi\n2E3AwxHxdiXndLibmTXeacDRdGfQsdwE3DaRE5qZWQOlW1r/QNInoHTzPEkfPtGf1t9nULq/VEUc\n7mZmU2yMGwEuA1akm4w9x7ufLHcT0BcT+HijPwppZpYhX7mbmWXI4W5mliGHu5lZhhzuZmYZcrib\nmWXI4W5mliGHu5lZhv4V6RIfwEl0isUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xy4E5YFWhSt",
        "colab_type": "code",
        "outputId": "f51bc8c5-2b5c-4f4d-b421-043400fa2884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#create log hammer price\n",
        "df['log hammer price'] = np.log10(df['hammer price'])\n",
        "df.hist(column='log hammer price', bins=100)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fc84a321828>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1FDXxRvgMhAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7zDpOLLyzUv",
        "colab_type": "code",
        "outputId": "800866b8-f7a2-48ba-f49b-5df6fa4e925c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#drop 'hammer price' column\n",
        "df.drop(columns='hammer price')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_author</th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height</th>\n",
              "      <th>width</th>\n",
              "      <th>estimate_low</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>log hammer price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1983.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>80669.0</td>\n",
              "      <td>141170.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>5.207737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1984.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>91964.0</td>\n",
              "      <td>118239.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>5.109747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>131377.0</td>\n",
              "      <td>197066.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>5.021611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>91964.0</td>\n",
              "      <td>131378.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>4.858886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1986.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>131377.0</td>\n",
              "      <td>197066.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>5.232465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>100</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>2117691.0</td>\n",
              "      <td>2911825.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>6.339227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>100</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1800000.0</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>6.278754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>100</td>\n",
              "      <td>1986.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>3500000.0</td>\n",
              "      <td>4500000.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>6.637790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>100</td>\n",
              "      <td>1989.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>3000000.0</td>\n",
              "      <td>5000000.0</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>6.518514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>100</td>\n",
              "      <td>1978.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1923.0</td>\n",
              "      <td>6.176091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1101 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      num_author  creation_year  ...  artist_birth  log hammer price\n",
              "0              1         1983.0  ...        1928.0          5.207737\n",
              "2              1         1984.0  ...        1928.0          5.109747\n",
              "3              1         1982.0  ...        1928.0          5.021611\n",
              "5              1         1982.0  ...        1928.0          4.858886\n",
              "6              1         1986.0  ...        1928.0          5.232465\n",
              "...          ...            ...  ...           ...               ...\n",
              "1453         100         1999.0  ...        1923.0          6.339227\n",
              "1454         100         2012.0  ...        1923.0          6.278754\n",
              "1455         100         1986.0  ...        1923.0          6.637790\n",
              "1456         100         1989.0  ...        1923.0          6.518514\n",
              "1457         100         1978.0  ...        1923.0          6.176091\n",
              "\n",
              "[1101 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATx94FXNWhaV",
        "colab_type": "code",
        "outputId": "e15f5d70-74c6-4d4c-bcc9-5ed626f2dec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#bin log hammer price\n",
        "bins = [0,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5,8]\n",
        "labels = [0,1,2,3,4,5,6,7,8,9,10]\n",
        "df['binned'] = pd.cut(df['log hammer price'], bins=bins, labels=labels)\n",
        "print(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      num_author  creation_year  height  ...  hammer price  log hammer price  binned\n",
            "0              1         1983.0    14.0  ...      161338.0          5.207737       5\n",
            "2              1         1984.0    20.0  ...      128750.0          5.109747       5\n",
            "3              1         1982.0     8.0  ...      105102.0          5.021611       5\n",
            "5              1         1982.0     6.0  ...       72258.0          4.858886       4\n",
            "6              1         1986.0    22.0  ...      170791.0          5.232465       5\n",
            "...          ...            ...     ...  ...           ...               ...     ...\n",
            "1453         100         1999.0     8.0  ...     2183869.0          6.339227       7\n",
            "1454         100         2012.0     6.0  ...     1900000.0          6.278754       7\n",
            "1455         100         1986.0     2.0  ...     4343000.0          6.637790       8\n",
            "1456         100         1989.0    76.0  ...     3300000.0          6.518514       8\n",
            "1457         100         1978.0     2.0  ...     1500000.0          6.176091       7\n",
            "\n",
            "[1101 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY7cr3HETvo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create X and Y\n",
        "dataset = df.values\n",
        "X = dataset[:, :8]\n",
        "Y = dataset[:,10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW71jXL5bEWR",
        "colab_type": "code",
        "outputId": "623c3a97-366a-46c9-a881-c19ea52bbcbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1983.0, 14.0, ..., 141170.0, 2018.0, 1928.0],\n",
              "       [1, 1984.0, 20.0, ..., 118239.0, 2018.0, 1928.0],\n",
              "       [1, 1982.0, 8.0, ..., 197066.0, 2018.0, 1928.0],\n",
              "       ...,\n",
              "       [100, 1986.0, 2.0, ..., 4500000.0, 2018.0, 1923.0],\n",
              "       [100, 1989.0, 76.0, ..., 5000000.0, 2017.0, 1923.0],\n",
              "       [100, 1978.0, 2.0, ..., 2500000.0, 2017.0, 1923.0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI9UuVfKab65",
        "colab_type": "code",
        "outputId": "dacd10f5-8aa3-407c-ca2d-df5228b5a5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 5, ..., 8, 8, 7], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83B-mqxyjo1a",
        "colab_type": "code",
        "outputId": "151bff55-1dd6-4120-82e6-3313641dd799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# one-hot encode the log hammer price\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y) \n",
        "print(dummy_y)\n",
        "print(dummy_y.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(1101, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9QfVGgLbZ-x",
        "colab_type": "code",
        "outputId": "2287eb12-f8be-4a5a-937e-730a52a6cc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# split training set and test set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size=0.25)\n",
        "print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)\n",
        "print(X_test[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(825, 8) (825, 11) (276, 8) (276, 11)\n",
            "[43 1998.0 4.0 70.0 157653.0 236479.0 2018.0 1945.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsH8c0v6cqwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#standardize X_train and X_test using mean and standard deviation of training samples\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWqcJ2kD657o",
        "colab_type": "code",
        "outputId": "47b276ae-fd28-426a-c869-36a486a650e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_train_scaled"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9.76594821e-02, -4.25762229e-01,  3.17705542e+00, ...,\n",
              "        -1.73962361e-01,  9.91550844e-01, -1.20056226e-01],\n",
              "       [ 1.22953662e+00, -8.94851224e-01,  9.83828451e-01, ...,\n",
              "        -2.30094532e-01, -1.00852115e+00, -1.99794104e+00],\n",
              "       [-1.30861211e+00, -4.25762229e-01, -3.12368690e-03, ...,\n",
              "        -2.44127575e-01,  9.91550844e-01, -7.19053331e-02],\n",
              "       ...,\n",
              "       [-1.30861211e+00, -8.47942324e-01, -3.32107733e-01, ...,\n",
              "        -2.72193661e-01, -1.00852115e+00, -7.19053331e-02],\n",
              "       [-1.30861211e+00,  9.02356648e-02, -3.32107733e-01, ...,\n",
              "        -2.94646530e-01,  9.91550844e-01, -7.19053331e-02],\n",
              "       [-2.11034283e-01, -3.58213414e-03,  1.42247385e+00, ...,\n",
              "        -3.05008248e-01, -1.00852115e+00, -2.37544404e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpd6nLwSSMaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create model\n",
        "model_16_16 = Sequential()\n",
        "model_16_32 = Sequential()\n",
        "model_16_64 = Sequential()\n",
        "model_16_128 = Sequential()\n",
        "\n",
        "model_32_16 = Sequential()\n",
        "model_32_32 = Sequential()\n",
        "model_32_64 = Sequential()\n",
        "model_32_128 = Sequential()\n",
        "\n",
        "model_64_16 = Sequential()\n",
        "model_64_32 = Sequential()\n",
        "model_64_64 = Sequential()\n",
        "model_64_128 = Sequential()\n",
        "\n",
        "model_128_16 = Sequential()\n",
        "model_128_32 = Sequential()\n",
        "model_128_64 = Sequential()\n",
        "model_128_128 = Sequential()\n",
        "\n",
        "#Add more layers\n",
        "model_16_16.add(Dense(16, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_16_16.add(Dense(16, activation='relu', kernel_initializer='normal'))\n",
        "model_16_16.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_16_32.add(Dense(16, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_16_32.add(Dense(32, activation='relu', kernel_initializer='normal'))\n",
        "model_16_32.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_16_64.add(Dense(16, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_16_64.add(Dense(64, activation='relu', kernel_initializer='normal'))\n",
        "model_16_64.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_16_128.add(Dense(16, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_16_128.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "model_16_128.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_32_16.add(Dense(32, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_32_16.add(Dense(16, activation='relu', kernel_initializer='normal'))\n",
        "model_32_16.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_32_32.add(Dense(32, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_32_32.add(Dense(32, activation='relu', kernel_initializer='normal'))\n",
        "model_32_32.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_32_64.add(Dense(32, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_32_64.add(Dense(64, activation='relu', kernel_initializer='normal'))\n",
        "model_32_64.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_32_128.add(Dense(32, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_32_128.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "model_32_128.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_64_16.add(Dense(64, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_64_16.add(Dense(16, activation='relu', kernel_initializer='normal'))\n",
        "model_64_16.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_64_32.add(Dense(64, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_64_32.add(Dense(32, activation='relu', kernel_initializer='normal'))\n",
        "model_64_32.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_64_64.add(Dense(64, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_64_64.add(Dense(64, activation='relu', kernel_initializer='normal'))\n",
        "model_64_64.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_64_128.add(Dense(64, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_64_128.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "model_64_128.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_128_16.add(Dense(128, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_128_16.add(Dense(16, activation='relu', kernel_initializer='normal'))\n",
        "model_128_16.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_128_32.add(Dense(128, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_128_32.add(Dense(32, activation='relu', kernel_initializer='normal'))\n",
        "model_128_32.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_128_64.add(Dense(128, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_128_64.add(Dense(64, activation='relu', kernel_initializer='normal'))\n",
        "model_128_64.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "#Add more layers\n",
        "model_128_128.add(Dense(128, activation='relu', input_shape = (8,), kernel_initializer='normal'))\n",
        "model_128_128.add(Dense(128, activation='relu', kernel_initializer='normal'))\n",
        "model_128_128.add(Dense(11, activation='softmax', kernel_initializer='normal'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc7LBgvMYWK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# complie the model using adam optimizer and categorical crossentropy \n",
        "model_16_16.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_16_32.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_16_64.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_16_128.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model_32_16.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_32_32.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_32_64.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_32_128.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model_64_16.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_64_32.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_64_64.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_64_128.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model_128_16.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_128_32.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_128_64.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model_128_128.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNR_ChetYg_Y",
        "colab_type": "code",
        "outputId": "c87210c5-3b06-432e-b55b-5b9b8dff76ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the model\n",
        "hist_16_16 = model_16_16.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_16_32 = model_16_32.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_16_64 = model_16_64.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_16_128 = model_16_128.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "\n",
        "hist_32_16 = model_32_16.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_32_32 = model_32_32.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_32_64 = model_32_64.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_32_128 = model_32_128.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "\n",
        "hist_64_16 = model_64_16.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_64_32 = model_64_32.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_64_64 = model_64_64.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_64_128 = model_64_128.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "\n",
        "hist_128_16 = model_128_16.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_128_32 = model_128_32.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_128_64 = model_128_64.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)\n",
        "hist_128_128 = model_128_128.fit(X_train_scaled, Y_train, batch_size=64, epochs=200,validation_split=0.1)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3900 - acc: 0.2075 - val_loss: 2.3790 - val_acc: 0.2771\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 2.3708 - acc: 0.2507 - val_loss: 2.3533 - val_acc: 0.3373\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 2.3412 - acc: 0.2520 - val_loss: 2.3106 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 2.2923 - acc: 0.2547 - val_loss: 2.2423 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.2164 - acc: 0.2547 - val_loss: 2.1460 - val_acc: 0.3373\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 2.1217 - acc: 0.2547 - val_loss: 2.0305 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 2.0226 - acc: 0.2547 - val_loss: 1.9231 - val_acc: 0.3373\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.9483 - acc: 0.2547 - val_loss: 1.8487 - val_acc: 0.3373\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.9033 - acc: 0.2547 - val_loss: 1.8092 - val_acc: 0.3373\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.8795 - acc: 0.2547 - val_loss: 1.7813 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.8605 - acc: 0.2547 - val_loss: 1.7687 - val_acc: 0.3373\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.8460 - acc: 0.2547 - val_loss: 1.7589 - val_acc: 0.3373\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.8344 - acc: 0.2547 - val_loss: 1.7475 - val_acc: 0.3373\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.8244 - acc: 0.2547 - val_loss: 1.7410 - val_acc: 0.3373\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.8154 - acc: 0.2547 - val_loss: 1.7318 - val_acc: 0.3373\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8062 - acc: 0.2547 - val_loss: 1.7247 - val_acc: 0.3373\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.7970 - acc: 0.2547 - val_loss: 1.7162 - val_acc: 0.3373\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.7885 - acc: 0.2547 - val_loss: 1.7104 - val_acc: 0.3373\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.7784 - acc: 0.2547 - val_loss: 1.7036 - val_acc: 0.3373\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.7693 - acc: 0.2574 - val_loss: 1.7003 - val_acc: 0.3373\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.7578 - acc: 0.2642 - val_loss: 1.6898 - val_acc: 0.3373\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.7472 - acc: 0.2628 - val_loss: 1.6745 - val_acc: 0.3614\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.7338 - acc: 0.2736 - val_loss: 1.6703 - val_acc: 0.3012\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.7218 - acc: 0.2722 - val_loss: 1.6659 - val_acc: 0.3012\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.7070 - acc: 0.2763 - val_loss: 1.6564 - val_acc: 0.3133\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.6932 - acc: 0.2817 - val_loss: 1.6404 - val_acc: 0.3133\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.6787 - acc: 0.2857 - val_loss: 1.6286 - val_acc: 0.3133\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6636 - acc: 0.2925 - val_loss: 1.6186 - val_acc: 0.3253\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6483 - acc: 0.2992 - val_loss: 1.6028 - val_acc: 0.3253\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.6336 - acc: 0.2978 - val_loss: 1.5914 - val_acc: 0.3373\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.6197 - acc: 0.3113 - val_loss: 1.5832 - val_acc: 0.3373\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.6055 - acc: 0.3194 - val_loss: 1.5695 - val_acc: 0.3494\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.5924 - acc: 0.3208 - val_loss: 1.5538 - val_acc: 0.3494\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.5778 - acc: 0.3221 - val_loss: 1.5361 - val_acc: 0.3614\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.5651 - acc: 0.3275 - val_loss: 1.5278 - val_acc: 0.3855\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.5518 - acc: 0.3410 - val_loss: 1.5083 - val_acc: 0.4096\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.5392 - acc: 0.3315 - val_loss: 1.5016 - val_acc: 0.3855\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5251 - acc: 0.3747 - val_loss: 1.4885 - val_acc: 0.3614\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.5120 - acc: 0.3949 - val_loss: 1.4750 - val_acc: 0.3855\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.4993 - acc: 0.4043 - val_loss: 1.4594 - val_acc: 0.4096\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.4860 - acc: 0.3962 - val_loss: 1.4431 - val_acc: 0.3735\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.4724 - acc: 0.4164 - val_loss: 1.4409 - val_acc: 0.3735\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.4588 - acc: 0.4286 - val_loss: 1.4251 - val_acc: 0.3855\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.4476 - acc: 0.4326 - val_loss: 1.4070 - val_acc: 0.4096\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4344 - acc: 0.4272 - val_loss: 1.4028 - val_acc: 0.3855\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.4225 - acc: 0.4232 - val_loss: 1.3971 - val_acc: 0.3976\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4100 - acc: 0.4394 - val_loss: 1.3808 - val_acc: 0.4217\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3982 - acc: 0.4488 - val_loss: 1.3685 - val_acc: 0.4096\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3887 - acc: 0.4474 - val_loss: 1.3590 - val_acc: 0.4217\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3777 - acc: 0.4650 - val_loss: 1.3478 - val_acc: 0.4096\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.3671 - acc: 0.4596 - val_loss: 1.3481 - val_acc: 0.3855\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.3566 - acc: 0.4730 - val_loss: 1.3324 - val_acc: 0.4458\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3465 - acc: 0.4771 - val_loss: 1.3311 - val_acc: 0.4337\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3362 - acc: 0.4663 - val_loss: 1.3170 - val_acc: 0.4578\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.3273 - acc: 0.4784 - val_loss: 1.3129 - val_acc: 0.4578\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3184 - acc: 0.4784 - val_loss: 1.2999 - val_acc: 0.4458\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.3104 - acc: 0.4852 - val_loss: 1.2968 - val_acc: 0.4096\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.3006 - acc: 0.4852 - val_loss: 1.2960 - val_acc: 0.4578\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2917 - acc: 0.4933 - val_loss: 1.2879 - val_acc: 0.4458\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2827 - acc: 0.5067 - val_loss: 1.2747 - val_acc: 0.4458\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.2747 - acc: 0.4973 - val_loss: 1.2745 - val_acc: 0.4458\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.2676 - acc: 0.5013 - val_loss: 1.2606 - val_acc: 0.4578\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2596 - acc: 0.5067 - val_loss: 1.2697 - val_acc: 0.4337\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.2522 - acc: 0.5135 - val_loss: 1.2567 - val_acc: 0.4337\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2456 - acc: 0.5162 - val_loss: 1.2518 - val_acc: 0.4337\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2383 - acc: 0.5270 - val_loss: 1.2515 - val_acc: 0.4458\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2295 - acc: 0.5081 - val_loss: 1.2277 - val_acc: 0.4578\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2226 - acc: 0.5216 - val_loss: 1.2339 - val_acc: 0.4819\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2151 - acc: 0.5189 - val_loss: 1.2468 - val_acc: 0.4096\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2089 - acc: 0.5296 - val_loss: 1.2295 - val_acc: 0.4217\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2017 - acc: 0.5243 - val_loss: 1.2127 - val_acc: 0.4940\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1947 - acc: 0.5391 - val_loss: 1.2174 - val_acc: 0.4217\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1883 - acc: 0.5431 - val_loss: 1.2124 - val_acc: 0.4458\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1824 - acc: 0.5512 - val_loss: 1.2149 - val_acc: 0.4819\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1759 - acc: 0.5458 - val_loss: 1.1955 - val_acc: 0.4578\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1701 - acc: 0.5431 - val_loss: 1.2017 - val_acc: 0.4337\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1636 - acc: 0.5620 - val_loss: 1.2034 - val_acc: 0.5181\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1579 - acc: 0.5701 - val_loss: 1.1922 - val_acc: 0.4699\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1528 - acc: 0.5580 - val_loss: 1.1812 - val_acc: 0.4699\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1460 - acc: 0.5539 - val_loss: 1.1844 - val_acc: 0.5422\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1411 - acc: 0.5701 - val_loss: 1.1915 - val_acc: 0.5422\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1375 - acc: 0.5539 - val_loss: 1.1760 - val_acc: 0.4819\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1289 - acc: 0.5660 - val_loss: 1.1683 - val_acc: 0.4940\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1256 - acc: 0.5795 - val_loss: 1.1754 - val_acc: 0.5542\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1210 - acc: 0.5728 - val_loss: 1.1571 - val_acc: 0.5060\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1140 - acc: 0.5741 - val_loss: 1.1593 - val_acc: 0.5904\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1119 - acc: 0.5768 - val_loss: 1.1594 - val_acc: 0.5422\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1055 - acc: 0.5593 - val_loss: 1.1568 - val_acc: 0.4819\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1016 - acc: 0.5539 - val_loss: 1.1581 - val_acc: 0.5060\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0952 - acc: 0.5768 - val_loss: 1.1375 - val_acc: 0.5904\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.0941 - acc: 0.5768 - val_loss: 1.1482 - val_acc: 0.5301\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0920 - acc: 0.5755 - val_loss: 1.1424 - val_acc: 0.5542\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0852 - acc: 0.5660 - val_loss: 1.1316 - val_acc: 0.5301\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0814 - acc: 0.5660 - val_loss: 1.1420 - val_acc: 0.5542\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0779 - acc: 0.5768 - val_loss: 1.1305 - val_acc: 0.5904\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0733 - acc: 0.5903 - val_loss: 1.1288 - val_acc: 0.5663\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0664 - acc: 0.5782 - val_loss: 1.1181 - val_acc: 0.5783\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0628 - acc: 0.5822 - val_loss: 1.1246 - val_acc: 0.5422\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.0593 - acc: 0.5836 - val_loss: 1.1145 - val_acc: 0.6145\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0563 - acc: 0.6038 - val_loss: 1.1205 - val_acc: 0.5663\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.0528 - acc: 0.5863 - val_loss: 1.1054 - val_acc: 0.5181\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.0475 - acc: 0.5930 - val_loss: 1.1083 - val_acc: 0.5181\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0453 - acc: 0.5957 - val_loss: 1.1053 - val_acc: 0.5663\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0416 - acc: 0.5984 - val_loss: 1.0992 - val_acc: 0.5783\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0420 - acc: 0.5903 - val_loss: 1.0819 - val_acc: 0.5663\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0368 - acc: 0.5984 - val_loss: 1.1031 - val_acc: 0.5542\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0317 - acc: 0.5930 - val_loss: 1.0931 - val_acc: 0.5783\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0301 - acc: 0.5876 - val_loss: 1.0814 - val_acc: 0.6024\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0265 - acc: 0.6051 - val_loss: 1.0960 - val_acc: 0.5060\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.0253 - acc: 0.6011 - val_loss: 1.0812 - val_acc: 0.6024\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0194 - acc: 0.6146 - val_loss: 1.0838 - val_acc: 0.5783\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0170 - acc: 0.5957 - val_loss: 1.0758 - val_acc: 0.5301\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0120 - acc: 0.6051 - val_loss: 1.0756 - val_acc: 0.5542\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0143 - acc: 0.6092 - val_loss: 1.0759 - val_acc: 0.6145\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0090 - acc: 0.6092 - val_loss: 1.0656 - val_acc: 0.5422\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0059 - acc: 0.5957 - val_loss: 1.0671 - val_acc: 0.6145\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.0014 - acc: 0.6105 - val_loss: 1.0650 - val_acc: 0.5663\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9976 - acc: 0.6105 - val_loss: 1.0597 - val_acc: 0.6386\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9962 - acc: 0.6092 - val_loss: 1.0574 - val_acc: 0.5663\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9920 - acc: 0.6173 - val_loss: 1.0526 - val_acc: 0.5542\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 0.9889 - acc: 0.6065 - val_loss: 1.0507 - val_acc: 0.5904\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9898 - acc: 0.6146 - val_loss: 1.0495 - val_acc: 0.5542\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9854 - acc: 0.6132 - val_loss: 1.0474 - val_acc: 0.6506\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9834 - acc: 0.6199 - val_loss: 1.0354 - val_acc: 0.5904\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9800 - acc: 0.6199 - val_loss: 1.0379 - val_acc: 0.5783\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9796 - acc: 0.5943 - val_loss: 1.0547 - val_acc: 0.5422\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9732 - acc: 0.6240 - val_loss: 1.0259 - val_acc: 0.6506\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9766 - acc: 0.5957 - val_loss: 1.0367 - val_acc: 0.6024\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9697 - acc: 0.6334 - val_loss: 1.0234 - val_acc: 0.6747\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9668 - acc: 0.6307 - val_loss: 1.0328 - val_acc: 0.5542\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9659 - acc: 0.6186 - val_loss: 1.0257 - val_acc: 0.6024\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9635 - acc: 0.6253 - val_loss: 1.0261 - val_acc: 0.6506\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9589 - acc: 0.6186 - val_loss: 1.0225 - val_acc: 0.5783\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9583 - acc: 0.6226 - val_loss: 1.0259 - val_acc: 0.5783\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9595 - acc: 0.6119 - val_loss: 1.0139 - val_acc: 0.6145\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9569 - acc: 0.6186 - val_loss: 1.0322 - val_acc: 0.5663\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9510 - acc: 0.6240 - val_loss: 1.0050 - val_acc: 0.6386\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9522 - acc: 0.6173 - val_loss: 1.0168 - val_acc: 0.6265\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9477 - acc: 0.6321 - val_loss: 1.0076 - val_acc: 0.6145\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9451 - acc: 0.6253 - val_loss: 1.0150 - val_acc: 0.5783\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9435 - acc: 0.6429 - val_loss: 0.9991 - val_acc: 0.6747\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9407 - acc: 0.6402 - val_loss: 1.0001 - val_acc: 0.5904\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9392 - acc: 0.6334 - val_loss: 0.9911 - val_acc: 0.6867\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9381 - acc: 0.6307 - val_loss: 1.0022 - val_acc: 0.5904\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9339 - acc: 0.6375 - val_loss: 0.9971 - val_acc: 0.6265\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9327 - acc: 0.6294 - val_loss: 0.9955 - val_acc: 0.5904\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9317 - acc: 0.6482 - val_loss: 0.9919 - val_acc: 0.6506\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9303 - acc: 0.6321 - val_loss: 0.9889 - val_acc: 0.6867\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9341 - acc: 0.6361 - val_loss: 0.9959 - val_acc: 0.6145\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9228 - acc: 0.6456 - val_loss: 0.9784 - val_acc: 0.6386\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9228 - acc: 0.6253 - val_loss: 0.9835 - val_acc: 0.6867\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9225 - acc: 0.6482 - val_loss: 0.9837 - val_acc: 0.6265\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9194 - acc: 0.6388 - val_loss: 0.9844 - val_acc: 0.6265\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9169 - acc: 0.6388 - val_loss: 0.9853 - val_acc: 0.6386\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9171 - acc: 0.6509 - val_loss: 0.9781 - val_acc: 0.6265\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9135 - acc: 0.6280 - val_loss: 0.9835 - val_acc: 0.5783\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9133 - acc: 0.6550 - val_loss: 0.9668 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9082 - acc: 0.6469 - val_loss: 0.9682 - val_acc: 0.6024\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9080 - acc: 0.6563 - val_loss: 0.9674 - val_acc: 0.6506\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9072 - acc: 0.6294 - val_loss: 0.9801 - val_acc: 0.6145\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9049 - acc: 0.6442 - val_loss: 0.9675 - val_acc: 0.6627\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9013 - acc: 0.6590 - val_loss: 0.9591 - val_acc: 0.6747\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8998 - acc: 0.6536 - val_loss: 0.9606 - val_acc: 0.6506\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8998 - acc: 0.6536 - val_loss: 0.9624 - val_acc: 0.6506\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8973 - acc: 0.6509 - val_loss: 0.9571 - val_acc: 0.6265\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8979 - acc: 0.6523 - val_loss: 0.9564 - val_acc: 0.6747\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8947 - acc: 0.6631 - val_loss: 0.9523 - val_acc: 0.6386\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8914 - acc: 0.6415 - val_loss: 0.9602 - val_acc: 0.6506\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8915 - acc: 0.6698 - val_loss: 0.9475 - val_acc: 0.6867\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8887 - acc: 0.6523 - val_loss: 0.9540 - val_acc: 0.6506\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8860 - acc: 0.6658 - val_loss: 0.9435 - val_acc: 0.6627\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8842 - acc: 0.6685 - val_loss: 0.9352 - val_acc: 0.6747\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8832 - acc: 0.6617 - val_loss: 0.9432 - val_acc: 0.6506\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8840 - acc: 0.6631 - val_loss: 0.9419 - val_acc: 0.6627\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8805 - acc: 0.6685 - val_loss: 0.9426 - val_acc: 0.6867\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8781 - acc: 0.6671 - val_loss: 0.9461 - val_acc: 0.6627\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8785 - acc: 0.6617 - val_loss: 0.9385 - val_acc: 0.6627\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8771 - acc: 0.6698 - val_loss: 0.9417 - val_acc: 0.6506\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8733 - acc: 0.6563 - val_loss: 0.9371 - val_acc: 0.6747\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8749 - acc: 0.6671 - val_loss: 0.9258 - val_acc: 0.6747\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8712 - acc: 0.6658 - val_loss: 0.9411 - val_acc: 0.6747\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8678 - acc: 0.6644 - val_loss: 0.9275 - val_acc: 0.6867\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8695 - acc: 0.6590 - val_loss: 0.9298 - val_acc: 0.6506\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8699 - acc: 0.6671 - val_loss: 0.9358 - val_acc: 0.6747\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8645 - acc: 0.6658 - val_loss: 0.9235 - val_acc: 0.6988\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8623 - acc: 0.6685 - val_loss: 0.9259 - val_acc: 0.6627\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8643 - acc: 0.6685 - val_loss: 0.9236 - val_acc: 0.6988\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8624 - acc: 0.6739 - val_loss: 0.9257 - val_acc: 0.6867\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8600 - acc: 0.6698 - val_loss: 0.9225 - val_acc: 0.7108\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8579 - acc: 0.6739 - val_loss: 0.9156 - val_acc: 0.6747\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8571 - acc: 0.6765 - val_loss: 0.9137 - val_acc: 0.6867\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8562 - acc: 0.6725 - val_loss: 0.9077 - val_acc: 0.7108\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8556 - acc: 0.6590 - val_loss: 0.9289 - val_acc: 0.6867\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8550 - acc: 0.6779 - val_loss: 0.9135 - val_acc: 0.6867\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8508 - acc: 0.6792 - val_loss: 0.9070 - val_acc: 0.6867\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8504 - acc: 0.6712 - val_loss: 0.9176 - val_acc: 0.6867\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8479 - acc: 0.6752 - val_loss: 0.9131 - val_acc: 0.6988\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8460 - acc: 0.6779 - val_loss: 0.9098 - val_acc: 0.6988\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8459 - acc: 0.6765 - val_loss: 0.9034 - val_acc: 0.7108\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8461 - acc: 0.6752 - val_loss: 0.9158 - val_acc: 0.6867\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3908 - acc: 0.1833 - val_loss: 2.3813 - val_acc: 0.1205\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 2.3719 - acc: 0.1927 - val_loss: 2.3573 - val_acc: 0.1205\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.3420 - acc: 0.1927 - val_loss: 2.3168 - val_acc: 0.1205\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 2.2917 - acc: 0.1927 - val_loss: 2.2486 - val_acc: 0.1205\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 2.2111 - acc: 0.1927 - val_loss: 2.1463 - val_acc: 0.1205\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.1041 - acc: 0.1927 - val_loss: 2.0270 - val_acc: 0.1205\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.9982 - acc: 0.1927 - val_loss: 1.9161 - val_acc: 0.1205\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.9156 - acc: 0.2197 - val_loss: 1.8344 - val_acc: 0.2892\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.8659 - acc: 0.2682 - val_loss: 1.7754 - val_acc: 0.3373\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.8298 - acc: 0.2547 - val_loss: 1.7377 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.7980 - acc: 0.2547 - val_loss: 1.7075 - val_acc: 0.3373\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.7682 - acc: 0.2642 - val_loss: 1.6831 - val_acc: 0.2892\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.7336 - acc: 0.2830 - val_loss: 1.6561 - val_acc: 0.3133\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6995 - acc: 0.2951 - val_loss: 1.6162 - val_acc: 0.3133\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.6686 - acc: 0.2898 - val_loss: 1.5897 - val_acc: 0.2892\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.6362 - acc: 0.2965 - val_loss: 1.5632 - val_acc: 0.3012\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.6082 - acc: 0.3100 - val_loss: 1.5375 - val_acc: 0.2892\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.5829 - acc: 0.3288 - val_loss: 1.5121 - val_acc: 0.3133\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.5588 - acc: 0.3302 - val_loss: 1.4786 - val_acc: 0.3012\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.5387 - acc: 0.3383 - val_loss: 1.4664 - val_acc: 0.3012\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5168 - acc: 0.3652 - val_loss: 1.4441 - val_acc: 0.3614\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.5001 - acc: 0.3922 - val_loss: 1.4275 - val_acc: 0.3614\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.4836 - acc: 0.3895 - val_loss: 1.4105 - val_acc: 0.3494\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.4672 - acc: 0.4030 - val_loss: 1.4048 - val_acc: 0.3494\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4529 - acc: 0.3976 - val_loss: 1.3787 - val_acc: 0.3614\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4389 - acc: 0.4218 - val_loss: 1.3711 - val_acc: 0.3614\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.4245 - acc: 0.4340 - val_loss: 1.3634 - val_acc: 0.3614\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.4120 - acc: 0.4178 - val_loss: 1.3438 - val_acc: 0.3735\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.3972 - acc: 0.4272 - val_loss: 1.3296 - val_acc: 0.4096\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.3859 - acc: 0.4447 - val_loss: 1.3230 - val_acc: 0.3735\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3730 - acc: 0.4569 - val_loss: 1.3266 - val_acc: 0.3614\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3602 - acc: 0.4582 - val_loss: 1.3090 - val_acc: 0.3614\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.3480 - acc: 0.4636 - val_loss: 1.2863 - val_acc: 0.4337\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3362 - acc: 0.4704 - val_loss: 1.2853 - val_acc: 0.3976\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3236 - acc: 0.4744 - val_loss: 1.2777 - val_acc: 0.3976\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3140 - acc: 0.4825 - val_loss: 1.2787 - val_acc: 0.3976\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.3023 - acc: 0.4838 - val_loss: 1.2580 - val_acc: 0.4217\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.2890 - acc: 0.5013 - val_loss: 1.2536 - val_acc: 0.4337\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2778 - acc: 0.4960 - val_loss: 1.2501 - val_acc: 0.4096\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2661 - acc: 0.5013 - val_loss: 1.2405 - val_acc: 0.4096\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2547 - acc: 0.5175 - val_loss: 1.2307 - val_acc: 0.4578\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2453 - acc: 0.5108 - val_loss: 1.2216 - val_acc: 0.4578\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2350 - acc: 0.5202 - val_loss: 1.2209 - val_acc: 0.4458\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.2268 - acc: 0.5243 - val_loss: 1.2159 - val_acc: 0.4699\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.2193 - acc: 0.5175 - val_loss: 1.2070 - val_acc: 0.4578\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2082 - acc: 0.5229 - val_loss: 1.2026 - val_acc: 0.4578\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1942 - acc: 0.5377 - val_loss: 1.1885 - val_acc: 0.4819\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1878 - acc: 0.5270 - val_loss: 1.1950 - val_acc: 0.4699\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.1763 - acc: 0.5323 - val_loss: 1.1778 - val_acc: 0.4578\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1689 - acc: 0.5243 - val_loss: 1.1724 - val_acc: 0.4819\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1599 - acc: 0.5458 - val_loss: 1.1692 - val_acc: 0.4819\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1504 - acc: 0.5323 - val_loss: 1.1596 - val_acc: 0.4578\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1416 - acc: 0.5512 - val_loss: 1.1615 - val_acc: 0.4699\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1345 - acc: 0.5593 - val_loss: 1.1510 - val_acc: 0.5060\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1261 - acc: 0.5633 - val_loss: 1.1495 - val_acc: 0.4699\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1190 - acc: 0.5431 - val_loss: 1.1492 - val_acc: 0.4699\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1117 - acc: 0.5499 - val_loss: 1.1318 - val_acc: 0.5181\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1048 - acc: 0.5741 - val_loss: 1.1320 - val_acc: 0.5060\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0964 - acc: 0.5687 - val_loss: 1.1338 - val_acc: 0.5060\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0887 - acc: 0.5782 - val_loss: 1.1214 - val_acc: 0.5060\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.5741 - val_loss: 1.1221 - val_acc: 0.5181\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0756 - acc: 0.5714 - val_loss: 1.1116 - val_acc: 0.5301\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0697 - acc: 0.5728 - val_loss: 1.1200 - val_acc: 0.5301\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0622 - acc: 0.5876 - val_loss: 1.1014 - val_acc: 0.5181\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0576 - acc: 0.6011 - val_loss: 1.1131 - val_acc: 0.5422\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0507 - acc: 0.5836 - val_loss: 1.0964 - val_acc: 0.5301\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.0456 - acc: 0.5876 - val_loss: 1.0961 - val_acc: 0.5060\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0386 - acc: 0.5930 - val_loss: 1.0973 - val_acc: 0.5181\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0330 - acc: 0.5970 - val_loss: 1.0836 - val_acc: 0.5181\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.0256 - acc: 0.5997 - val_loss: 1.0860 - val_acc: 0.5301\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0234 - acc: 0.6038 - val_loss: 1.0914 - val_acc: 0.5422\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0154 - acc: 0.6092 - val_loss: 1.0646 - val_acc: 0.5301\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0129 - acc: 0.6078 - val_loss: 1.0724 - val_acc: 0.5663\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0102 - acc: 0.5970 - val_loss: 1.0643 - val_acc: 0.5663\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0019 - acc: 0.6024 - val_loss: 1.0603 - val_acc: 0.5904\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9985 - acc: 0.6173 - val_loss: 1.0567 - val_acc: 0.5663\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9928 - acc: 0.6132 - val_loss: 1.0653 - val_acc: 0.5663\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9883 - acc: 0.6119 - val_loss: 1.0425 - val_acc: 0.5783\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9839 - acc: 0.6173 - val_loss: 1.0592 - val_acc: 0.5783\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9794 - acc: 0.6186 - val_loss: 1.0448 - val_acc: 0.6024\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9732 - acc: 0.6226 - val_loss: 1.0452 - val_acc: 0.5783\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9716 - acc: 0.6186 - val_loss: 1.0306 - val_acc: 0.5904\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9672 - acc: 0.6321 - val_loss: 1.0400 - val_acc: 0.6145\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9623 - acc: 0.6186 - val_loss: 1.0318 - val_acc: 0.6024\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9560 - acc: 0.6253 - val_loss: 1.0334 - val_acc: 0.6145\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9537 - acc: 0.6280 - val_loss: 1.0156 - val_acc: 0.6145\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9515 - acc: 0.6321 - val_loss: 1.0242 - val_acc: 0.5783\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9462 - acc: 0.6321 - val_loss: 1.0136 - val_acc: 0.6386\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9447 - acc: 0.6456 - val_loss: 1.0219 - val_acc: 0.6265\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.6334 - val_loss: 1.0084 - val_acc: 0.5904\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9346 - acc: 0.6361 - val_loss: 1.0186 - val_acc: 0.6145\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9317 - acc: 0.6402 - val_loss: 1.0075 - val_acc: 0.6506\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9324 - acc: 0.6388 - val_loss: 1.0076 - val_acc: 0.6024\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9230 - acc: 0.6456 - val_loss: 0.9959 - val_acc: 0.6506\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9190 - acc: 0.6456 - val_loss: 0.9919 - val_acc: 0.5904\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9172 - acc: 0.6388 - val_loss: 1.0019 - val_acc: 0.6386\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9152 - acc: 0.6482 - val_loss: 0.9919 - val_acc: 0.6265\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9117 - acc: 0.6509 - val_loss: 0.9841 - val_acc: 0.6386\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9067 - acc: 0.6402 - val_loss: 0.9848 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9051 - acc: 0.6523 - val_loss: 0.9888 - val_acc: 0.6627\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9036 - acc: 0.6550 - val_loss: 0.9874 - val_acc: 0.6265\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9011 - acc: 0.6563 - val_loss: 0.9720 - val_acc: 0.6386\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8997 - acc: 0.6415 - val_loss: 0.9769 - val_acc: 0.6386\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8990 - acc: 0.6509 - val_loss: 0.9691 - val_acc: 0.6506\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8919 - acc: 0.6442 - val_loss: 0.9662 - val_acc: 0.6265\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8894 - acc: 0.6577 - val_loss: 0.9742 - val_acc: 0.6627\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8866 - acc: 0.6577 - val_loss: 0.9602 - val_acc: 0.6506\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8810 - acc: 0.6617 - val_loss: 0.9685 - val_acc: 0.6506\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8780 - acc: 0.6496 - val_loss: 0.9535 - val_acc: 0.6265\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8764 - acc: 0.6671 - val_loss: 0.9551 - val_acc: 0.6506\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8719 - acc: 0.6631 - val_loss: 0.9603 - val_acc: 0.6627\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8690 - acc: 0.6590 - val_loss: 0.9496 - val_acc: 0.6265\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8663 - acc: 0.6712 - val_loss: 0.9515 - val_acc: 0.6627\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8652 - acc: 0.6658 - val_loss: 0.9489 - val_acc: 0.6867\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8616 - acc: 0.6739 - val_loss: 0.9406 - val_acc: 0.6506\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8594 - acc: 0.6752 - val_loss: 0.9475 - val_acc: 0.6265\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8579 - acc: 0.6563 - val_loss: 0.9462 - val_acc: 0.6867\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8575 - acc: 0.6752 - val_loss: 0.9398 - val_acc: 0.6747\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8520 - acc: 0.6671 - val_loss: 0.9350 - val_acc: 0.6747\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8496 - acc: 0.6765 - val_loss: 0.9367 - val_acc: 0.6386\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8488 - acc: 0.6860 - val_loss: 0.9314 - val_acc: 0.6627\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8441 - acc: 0.6712 - val_loss: 0.9382 - val_acc: 0.6747\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8422 - acc: 0.6860 - val_loss: 0.9216 - val_acc: 0.6627\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8400 - acc: 0.6846 - val_loss: 0.9265 - val_acc: 0.6627\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8371 - acc: 0.6833 - val_loss: 0.9272 - val_acc: 0.6747\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8349 - acc: 0.6806 - val_loss: 0.9240 - val_acc: 0.6627\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8310 - acc: 0.6792 - val_loss: 0.9172 - val_acc: 0.6627\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8305 - acc: 0.6900 - val_loss: 0.9168 - val_acc: 0.6747\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8292 - acc: 0.6833 - val_loss: 0.9257 - val_acc: 0.6867\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8281 - acc: 0.6765 - val_loss: 0.9128 - val_acc: 0.6747\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8244 - acc: 0.6981 - val_loss: 0.9188 - val_acc: 0.6506\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8217 - acc: 0.6914 - val_loss: 0.9088 - val_acc: 0.6747\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8233 - acc: 0.6779 - val_loss: 0.9218 - val_acc: 0.6988\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8154 - acc: 0.6981 - val_loss: 0.9090 - val_acc: 0.6627\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8165 - acc: 0.6914 - val_loss: 0.9058 - val_acc: 0.6627\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8146 - acc: 0.6927 - val_loss: 0.8989 - val_acc: 0.7108\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8157 - acc: 0.6914 - val_loss: 0.9000 - val_acc: 0.6627\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8113 - acc: 0.7022 - val_loss: 0.9136 - val_acc: 0.6988\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8064 - acc: 0.7062 - val_loss: 0.9007 - val_acc: 0.6747\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8058 - acc: 0.7008 - val_loss: 0.9055 - val_acc: 0.6867\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8050 - acc: 0.6968 - val_loss: 0.8928 - val_acc: 0.6747\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8026 - acc: 0.7022 - val_loss: 0.8959 - val_acc: 0.6506\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.8055 - acc: 0.6914 - val_loss: 0.8944 - val_acc: 0.6747\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7981 - acc: 0.6981 - val_loss: 0.8996 - val_acc: 0.6747\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7962 - acc: 0.7075 - val_loss: 0.8869 - val_acc: 0.6988\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7961 - acc: 0.6968 - val_loss: 0.8858 - val_acc: 0.6988\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7917 - acc: 0.7102 - val_loss: 0.8952 - val_acc: 0.6747\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7929 - acc: 0.7035 - val_loss: 0.8758 - val_acc: 0.7108\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7881 - acc: 0.7062 - val_loss: 0.8873 - val_acc: 0.6747\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7862 - acc: 0.7143 - val_loss: 0.8779 - val_acc: 0.6867\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7832 - acc: 0.7170 - val_loss: 0.8813 - val_acc: 0.6867\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7837 - acc: 0.7129 - val_loss: 0.8867 - val_acc: 0.6627\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7805 - acc: 0.7102 - val_loss: 0.8649 - val_acc: 0.7108\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7815 - acc: 0.7075 - val_loss: 0.8775 - val_acc: 0.6867\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7786 - acc: 0.7102 - val_loss: 0.8782 - val_acc: 0.6988\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7760 - acc: 0.7143 - val_loss: 0.8765 - val_acc: 0.6747\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7739 - acc: 0.7116 - val_loss: 0.8657 - val_acc: 0.7108\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7723 - acc: 0.7075 - val_loss: 0.8731 - val_acc: 0.6988\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7726 - acc: 0.7129 - val_loss: 0.8641 - val_acc: 0.7229\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7718 - acc: 0.7062 - val_loss: 0.8696 - val_acc: 0.6747\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7721 - acc: 0.7035 - val_loss: 0.8627 - val_acc: 0.7229\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7701 - acc: 0.7237 - val_loss: 0.8796 - val_acc: 0.6988\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7621 - acc: 0.7116 - val_loss: 0.8595 - val_acc: 0.6747\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7661 - acc: 0.7102 - val_loss: 0.8591 - val_acc: 0.6747\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7650 - acc: 0.7129 - val_loss: 0.8644 - val_acc: 0.6988\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7642 - acc: 0.7170 - val_loss: 0.8600 - val_acc: 0.7349\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7617 - acc: 0.7143 - val_loss: 0.8692 - val_acc: 0.6867\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7594 - acc: 0.7237 - val_loss: 0.8622 - val_acc: 0.6867\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7574 - acc: 0.7170 - val_loss: 0.8485 - val_acc: 0.7349\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7557 - acc: 0.7237 - val_loss: 0.8646 - val_acc: 0.6867\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7538 - acc: 0.7183 - val_loss: 0.8589 - val_acc: 0.6867\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7542 - acc: 0.7318 - val_loss: 0.8603 - val_acc: 0.6988\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 0.7495 - acc: 0.7237 - val_loss: 0.8637 - val_acc: 0.7108\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7511 - acc: 0.7156 - val_loss: 0.8521 - val_acc: 0.7349\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7490 - acc: 0.7129 - val_loss: 0.8468 - val_acc: 0.7108\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7481 - acc: 0.7170 - val_loss: 0.8514 - val_acc: 0.7349\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7474 - acc: 0.7129 - val_loss: 0.8592 - val_acc: 0.7108\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7431 - acc: 0.7197 - val_loss: 0.8519 - val_acc: 0.6747\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7434 - acc: 0.7251 - val_loss: 0.8521 - val_acc: 0.6988\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7414 - acc: 0.7251 - val_loss: 0.8525 - val_acc: 0.7349\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7402 - acc: 0.7251 - val_loss: 0.8542 - val_acc: 0.6867\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7419 - acc: 0.7210 - val_loss: 0.8486 - val_acc: 0.6988\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7419 - acc: 0.7183 - val_loss: 0.8514 - val_acc: 0.7229\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7386 - acc: 0.7183 - val_loss: 0.8590 - val_acc: 0.6988\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7349 - acc: 0.7224 - val_loss: 0.8473 - val_acc: 0.7470\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7321 - acc: 0.7264 - val_loss: 0.8458 - val_acc: 0.7108\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7336 - acc: 0.7264 - val_loss: 0.8579 - val_acc: 0.7108\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7308 - acc: 0.7291 - val_loss: 0.8422 - val_acc: 0.7108\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7304 - acc: 0.7224 - val_loss: 0.8501 - val_acc: 0.6867\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7307 - acc: 0.7210 - val_loss: 0.8482 - val_acc: 0.7349\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7275 - acc: 0.7197 - val_loss: 0.8434 - val_acc: 0.7470\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7249 - acc: 0.7264 - val_loss: 0.8503 - val_acc: 0.7229\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7245 - acc: 0.7305 - val_loss: 0.8416 - val_acc: 0.7229\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7236 - acc: 0.7278 - val_loss: 0.8425 - val_acc: 0.7470\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7229 - acc: 0.7318 - val_loss: 0.8458 - val_acc: 0.7349\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.7224 - acc: 0.7278 - val_loss: 0.8454 - val_acc: 0.7108\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7195 - acc: 0.7278 - val_loss: 0.8489 - val_acc: 0.7349\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7184 - acc: 0.7251 - val_loss: 0.8331 - val_acc: 0.7349\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7159 - acc: 0.7278 - val_loss: 0.8411 - val_acc: 0.7349\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7157 - acc: 0.7278 - val_loss: 0.8381 - val_acc: 0.7349\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3909 - acc: 0.1995 - val_loss: 2.3771 - val_acc: 0.2530\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 2.3660 - acc: 0.2399 - val_loss: 2.3405 - val_acc: 0.2771\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.3215 - acc: 0.2466 - val_loss: 2.2736 - val_acc: 0.3133\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 2.2419 - acc: 0.2480 - val_loss: 2.1589 - val_acc: 0.3253\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.1201 - acc: 0.2507 - val_loss: 2.0031 - val_acc: 0.2892\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.9807 - acc: 0.2534 - val_loss: 1.8597 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.8796 - acc: 0.2561 - val_loss: 1.7615 - val_acc: 0.3373\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.8126 - acc: 0.2817 - val_loss: 1.6979 - val_acc: 0.3373\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.7549 - acc: 0.2871 - val_loss: 1.6460 - val_acc: 0.3614\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.7024 - acc: 0.3019 - val_loss: 1.6016 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.6597 - acc: 0.3181 - val_loss: 1.5749 - val_acc: 0.3253\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6209 - acc: 0.3275 - val_loss: 1.5230 - val_acc: 0.3735\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.5862 - acc: 0.3423 - val_loss: 1.5001 - val_acc: 0.3253\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.5573 - acc: 0.3396 - val_loss: 1.4704 - val_acc: 0.3494\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.5301 - acc: 0.3491 - val_loss: 1.4496 - val_acc: 0.3494\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.5067 - acc: 0.3652 - val_loss: 1.4149 - val_acc: 0.3735\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4838 - acc: 0.4016 - val_loss: 1.4024 - val_acc: 0.3494\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.4631 - acc: 0.4084 - val_loss: 1.3733 - val_acc: 0.3976\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.4416 - acc: 0.4097 - val_loss: 1.3606 - val_acc: 0.3855\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.4239 - acc: 0.4218 - val_loss: 1.3530 - val_acc: 0.4217\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4040 - acc: 0.4245 - val_loss: 1.3347 - val_acc: 0.4337\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3857 - acc: 0.4501 - val_loss: 1.3145 - val_acc: 0.4217\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3703 - acc: 0.4447 - val_loss: 1.3060 - val_acc: 0.4578\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3546 - acc: 0.4690 - val_loss: 1.2973 - val_acc: 0.4699\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3368 - acc: 0.4569 - val_loss: 1.2845 - val_acc: 0.4337\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.3199 - acc: 0.4825 - val_loss: 1.2754 - val_acc: 0.4458\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.3047 - acc: 0.4838 - val_loss: 1.2644 - val_acc: 0.4458\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.2899 - acc: 0.4838 - val_loss: 1.2483 - val_acc: 0.4458\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.2749 - acc: 0.4960 - val_loss: 1.2392 - val_acc: 0.4458\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2608 - acc: 0.4946 - val_loss: 1.2325 - val_acc: 0.4940\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2450 - acc: 0.5054 - val_loss: 1.2312 - val_acc: 0.4458\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.2285 - acc: 0.5054 - val_loss: 1.2141 - val_acc: 0.4458\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2166 - acc: 0.5121 - val_loss: 1.2014 - val_acc: 0.4458\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2037 - acc: 0.5270 - val_loss: 1.1959 - val_acc: 0.4337\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1902 - acc: 0.5364 - val_loss: 1.1920 - val_acc: 0.4578\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1751 - acc: 0.5364 - val_loss: 1.1689 - val_acc: 0.5060\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1655 - acc: 0.5391 - val_loss: 1.1780 - val_acc: 0.4699\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1522 - acc: 0.5323 - val_loss: 1.1658 - val_acc: 0.4458\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1390 - acc: 0.5283 - val_loss: 1.1483 - val_acc: 0.4819\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1286 - acc: 0.5606 - val_loss: 1.1507 - val_acc: 0.5301\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1199 - acc: 0.5728 - val_loss: 1.1432 - val_acc: 0.5060\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1100 - acc: 0.5593 - val_loss: 1.1355 - val_acc: 0.5542\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0991 - acc: 0.5566 - val_loss: 1.1419 - val_acc: 0.5301\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0902 - acc: 0.5755 - val_loss: 1.1194 - val_acc: 0.5542\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0821 - acc: 0.5687 - val_loss: 1.1099 - val_acc: 0.5422\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0692 - acc: 0.5714 - val_loss: 1.1114 - val_acc: 0.5542\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0614 - acc: 0.5674 - val_loss: 1.1107 - val_acc: 0.5422\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0507 - acc: 0.5809 - val_loss: 1.0899 - val_acc: 0.5542\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0455 - acc: 0.5876 - val_loss: 1.0858 - val_acc: 0.5663\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0394 - acc: 0.5741 - val_loss: 1.1020 - val_acc: 0.4940\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0249 - acc: 0.5957 - val_loss: 1.0704 - val_acc: 0.5663\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0210 - acc: 0.6146 - val_loss: 1.0711 - val_acc: 0.6145\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0127 - acc: 0.5984 - val_loss: 1.0839 - val_acc: 0.5542\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0045 - acc: 0.6011 - val_loss: 1.0705 - val_acc: 0.5904\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.0000 - acc: 0.6065 - val_loss: 1.0572 - val_acc: 0.6145\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9938 - acc: 0.6024 - val_loss: 1.0635 - val_acc: 0.5783\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9870 - acc: 0.6186 - val_loss: 1.0463 - val_acc: 0.6145\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9820 - acc: 0.6186 - val_loss: 1.0550 - val_acc: 0.6024\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9752 - acc: 0.6199 - val_loss: 1.0527 - val_acc: 0.5904\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.9687 - acc: 0.6280 - val_loss: 1.0341 - val_acc: 0.5904\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.9623 - acc: 0.6348 - val_loss: 1.0438 - val_acc: 0.6265\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9581 - acc: 0.6321 - val_loss: 1.0315 - val_acc: 0.6145\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9528 - acc: 0.6415 - val_loss: 1.0272 - val_acc: 0.6386\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9489 - acc: 0.6280 - val_loss: 1.0257 - val_acc: 0.5783\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9458 - acc: 0.6536 - val_loss: 1.0091 - val_acc: 0.6145\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9395 - acc: 0.6334 - val_loss: 1.0223 - val_acc: 0.6265\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9347 - acc: 0.6375 - val_loss: 1.0046 - val_acc: 0.6265\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9290 - acc: 0.6456 - val_loss: 1.0116 - val_acc: 0.6265\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9250 - acc: 0.6361 - val_loss: 1.0043 - val_acc: 0.6506\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9232 - acc: 0.6429 - val_loss: 0.9869 - val_acc: 0.6747\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9140 - acc: 0.6550 - val_loss: 0.9972 - val_acc: 0.6386\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9122 - acc: 0.6469 - val_loss: 0.9956 - val_acc: 0.6386\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9060 - acc: 0.6509 - val_loss: 0.9752 - val_acc: 0.6386\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9035 - acc: 0.6509 - val_loss: 0.9849 - val_acc: 0.6627\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9019 - acc: 0.6590 - val_loss: 0.9848 - val_acc: 0.6627\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8971 - acc: 0.6604 - val_loss: 0.9858 - val_acc: 0.6506\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8965 - acc: 0.6523 - val_loss: 0.9655 - val_acc: 0.6265\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8914 - acc: 0.6604 - val_loss: 0.9624 - val_acc: 0.6506\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8872 - acc: 0.6765 - val_loss: 0.9624 - val_acc: 0.6747\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8897 - acc: 0.6550 - val_loss: 0.9715 - val_acc: 0.6867\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8773 - acc: 0.6617 - val_loss: 0.9546 - val_acc: 0.6386\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8754 - acc: 0.6698 - val_loss: 0.9588 - val_acc: 0.6747\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.6577 - val_loss: 0.9568 - val_acc: 0.6506\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8724 - acc: 0.6617 - val_loss: 0.9420 - val_acc: 0.6506\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8686 - acc: 0.6577 - val_loss: 0.9571 - val_acc: 0.6627\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8625 - acc: 0.6765 - val_loss: 0.9409 - val_acc: 0.6386\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8607 - acc: 0.6712 - val_loss: 0.9406 - val_acc: 0.6867\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8561 - acc: 0.6792 - val_loss: 0.9426 - val_acc: 0.6627\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8531 - acc: 0.6752 - val_loss: 0.9372 - val_acc: 0.6627\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8529 - acc: 0.6792 - val_loss: 0.9410 - val_acc: 0.6867\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8489 - acc: 0.6779 - val_loss: 0.9261 - val_acc: 0.6506\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8491 - acc: 0.6739 - val_loss: 0.9449 - val_acc: 0.6506\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8479 - acc: 0.6819 - val_loss: 0.9193 - val_acc: 0.6386\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 0.8453 - acc: 0.6577 - val_loss: 0.9384 - val_acc: 0.6627\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8390 - acc: 0.6873 - val_loss: 0.9271 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8381 - acc: 0.6739 - val_loss: 0.9227 - val_acc: 0.6867\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.6941 - val_loss: 0.9227 - val_acc: 0.6988\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8297 - acc: 0.6914 - val_loss: 0.9094 - val_acc: 0.7108\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8290 - acc: 0.6860 - val_loss: 0.9132 - val_acc: 0.6747\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8261 - acc: 0.6941 - val_loss: 0.9260 - val_acc: 0.6627\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8228 - acc: 0.6941 - val_loss: 0.9108 - val_acc: 0.6867\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8202 - acc: 0.6819 - val_loss: 0.9070 - val_acc: 0.6988\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8193 - acc: 0.6954 - val_loss: 0.9076 - val_acc: 0.6867\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8182 - acc: 0.6873 - val_loss: 0.9170 - val_acc: 0.6747\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8163 - acc: 0.7116 - val_loss: 0.8982 - val_acc: 0.6867\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8113 - acc: 0.6981 - val_loss: 0.9137 - val_acc: 0.6747\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8073 - acc: 0.7062 - val_loss: 0.8883 - val_acc: 0.6988\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 60us/step - loss: 0.8053 - acc: 0.7035 - val_loss: 0.8957 - val_acc: 0.6867\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8028 - acc: 0.7008 - val_loss: 0.8946 - val_acc: 0.6627\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.8025 - acc: 0.7102 - val_loss: 0.8976 - val_acc: 0.6988\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8037 - acc: 0.6981 - val_loss: 0.8856 - val_acc: 0.6988\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7995 - acc: 0.6995 - val_loss: 0.8985 - val_acc: 0.6867\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8041 - acc: 0.7035 - val_loss: 0.8877 - val_acc: 0.6988\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8018 - acc: 0.6873 - val_loss: 0.8974 - val_acc: 0.6988\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7965 - acc: 0.7143 - val_loss: 0.8811 - val_acc: 0.6747\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7937 - acc: 0.6995 - val_loss: 0.9003 - val_acc: 0.6867\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7908 - acc: 0.7129 - val_loss: 0.8746 - val_acc: 0.6988\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7882 - acc: 0.7143 - val_loss: 0.8831 - val_acc: 0.6747\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7865 - acc: 0.7102 - val_loss: 0.8788 - val_acc: 0.6988\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7841 - acc: 0.7102 - val_loss: 0.8740 - val_acc: 0.6867\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7841 - acc: 0.7049 - val_loss: 0.8741 - val_acc: 0.6988\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7809 - acc: 0.7156 - val_loss: 0.8712 - val_acc: 0.6988\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7803 - acc: 0.7183 - val_loss: 0.8701 - val_acc: 0.7108\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7794 - acc: 0.7129 - val_loss: 0.8745 - val_acc: 0.7108\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7763 - acc: 0.7089 - val_loss: 0.8843 - val_acc: 0.6747\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7705 - acc: 0.7224 - val_loss: 0.8613 - val_acc: 0.7108\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7723 - acc: 0.7170 - val_loss: 0.8685 - val_acc: 0.7229\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7722 - acc: 0.7143 - val_loss: 0.8645 - val_acc: 0.7108\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7731 - acc: 0.7156 - val_loss: 0.8682 - val_acc: 0.6867\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7753 - acc: 0.7129 - val_loss: 0.8780 - val_acc: 0.6627\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7658 - acc: 0.7210 - val_loss: 0.8543 - val_acc: 0.6867\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7686 - acc: 0.7116 - val_loss: 0.8829 - val_acc: 0.6627\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7649 - acc: 0.7210 - val_loss: 0.8577 - val_acc: 0.7229\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7605 - acc: 0.7251 - val_loss: 0.8658 - val_acc: 0.7108\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7584 - acc: 0.7291 - val_loss: 0.8626 - val_acc: 0.6867\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7593 - acc: 0.7210 - val_loss: 0.8499 - val_acc: 0.7108\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7561 - acc: 0.7278 - val_loss: 0.8634 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7538 - acc: 0.7332 - val_loss: 0.8532 - val_acc: 0.7108\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7558 - acc: 0.7224 - val_loss: 0.8577 - val_acc: 0.7229\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7506 - acc: 0.7345 - val_loss: 0.8469 - val_acc: 0.6867\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7533 - acc: 0.7251 - val_loss: 0.8551 - val_acc: 0.6988\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7461 - acc: 0.7305 - val_loss: 0.8462 - val_acc: 0.7229\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7491 - acc: 0.7170 - val_loss: 0.8591 - val_acc: 0.6867\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7479 - acc: 0.7237 - val_loss: 0.8426 - val_acc: 0.7108\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7437 - acc: 0.7358 - val_loss: 0.8428 - val_acc: 0.7108\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7468 - acc: 0.7278 - val_loss: 0.8548 - val_acc: 0.7108\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7415 - acc: 0.7251 - val_loss: 0.8354 - val_acc: 0.7470\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7403 - acc: 0.7251 - val_loss: 0.8422 - val_acc: 0.7470\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7412 - acc: 0.7345 - val_loss: 0.8461 - val_acc: 0.7108\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7370 - acc: 0.7332 - val_loss: 0.8398 - val_acc: 0.7229\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7360 - acc: 0.7305 - val_loss: 0.8455 - val_acc: 0.6988\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7375 - acc: 0.7278 - val_loss: 0.8328 - val_acc: 0.7229\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7383 - acc: 0.7372 - val_loss: 0.8420 - val_acc: 0.7108\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7308 - acc: 0.7412 - val_loss: 0.8373 - val_acc: 0.7349\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7311 - acc: 0.7210 - val_loss: 0.8431 - val_acc: 0.7470\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7310 - acc: 0.7372 - val_loss: 0.8313 - val_acc: 0.7108\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7335 - acc: 0.7278 - val_loss: 0.8293 - val_acc: 0.7108\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7277 - acc: 0.7358 - val_loss: 0.8291 - val_acc: 0.7349\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7253 - acc: 0.7291 - val_loss: 0.8332 - val_acc: 0.7229\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7258 - acc: 0.7426 - val_loss: 0.8231 - val_acc: 0.6988\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7243 - acc: 0.7210 - val_loss: 0.8432 - val_acc: 0.7108\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7244 - acc: 0.7358 - val_loss: 0.8240 - val_acc: 0.7229\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.7218 - acc: 0.7439 - val_loss: 0.8338 - val_acc: 0.7229\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7223 - acc: 0.7197 - val_loss: 0.8201 - val_acc: 0.7229\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7248 - acc: 0.7399 - val_loss: 0.8324 - val_acc: 0.7229\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7191 - acc: 0.7318 - val_loss: 0.8160 - val_acc: 0.7470\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7169 - acc: 0.7318 - val_loss: 0.8180 - val_acc: 0.7349\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7164 - acc: 0.7399 - val_loss: 0.8293 - val_acc: 0.6988\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7174 - acc: 0.7358 - val_loss: 0.8139 - val_acc: 0.7108\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7157 - acc: 0.7358 - val_loss: 0.8442 - val_acc: 0.6988\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7162 - acc: 0.7358 - val_loss: 0.8070 - val_acc: 0.7108\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7122 - acc: 0.7453 - val_loss: 0.8328 - val_acc: 0.6988\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7112 - acc: 0.7358 - val_loss: 0.8190 - val_acc: 0.7229\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7081 - acc: 0.7345 - val_loss: 0.8110 - val_acc: 0.7470\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7103 - acc: 0.7372 - val_loss: 0.8321 - val_acc: 0.7229\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7061 - acc: 0.7480 - val_loss: 0.8108 - val_acc: 0.7108\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7066 - acc: 0.7385 - val_loss: 0.8166 - val_acc: 0.7590\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7038 - acc: 0.7561 - val_loss: 0.8185 - val_acc: 0.7229\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7042 - acc: 0.7399 - val_loss: 0.8113 - val_acc: 0.7229\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7062 - acc: 0.7385 - val_loss: 0.7998 - val_acc: 0.7349\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7003 - acc: 0.7345 - val_loss: 0.8188 - val_acc: 0.7229\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7001 - acc: 0.7453 - val_loss: 0.8129 - val_acc: 0.7470\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7001 - acc: 0.7493 - val_loss: 0.7940 - val_acc: 0.7590\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7014 - acc: 0.7372 - val_loss: 0.8124 - val_acc: 0.7229\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6964 - acc: 0.7399 - val_loss: 0.8046 - val_acc: 0.7470\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6976 - acc: 0.7520 - val_loss: 0.8005 - val_acc: 0.7349\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6941 - acc: 0.7480 - val_loss: 0.8056 - val_acc: 0.7590\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6920 - acc: 0.7453 - val_loss: 0.7971 - val_acc: 0.7590\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6903 - acc: 0.7534 - val_loss: 0.7962 - val_acc: 0.7470\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6962 - acc: 0.7385 - val_loss: 0.8082 - val_acc: 0.7229\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6941 - acc: 0.7439 - val_loss: 0.7911 - val_acc: 0.7470\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6915 - acc: 0.7561 - val_loss: 0.7967 - val_acc: 0.7470\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6896 - acc: 0.7439 - val_loss: 0.8022 - val_acc: 0.7349\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6879 - acc: 0.7547 - val_loss: 0.7903 - val_acc: 0.7470\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6875 - acc: 0.7561 - val_loss: 0.8043 - val_acc: 0.7349\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6831 - acc: 0.7358 - val_loss: 0.7835 - val_acc: 0.7470\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6867 - acc: 0.7561 - val_loss: 0.8036 - val_acc: 0.7470\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6870 - acc: 0.7507 - val_loss: 0.7909 - val_acc: 0.7349\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6852 - acc: 0.7385 - val_loss: 0.7916 - val_acc: 0.7349\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6876 - acc: 0.7480 - val_loss: 0.7898 - val_acc: 0.7470\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3818 - acc: 0.2480 - val_loss: 2.3579 - val_acc: 0.3133\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.3330 - acc: 0.2615 - val_loss: 2.2824 - val_acc: 0.3373\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 2.2387 - acc: 0.2547 - val_loss: 2.1474 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 2.0996 - acc: 0.2547 - val_loss: 1.9653 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.9513 - acc: 0.2547 - val_loss: 1.8247 - val_acc: 0.3373\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.8686 - acc: 0.2574 - val_loss: 1.7602 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8113 - acc: 0.2790 - val_loss: 1.7283 - val_acc: 0.3133\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.7647 - acc: 0.2790 - val_loss: 1.6986 - val_acc: 0.2651\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.7196 - acc: 0.2871 - val_loss: 1.6480 - val_acc: 0.3012\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.6791 - acc: 0.2992 - val_loss: 1.6232 - val_acc: 0.3253\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.6388 - acc: 0.3005 - val_loss: 1.5832 - val_acc: 0.3133\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6015 - acc: 0.3100 - val_loss: 1.5352 - val_acc: 0.3494\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5651 - acc: 0.3571 - val_loss: 1.5008 - val_acc: 0.3976\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.5314 - acc: 0.3693 - val_loss: 1.4662 - val_acc: 0.3855\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.4991 - acc: 0.3868 - val_loss: 1.4304 - val_acc: 0.4217\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.4711 - acc: 0.4030 - val_loss: 1.3884 - val_acc: 0.4096\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4463 - acc: 0.4218 - val_loss: 1.3746 - val_acc: 0.4096\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4221 - acc: 0.4313 - val_loss: 1.3479 - val_acc: 0.3855\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.3973 - acc: 0.4407 - val_loss: 1.3178 - val_acc: 0.4217\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3752 - acc: 0.4542 - val_loss: 1.3037 - val_acc: 0.4217\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3536 - acc: 0.4488 - val_loss: 1.2880 - val_acc: 0.4458\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.3324 - acc: 0.4704 - val_loss: 1.2675 - val_acc: 0.4458\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3123 - acc: 0.4838 - val_loss: 1.2541 - val_acc: 0.4458\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.2934 - acc: 0.4879 - val_loss: 1.2415 - val_acc: 0.4458\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2755 - acc: 0.4892 - val_loss: 1.2170 - val_acc: 0.4819\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2609 - acc: 0.4946 - val_loss: 1.2193 - val_acc: 0.4217\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2424 - acc: 0.5148 - val_loss: 1.1941 - val_acc: 0.4578\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2256 - acc: 0.5256 - val_loss: 1.1971 - val_acc: 0.4819\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2134 - acc: 0.5364 - val_loss: 1.1657 - val_acc: 0.4819\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1969 - acc: 0.5189 - val_loss: 1.1785 - val_acc: 0.4699\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1795 - acc: 0.5310 - val_loss: 1.1574 - val_acc: 0.4458\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1610 - acc: 0.5553 - val_loss: 1.1441 - val_acc: 0.5301\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1461 - acc: 0.5512 - val_loss: 1.1399 - val_acc: 0.5181\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1314 - acc: 0.5660 - val_loss: 1.1300 - val_acc: 0.4940\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1173 - acc: 0.5674 - val_loss: 1.1177 - val_acc: 0.5663\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1075 - acc: 0.5822 - val_loss: 1.1160 - val_acc: 0.5301\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0935 - acc: 0.5633 - val_loss: 1.1047 - val_acc: 0.5181\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0801 - acc: 0.5849 - val_loss: 1.0954 - val_acc: 0.5542\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0683 - acc: 0.5849 - val_loss: 1.0793 - val_acc: 0.5542\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0551 - acc: 0.5863 - val_loss: 1.0822 - val_acc: 0.5422\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0442 - acc: 0.6024 - val_loss: 1.0619 - val_acc: 0.5301\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0355 - acc: 0.6051 - val_loss: 1.0632 - val_acc: 0.5542\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0299 - acc: 0.5943 - val_loss: 1.0578 - val_acc: 0.6024\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0140 - acc: 0.6024 - val_loss: 1.0460 - val_acc: 0.5301\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0063 - acc: 0.6119 - val_loss: 1.0446 - val_acc: 0.5904\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9991 - acc: 0.6092 - val_loss: 1.0358 - val_acc: 0.5663\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9911 - acc: 0.6213 - val_loss: 1.0178 - val_acc: 0.5904\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9835 - acc: 0.6213 - val_loss: 1.0128 - val_acc: 0.5542\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9739 - acc: 0.6092 - val_loss: 1.0232 - val_acc: 0.5663\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9682 - acc: 0.6226 - val_loss: 1.0064 - val_acc: 0.5783\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9611 - acc: 0.6213 - val_loss: 0.9968 - val_acc: 0.6145\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9513 - acc: 0.6375 - val_loss: 1.0119 - val_acc: 0.5663\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9422 - acc: 0.6482 - val_loss: 0.9891 - val_acc: 0.6024\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9374 - acc: 0.6482 - val_loss: 0.9816 - val_acc: 0.5783\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9311 - acc: 0.6456 - val_loss: 0.9752 - val_acc: 0.6145\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9264 - acc: 0.6496 - val_loss: 0.9801 - val_acc: 0.5663\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9198 - acc: 0.6375 - val_loss: 0.9716 - val_acc: 0.5904\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9114 - acc: 0.6685 - val_loss: 0.9512 - val_acc: 0.5904\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.9045 - acc: 0.6765 - val_loss: 0.9607 - val_acc: 0.6024\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 67us/step - loss: 0.9020 - acc: 0.6482 - val_loss: 0.9618 - val_acc: 0.6145\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8951 - acc: 0.6644 - val_loss: 0.9384 - val_acc: 0.6145\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8870 - acc: 0.6900 - val_loss: 0.9455 - val_acc: 0.6024\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8827 - acc: 0.6685 - val_loss: 0.9430 - val_acc: 0.6024\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8792 - acc: 0.6671 - val_loss: 0.9302 - val_acc: 0.6024\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8740 - acc: 0.6779 - val_loss: 0.9280 - val_acc: 0.6145\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8685 - acc: 0.6860 - val_loss: 0.9204 - val_acc: 0.5904\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8623 - acc: 0.6739 - val_loss: 0.9378 - val_acc: 0.5904\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8617 - acc: 0.6712 - val_loss: 0.9095 - val_acc: 0.6145\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8551 - acc: 0.6739 - val_loss: 0.9252 - val_acc: 0.6024\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8472 - acc: 0.6846 - val_loss: 0.9031 - val_acc: 0.6024\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8452 - acc: 0.6981 - val_loss: 0.9072 - val_acc: 0.6024\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8371 - acc: 0.7008 - val_loss: 0.9013 - val_acc: 0.6024\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8348 - acc: 0.6954 - val_loss: 0.9017 - val_acc: 0.5904\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8307 - acc: 0.6887 - val_loss: 0.8948 - val_acc: 0.6145\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8285 - acc: 0.7008 - val_loss: 0.8939 - val_acc: 0.6265\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8283 - acc: 0.6968 - val_loss: 0.8927 - val_acc: 0.6145\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8177 - acc: 0.7022 - val_loss: 0.8922 - val_acc: 0.6506\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8139 - acc: 0.7035 - val_loss: 0.8855 - val_acc: 0.6265\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8126 - acc: 0.7129 - val_loss: 0.8963 - val_acc: 0.6265\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8049 - acc: 0.7008 - val_loss: 0.8703 - val_acc: 0.6145\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8045 - acc: 0.7089 - val_loss: 0.9060 - val_acc: 0.6024\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8061 - acc: 0.7008 - val_loss: 0.8750 - val_acc: 0.6386\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7990 - acc: 0.7062 - val_loss: 0.8807 - val_acc: 0.6024\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7925 - acc: 0.7049 - val_loss: 0.8816 - val_acc: 0.6386\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7873 - acc: 0.7170 - val_loss: 0.8692 - val_acc: 0.6265\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7855 - acc: 0.7170 - val_loss: 0.8749 - val_acc: 0.6265\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7850 - acc: 0.6995 - val_loss: 0.8703 - val_acc: 0.6386\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7794 - acc: 0.7143 - val_loss: 0.8770 - val_acc: 0.6386\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7816 - acc: 0.7156 - val_loss: 0.8680 - val_acc: 0.6386\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7738 - acc: 0.7237 - val_loss: 0.8693 - val_acc: 0.6386\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7711 - acc: 0.7291 - val_loss: 0.8750 - val_acc: 0.6145\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7756 - acc: 0.7062 - val_loss: 0.8595 - val_acc: 0.6265\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7699 - acc: 0.7062 - val_loss: 0.8684 - val_acc: 0.6506\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7696 - acc: 0.7089 - val_loss: 0.8598 - val_acc: 0.6145\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7604 - acc: 0.7129 - val_loss: 0.8620 - val_acc: 0.6265\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7602 - acc: 0.7129 - val_loss: 0.8612 - val_acc: 0.6506\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7571 - acc: 0.7075 - val_loss: 0.8570 - val_acc: 0.6506\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.7550 - acc: 0.7224 - val_loss: 0.8598 - val_acc: 0.6265\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7493 - acc: 0.7345 - val_loss: 0.8578 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7456 - acc: 0.7345 - val_loss: 0.8407 - val_acc: 0.6386\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7443 - acc: 0.7224 - val_loss: 0.8749 - val_acc: 0.6265\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7447 - acc: 0.7264 - val_loss: 0.8468 - val_acc: 0.6506\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7388 - acc: 0.7264 - val_loss: 0.8556 - val_acc: 0.6867\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7398 - acc: 0.7278 - val_loss: 0.8527 - val_acc: 0.6747\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.7374 - acc: 0.7156 - val_loss: 0.8390 - val_acc: 0.6265\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7315 - acc: 0.7291 - val_loss: 0.8614 - val_acc: 0.6747\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7313 - acc: 0.7251 - val_loss: 0.8593 - val_acc: 0.6265\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7324 - acc: 0.7251 - val_loss: 0.8539 - val_acc: 0.6747\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7270 - acc: 0.7291 - val_loss: 0.8535 - val_acc: 0.6506\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7237 - acc: 0.7332 - val_loss: 0.8462 - val_acc: 0.6506\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7241 - acc: 0.7129 - val_loss: 0.8774 - val_acc: 0.6506\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7243 - acc: 0.7278 - val_loss: 0.8409 - val_acc: 0.6024\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7167 - acc: 0.7251 - val_loss: 0.8561 - val_acc: 0.6506\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7207 - acc: 0.7116 - val_loss: 0.8652 - val_acc: 0.6627\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7196 - acc: 0.7318 - val_loss: 0.8277 - val_acc: 0.6506\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7206 - acc: 0.7102 - val_loss: 0.8737 - val_acc: 0.6386\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7142 - acc: 0.7305 - val_loss: 0.8460 - val_acc: 0.6386\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7146 - acc: 0.7291 - val_loss: 0.8443 - val_acc: 0.6627\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7059 - acc: 0.7453 - val_loss: 0.8561 - val_acc: 0.6506\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7070 - acc: 0.7345 - val_loss: 0.8521 - val_acc: 0.6265\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7023 - acc: 0.7291 - val_loss: 0.8478 - val_acc: 0.6747\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6981 - acc: 0.7493 - val_loss: 0.8508 - val_acc: 0.6867\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6993 - acc: 0.7426 - val_loss: 0.8483 - val_acc: 0.6386\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6980 - acc: 0.7385 - val_loss: 0.8351 - val_acc: 0.6988\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6994 - acc: 0.7332 - val_loss: 0.8551 - val_acc: 0.6627\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7053 - acc: 0.7291 - val_loss: 0.8462 - val_acc: 0.6506\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7003 - acc: 0.7237 - val_loss: 0.8549 - val_acc: 0.6386\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6936 - acc: 0.7251 - val_loss: 0.8552 - val_acc: 0.6506\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6959 - acc: 0.7278 - val_loss: 0.8584 - val_acc: 0.6627\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6920 - acc: 0.7466 - val_loss: 0.8406 - val_acc: 0.6386\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6895 - acc: 0.7399 - val_loss: 0.8368 - val_acc: 0.6506\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6838 - acc: 0.7358 - val_loss: 0.8348 - val_acc: 0.6867\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6827 - acc: 0.7466 - val_loss: 0.8521 - val_acc: 0.6506\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6820 - acc: 0.7305 - val_loss: 0.8413 - val_acc: 0.6386\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6803 - acc: 0.7439 - val_loss: 0.8716 - val_acc: 0.6627\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6822 - acc: 0.7345 - val_loss: 0.8402 - val_acc: 0.6386\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6829 - acc: 0.7332 - val_loss: 0.8464 - val_acc: 0.6506\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6782 - acc: 0.7412 - val_loss: 0.8489 - val_acc: 0.6386\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6757 - acc: 0.7439 - val_loss: 0.8647 - val_acc: 0.6506\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6769 - acc: 0.7372 - val_loss: 0.8490 - val_acc: 0.6386\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6771 - acc: 0.7385 - val_loss: 0.8550 - val_acc: 0.6506\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6718 - acc: 0.7358 - val_loss: 0.8573 - val_acc: 0.6627\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6732 - acc: 0.7439 - val_loss: 0.8363 - val_acc: 0.6506\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6740 - acc: 0.7358 - val_loss: 0.8663 - val_acc: 0.6627\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6726 - acc: 0.7318 - val_loss: 0.8541 - val_acc: 0.6506\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6693 - acc: 0.7372 - val_loss: 0.8468 - val_acc: 0.6265\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6708 - acc: 0.7372 - val_loss: 0.8790 - val_acc: 0.6506\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6752 - acc: 0.7399 - val_loss: 0.8435 - val_acc: 0.6627\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6668 - acc: 0.7345 - val_loss: 0.8543 - val_acc: 0.6506\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6621 - acc: 0.7385 - val_loss: 0.8515 - val_acc: 0.6506\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6596 - acc: 0.7385 - val_loss: 0.8558 - val_acc: 0.6506\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6592 - acc: 0.7426 - val_loss: 0.8436 - val_acc: 0.6386\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6609 - acc: 0.7493 - val_loss: 0.8537 - val_acc: 0.6506\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6597 - acc: 0.7412 - val_loss: 0.8599 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6623 - acc: 0.7453 - val_loss: 0.8836 - val_acc: 0.6506\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6558 - acc: 0.7547 - val_loss: 0.8435 - val_acc: 0.6386\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6544 - acc: 0.7480 - val_loss: 0.8453 - val_acc: 0.6506\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6518 - acc: 0.7534 - val_loss: 0.8346 - val_acc: 0.6265\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6527 - acc: 0.7439 - val_loss: 0.8478 - val_acc: 0.6386\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6530 - acc: 0.7426 - val_loss: 0.8622 - val_acc: 0.6506\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6546 - acc: 0.7453 - val_loss: 0.8427 - val_acc: 0.6386\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6489 - acc: 0.7412 - val_loss: 0.8962 - val_acc: 0.6627\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6572 - acc: 0.7439 - val_loss: 0.8396 - val_acc: 0.6506\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6527 - acc: 0.7426 - val_loss: 0.8452 - val_acc: 0.6506\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6509 - acc: 0.7493 - val_loss: 0.8497 - val_acc: 0.6265\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6530 - acc: 0.7412 - val_loss: 0.8735 - val_acc: 0.6627\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6470 - acc: 0.7588 - val_loss: 0.8524 - val_acc: 0.6506\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6440 - acc: 0.7466 - val_loss: 0.8596 - val_acc: 0.6506\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6407 - acc: 0.7588 - val_loss: 0.8379 - val_acc: 0.6386\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6410 - acc: 0.7561 - val_loss: 0.8492 - val_acc: 0.6747\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6401 - acc: 0.7601 - val_loss: 0.8511 - val_acc: 0.6506\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6400 - acc: 0.7547 - val_loss: 0.8521 - val_acc: 0.6506\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6421 - acc: 0.7574 - val_loss: 0.8504 - val_acc: 0.6506\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6431 - acc: 0.7426 - val_loss: 0.8581 - val_acc: 0.6265\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.6416 - acc: 0.7520 - val_loss: 0.8505 - val_acc: 0.6627\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6429 - acc: 0.7426 - val_loss: 0.8823 - val_acc: 0.6265\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6331 - acc: 0.7601 - val_loss: 0.8413 - val_acc: 0.6747\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6412 - acc: 0.7466 - val_loss: 0.8661 - val_acc: 0.6627\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6331 - acc: 0.7493 - val_loss: 0.8557 - val_acc: 0.6506\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6325 - acc: 0.7520 - val_loss: 0.8419 - val_acc: 0.6747\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6364 - acc: 0.7588 - val_loss: 0.8559 - val_acc: 0.6386\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6293 - acc: 0.7493 - val_loss: 0.8678 - val_acc: 0.6627\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6329 - acc: 0.7615 - val_loss: 0.8449 - val_acc: 0.6506\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6400 - acc: 0.7493 - val_loss: 0.8567 - val_acc: 0.6506\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6381 - acc: 0.7439 - val_loss: 0.8622 - val_acc: 0.6627\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6345 - acc: 0.7480 - val_loss: 0.8749 - val_acc: 0.6506\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6279 - acc: 0.7534 - val_loss: 0.8362 - val_acc: 0.6627\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6276 - acc: 0.7588 - val_loss: 0.8574 - val_acc: 0.6506\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6261 - acc: 0.7588 - val_loss: 0.8561 - val_acc: 0.6506\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6299 - acc: 0.7561 - val_loss: 0.8534 - val_acc: 0.6506\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6399 - acc: 0.7426 - val_loss: 0.8500 - val_acc: 0.6506\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6336 - acc: 0.7520 - val_loss: 0.8734 - val_acc: 0.6506\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6282 - acc: 0.7615 - val_loss: 0.8525 - val_acc: 0.6506\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6303 - acc: 0.7399 - val_loss: 0.8587 - val_acc: 0.6506\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6240 - acc: 0.7628 - val_loss: 0.8516 - val_acc: 0.6627\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6178 - acc: 0.7547 - val_loss: 0.8639 - val_acc: 0.6386\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6182 - acc: 0.7695 - val_loss: 0.8502 - val_acc: 0.6506\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6244 - acc: 0.7493 - val_loss: 0.8714 - val_acc: 0.6265\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6459 - acc: 0.7439 - val_loss: 0.8843 - val_acc: 0.6627\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6206 - acc: 0.7574 - val_loss: 0.8429 - val_acc: 0.6506\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3931 - acc: 0.1981 - val_loss: 2.3852 - val_acc: 0.2892\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.3776 - acc: 0.2507 - val_loss: 2.3661 - val_acc: 0.2169\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.3528 - acc: 0.2628 - val_loss: 2.3334 - val_acc: 0.2048\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 2.3105 - acc: 0.2817 - val_loss: 2.2762 - val_acc: 0.1566\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 2.2382 - acc: 0.2210 - val_loss: 2.1841 - val_acc: 0.1687\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 2.1353 - acc: 0.2116 - val_loss: 2.0596 - val_acc: 0.1566\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 29us/step - loss: 2.0151 - acc: 0.2156 - val_loss: 1.9357 - val_acc: 0.1807\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.9210 - acc: 0.2722 - val_loss: 1.8335 - val_acc: 0.3133\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.8564 - acc: 0.2736 - val_loss: 1.7607 - val_acc: 0.3373\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.8079 - acc: 0.2749 - val_loss: 1.7076 - val_acc: 0.3494\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.7637 - acc: 0.2871 - val_loss: 1.6597 - val_acc: 0.3614\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.7196 - acc: 0.3032 - val_loss: 1.6264 - val_acc: 0.3614\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.6776 - acc: 0.3208 - val_loss: 1.5895 - val_acc: 0.3494\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.6421 - acc: 0.3329 - val_loss: 1.5588 - val_acc: 0.3494\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6085 - acc: 0.3369 - val_loss: 1.5273 - val_acc: 0.3494\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.5822 - acc: 0.3423 - val_loss: 1.5020 - val_acc: 0.3494\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5571 - acc: 0.3598 - val_loss: 1.4771 - val_acc: 0.3735\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5363 - acc: 0.3693 - val_loss: 1.4587 - val_acc: 0.3373\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.5167 - acc: 0.3814 - val_loss: 1.4367 - val_acc: 0.3614\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.4999 - acc: 0.3841 - val_loss: 1.4170 - val_acc: 0.3855\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.4828 - acc: 0.3962 - val_loss: 1.4130 - val_acc: 0.3735\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4672 - acc: 0.4030 - val_loss: 1.3917 - val_acc: 0.3614\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.4521 - acc: 0.4151 - val_loss: 1.3781 - val_acc: 0.3614\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4383 - acc: 0.4205 - val_loss: 1.3635 - val_acc: 0.3614\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4256 - acc: 0.4232 - val_loss: 1.3467 - val_acc: 0.4096\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4131 - acc: 0.4205 - val_loss: 1.3510 - val_acc: 0.3976\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.3988 - acc: 0.4313 - val_loss: 1.3343 - val_acc: 0.4096\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3861 - acc: 0.4367 - val_loss: 1.3281 - val_acc: 0.4217\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3746 - acc: 0.4461 - val_loss: 1.3114 - val_acc: 0.4337\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.3615 - acc: 0.4434 - val_loss: 1.3018 - val_acc: 0.4217\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.3499 - acc: 0.4501 - val_loss: 1.2983 - val_acc: 0.4578\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3385 - acc: 0.4663 - val_loss: 1.2932 - val_acc: 0.4458\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3258 - acc: 0.4744 - val_loss: 1.2787 - val_acc: 0.4578\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.3151 - acc: 0.4757 - val_loss: 1.2714 - val_acc: 0.4578\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3032 - acc: 0.4852 - val_loss: 1.2711 - val_acc: 0.4458\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2922 - acc: 0.4811 - val_loss: 1.2644 - val_acc: 0.4217\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.2814 - acc: 0.4825 - val_loss: 1.2586 - val_acc: 0.4337\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2711 - acc: 0.4879 - val_loss: 1.2474 - val_acc: 0.4699\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2612 - acc: 0.4919 - val_loss: 1.2404 - val_acc: 0.4699\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2497 - acc: 0.5013 - val_loss: 1.2435 - val_acc: 0.4217\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2397 - acc: 0.5013 - val_loss: 1.2406 - val_acc: 0.4217\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.2294 - acc: 0.5040 - val_loss: 1.2292 - val_acc: 0.3976\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2205 - acc: 0.5000 - val_loss: 1.2288 - val_acc: 0.4096\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2110 - acc: 0.5162 - val_loss: 1.2206 - val_acc: 0.4337\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.2030 - acc: 0.5216 - val_loss: 1.2223 - val_acc: 0.4337\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1911 - acc: 0.5256 - val_loss: 1.2122 - val_acc: 0.4096\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1843 - acc: 0.5162 - val_loss: 1.2141 - val_acc: 0.4337\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1728 - acc: 0.5229 - val_loss: 1.1911 - val_acc: 0.4578\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1648 - acc: 0.5229 - val_loss: 1.1946 - val_acc: 0.4578\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1567 - acc: 0.5296 - val_loss: 1.2003 - val_acc: 0.4578\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1449 - acc: 0.5310 - val_loss: 1.1873 - val_acc: 0.4458\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1394 - acc: 0.5377 - val_loss: 1.1841 - val_acc: 0.4699\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1303 - acc: 0.5445 - val_loss: 1.1760 - val_acc: 0.4699\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1214 - acc: 0.5350 - val_loss: 1.1800 - val_acc: 0.4699\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1148 - acc: 0.5458 - val_loss: 1.1702 - val_acc: 0.4819\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1103 - acc: 0.5526 - val_loss: 1.1606 - val_acc: 0.4699\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1015 - acc: 0.5539 - val_loss: 1.1605 - val_acc: 0.4699\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0925 - acc: 0.5687 - val_loss: 1.1551 - val_acc: 0.5422\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0876 - acc: 0.5674 - val_loss: 1.1449 - val_acc: 0.4940\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0791 - acc: 0.5728 - val_loss: 1.1490 - val_acc: 0.4940\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.5660 - val_loss: 1.1425 - val_acc: 0.4940\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0682 - acc: 0.5687 - val_loss: 1.1385 - val_acc: 0.5060\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0597 - acc: 0.5674 - val_loss: 1.1433 - val_acc: 0.4940\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0549 - acc: 0.5660 - val_loss: 1.1263 - val_acc: 0.5422\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0495 - acc: 0.5876 - val_loss: 1.1236 - val_acc: 0.5783\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.0426 - acc: 0.5795 - val_loss: 1.1227 - val_acc: 0.5181\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0371 - acc: 0.5863 - val_loss: 1.1214 - val_acc: 0.5301\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0303 - acc: 0.5741 - val_loss: 1.1186 - val_acc: 0.5301\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0263 - acc: 0.5916 - val_loss: 1.1145 - val_acc: 0.5422\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0214 - acc: 0.5930 - val_loss: 1.1054 - val_acc: 0.5663\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 1.0163 - acc: 0.5930 - val_loss: 1.0988 - val_acc: 0.5783\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0104 - acc: 0.5943 - val_loss: 1.0989 - val_acc: 0.5422\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0060 - acc: 0.5755 - val_loss: 1.0982 - val_acc: 0.5422\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0012 - acc: 0.5943 - val_loss: 1.0909 - val_acc: 0.5904\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9954 - acc: 0.6065 - val_loss: 1.0830 - val_acc: 0.6024\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9922 - acc: 0.5863 - val_loss: 1.0907 - val_acc: 0.5783\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.9888 - acc: 0.6024 - val_loss: 1.0775 - val_acc: 0.6024\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9819 - acc: 0.5970 - val_loss: 1.0802 - val_acc: 0.5783\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9803 - acc: 0.6065 - val_loss: 1.0627 - val_acc: 0.6024\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9743 - acc: 0.6038 - val_loss: 1.0723 - val_acc: 0.5663\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9693 - acc: 0.5984 - val_loss: 1.0667 - val_acc: 0.6024\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9655 - acc: 0.6132 - val_loss: 1.0543 - val_acc: 0.6145\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9597 - acc: 0.6173 - val_loss: 1.0560 - val_acc: 0.6024\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9587 - acc: 0.6051 - val_loss: 1.0560 - val_acc: 0.5904\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9525 - acc: 0.6186 - val_loss: 1.0592 - val_acc: 0.6265\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9488 - acc: 0.6186 - val_loss: 1.0464 - val_acc: 0.6024\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9452 - acc: 0.6038 - val_loss: 1.0501 - val_acc: 0.6024\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9466 - acc: 0.6186 - val_loss: 1.0383 - val_acc: 0.6265\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9398 - acc: 0.6159 - val_loss: 1.0386 - val_acc: 0.5904\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9358 - acc: 0.6226 - val_loss: 1.0324 - val_acc: 0.6386\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9308 - acc: 0.6294 - val_loss: 1.0376 - val_acc: 0.6627\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9285 - acc: 0.6253 - val_loss: 1.0198 - val_acc: 0.6506\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9257 - acc: 0.6226 - val_loss: 1.0283 - val_acc: 0.5904\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9201 - acc: 0.6253 - val_loss: 1.0211 - val_acc: 0.6386\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9159 - acc: 0.6348 - val_loss: 1.0151 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9148 - acc: 0.6307 - val_loss: 1.0214 - val_acc: 0.5783\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9103 - acc: 0.6334 - val_loss: 1.0066 - val_acc: 0.6627\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9065 - acc: 0.6294 - val_loss: 1.0104 - val_acc: 0.6386\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9042 - acc: 0.6456 - val_loss: 1.0035 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9029 - acc: 0.6388 - val_loss: 0.9989 - val_acc: 0.6627\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8984 - acc: 0.6482 - val_loss: 0.9989 - val_acc: 0.6747\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8952 - acc: 0.6496 - val_loss: 0.9937 - val_acc: 0.6265\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8935 - acc: 0.6429 - val_loss: 1.0007 - val_acc: 0.6386\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8919 - acc: 0.6563 - val_loss: 0.9864 - val_acc: 0.6627\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8855 - acc: 0.6577 - val_loss: 0.9879 - val_acc: 0.6747\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8849 - acc: 0.6509 - val_loss: 0.9868 - val_acc: 0.6627\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8815 - acc: 0.6698 - val_loss: 0.9826 - val_acc: 0.6627\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.6469 - val_loss: 0.9808 - val_acc: 0.6386\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8796 - acc: 0.6523 - val_loss: 0.9733 - val_acc: 0.6627\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8715 - acc: 0.6685 - val_loss: 0.9763 - val_acc: 0.6506\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8695 - acc: 0.6590 - val_loss: 0.9664 - val_acc: 0.6506\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.6698 - val_loss: 0.9666 - val_acc: 0.6867\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8621 - acc: 0.6685 - val_loss: 0.9610 - val_acc: 0.6627\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8619 - acc: 0.6779 - val_loss: 0.9629 - val_acc: 0.6506\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8594 - acc: 0.6752 - val_loss: 0.9535 - val_acc: 0.6627\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8598 - acc: 0.6739 - val_loss: 0.9559 - val_acc: 0.6747\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8564 - acc: 0.6469 - val_loss: 0.9568 - val_acc: 0.6747\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8536 - acc: 0.6712 - val_loss: 0.9550 - val_acc: 0.6747\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8508 - acc: 0.6698 - val_loss: 0.9454 - val_acc: 0.6988\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8466 - acc: 0.6739 - val_loss: 0.9467 - val_acc: 0.6867\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8449 - acc: 0.6617 - val_loss: 0.9526 - val_acc: 0.6506\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8424 - acc: 0.6765 - val_loss: 0.9433 - val_acc: 0.6747\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8400 - acc: 0.6833 - val_loss: 0.9366 - val_acc: 0.6867\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8374 - acc: 0.6806 - val_loss: 0.9420 - val_acc: 0.6747\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8345 - acc: 0.6873 - val_loss: 0.9385 - val_acc: 0.6747\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8324 - acc: 0.6819 - val_loss: 0.9260 - val_acc: 0.7108\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8307 - acc: 0.6833 - val_loss: 0.9313 - val_acc: 0.6867\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8327 - acc: 0.6685 - val_loss: 0.9293 - val_acc: 0.6867\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8334 - acc: 0.6725 - val_loss: 0.9271 - val_acc: 0.6988\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8263 - acc: 0.6698 - val_loss: 0.9196 - val_acc: 0.7108\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8263 - acc: 0.6806 - val_loss: 0.9219 - val_acc: 0.6988\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8233 - acc: 0.6685 - val_loss: 0.9109 - val_acc: 0.7229\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8210 - acc: 0.6900 - val_loss: 0.9200 - val_acc: 0.7108\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8158 - acc: 0.6846 - val_loss: 0.9131 - val_acc: 0.7229\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8128 - acc: 0.6806 - val_loss: 0.9144 - val_acc: 0.7108\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8117 - acc: 0.6914 - val_loss: 0.9028 - val_acc: 0.6988\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8101 - acc: 0.6752 - val_loss: 0.9085 - val_acc: 0.7108\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8065 - acc: 0.6941 - val_loss: 0.9117 - val_acc: 0.7108\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8069 - acc: 0.6819 - val_loss: 0.9024 - val_acc: 0.7229\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8025 - acc: 0.6860 - val_loss: 0.8992 - val_acc: 0.7229\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7997 - acc: 0.6941 - val_loss: 0.8995 - val_acc: 0.7349\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7972 - acc: 0.6941 - val_loss: 0.9012 - val_acc: 0.7349\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7965 - acc: 0.6860 - val_loss: 0.8909 - val_acc: 0.7470\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7928 - acc: 0.6954 - val_loss: 0.8925 - val_acc: 0.7470\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7923 - acc: 0.6941 - val_loss: 0.8963 - val_acc: 0.7470\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7908 - acc: 0.6792 - val_loss: 0.8927 - val_acc: 0.7349\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7885 - acc: 0.6819 - val_loss: 0.8809 - val_acc: 0.7470\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7893 - acc: 0.6981 - val_loss: 0.9001 - val_acc: 0.7349\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7871 - acc: 0.6860 - val_loss: 0.8853 - val_acc: 0.7229\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7836 - acc: 0.6954 - val_loss: 0.8799 - val_acc: 0.7349\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7828 - acc: 0.7022 - val_loss: 0.8876 - val_acc: 0.7229\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7772 - acc: 0.6941 - val_loss: 0.8729 - val_acc: 0.7470\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7759 - acc: 0.6900 - val_loss: 0.8785 - val_acc: 0.7349\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7732 - acc: 0.6941 - val_loss: 0.8827 - val_acc: 0.7470\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7730 - acc: 0.6981 - val_loss: 0.8760 - val_acc: 0.7711\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.7707 - acc: 0.6833 - val_loss: 0.8737 - val_acc: 0.7590\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7704 - acc: 0.7102 - val_loss: 0.8760 - val_acc: 0.7470\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7703 - acc: 0.6846 - val_loss: 0.8658 - val_acc: 0.7470\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7670 - acc: 0.7008 - val_loss: 0.8666 - val_acc: 0.7229\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 57us/step - loss: 0.7660 - acc: 0.7049 - val_loss: 0.8796 - val_acc: 0.7470\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7634 - acc: 0.6927 - val_loss: 0.8729 - val_acc: 0.7229\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7595 - acc: 0.6995 - val_loss: 0.8654 - val_acc: 0.7590\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7596 - acc: 0.7022 - val_loss: 0.8648 - val_acc: 0.7470\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7570 - acc: 0.7035 - val_loss: 0.8699 - val_acc: 0.7229\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7573 - acc: 0.7049 - val_loss: 0.8658 - val_acc: 0.7470\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7540 - acc: 0.7049 - val_loss: 0.8588 - val_acc: 0.7470\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7544 - acc: 0.7129 - val_loss: 0.8648 - val_acc: 0.7349\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7524 - acc: 0.7035 - val_loss: 0.8561 - val_acc: 0.7349\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7493 - acc: 0.7197 - val_loss: 0.8571 - val_acc: 0.7108\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7518 - acc: 0.7089 - val_loss: 0.8599 - val_acc: 0.7711\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7446 - acc: 0.7143 - val_loss: 0.8536 - val_acc: 0.7349\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7458 - acc: 0.7035 - val_loss: 0.8558 - val_acc: 0.7349\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7472 - acc: 0.7183 - val_loss: 0.8541 - val_acc: 0.7349\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7439 - acc: 0.7089 - val_loss: 0.8497 - val_acc: 0.7590\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7443 - acc: 0.7129 - val_loss: 0.8546 - val_acc: 0.7229\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7420 - acc: 0.7251 - val_loss: 0.8501 - val_acc: 0.7229\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7429 - acc: 0.6914 - val_loss: 0.8431 - val_acc: 0.7590\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7430 - acc: 0.7102 - val_loss: 0.8478 - val_acc: 0.7711\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7349 - acc: 0.7183 - val_loss: 0.8502 - val_acc: 0.7349\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7320 - acc: 0.7237 - val_loss: 0.8382 - val_acc: 0.7349\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7319 - acc: 0.7089 - val_loss: 0.8422 - val_acc: 0.7470\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7295 - acc: 0.7224 - val_loss: 0.8480 - val_acc: 0.7470\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7296 - acc: 0.7116 - val_loss: 0.8381 - val_acc: 0.7108\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7289 - acc: 0.7156 - val_loss: 0.8369 - val_acc: 0.7349\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7265 - acc: 0.7305 - val_loss: 0.8449 - val_acc: 0.7590\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7262 - acc: 0.7237 - val_loss: 0.8356 - val_acc: 0.7590\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7249 - acc: 0.7116 - val_loss: 0.8382 - val_acc: 0.7349\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7223 - acc: 0.7210 - val_loss: 0.8390 - val_acc: 0.7229\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7207 - acc: 0.7264 - val_loss: 0.8458 - val_acc: 0.7349\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7218 - acc: 0.7264 - val_loss: 0.8380 - val_acc: 0.7470\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7204 - acc: 0.7129 - val_loss: 0.8296 - val_acc: 0.7590\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7179 - acc: 0.7358 - val_loss: 0.8370 - val_acc: 0.7590\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7180 - acc: 0.7237 - val_loss: 0.8331 - val_acc: 0.7590\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7147 - acc: 0.7358 - val_loss: 0.8339 - val_acc: 0.7349\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7147 - acc: 0.7278 - val_loss: 0.8297 - val_acc: 0.7590\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7126 - acc: 0.7372 - val_loss: 0.8294 - val_acc: 0.7470\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7109 - acc: 0.7183 - val_loss: 0.8348 - val_acc: 0.7711\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7105 - acc: 0.7305 - val_loss: 0.8247 - val_acc: 0.7229\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7095 - acc: 0.7332 - val_loss: 0.8172 - val_acc: 0.7590\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7096 - acc: 0.7278 - val_loss: 0.8319 - val_acc: 0.7711\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3882 - acc: 0.2399 - val_loss: 2.3768 - val_acc: 0.1807\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 2.3597 - acc: 0.2453 - val_loss: 2.3372 - val_acc: 0.1325\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 2.3040 - acc: 0.2035 - val_loss: 2.2586 - val_acc: 0.1205\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.2035 - acc: 0.1927 - val_loss: 2.1238 - val_acc: 0.1205\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 2.0584 - acc: 0.2332 - val_loss: 1.9612 - val_acc: 0.2771\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.9308 - acc: 0.2898 - val_loss: 1.8300 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8542 - acc: 0.2655 - val_loss: 1.7447 - val_acc: 0.3373\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.7987 - acc: 0.2588 - val_loss: 1.6850 - val_acc: 0.3253\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.7484 - acc: 0.2736 - val_loss: 1.6398 - val_acc: 0.3614\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.7020 - acc: 0.2857 - val_loss: 1.5991 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.6633 - acc: 0.3005 - val_loss: 1.5619 - val_acc: 0.3494\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.6261 - acc: 0.3248 - val_loss: 1.5347 - val_acc: 0.3494\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.5933 - acc: 0.3248 - val_loss: 1.5013 - val_acc: 0.3373\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5652 - acc: 0.3396 - val_loss: 1.4721 - val_acc: 0.3373\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.5398 - acc: 0.3625 - val_loss: 1.4497 - val_acc: 0.3614\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.5179 - acc: 0.3747 - val_loss: 1.4235 - val_acc: 0.3614\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4983 - acc: 0.3827 - val_loss: 1.4134 - val_acc: 0.3855\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.4808 - acc: 0.3881 - val_loss: 1.3783 - val_acc: 0.3976\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4645 - acc: 0.3976 - val_loss: 1.3707 - val_acc: 0.4096\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.4450 - acc: 0.4111 - val_loss: 1.3605 - val_acc: 0.3735\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.4288 - acc: 0.4218 - val_loss: 1.3395 - val_acc: 0.3976\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.4134 - acc: 0.4326 - val_loss: 1.3227 - val_acc: 0.4699\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3983 - acc: 0.4367 - val_loss: 1.3185 - val_acc: 0.4337\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3850 - acc: 0.4394 - val_loss: 1.3137 - val_acc: 0.4337\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.3697 - acc: 0.4528 - val_loss: 1.2810 - val_acc: 0.4699\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.3570 - acc: 0.4582 - val_loss: 1.2920 - val_acc: 0.4217\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3419 - acc: 0.4744 - val_loss: 1.2607 - val_acc: 0.4699\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3274 - acc: 0.4798 - val_loss: 1.2610 - val_acc: 0.4458\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.3133 - acc: 0.4798 - val_loss: 1.2594 - val_acc: 0.4699\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2989 - acc: 0.4892 - val_loss: 1.2398 - val_acc: 0.4458\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2860 - acc: 0.4892 - val_loss: 1.2368 - val_acc: 0.4458\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.2731 - acc: 0.4933 - val_loss: 1.2221 - val_acc: 0.4819\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2609 - acc: 0.4973 - val_loss: 1.2273 - val_acc: 0.4940\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2480 - acc: 0.5067 - val_loss: 1.2088 - val_acc: 0.4458\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2365 - acc: 0.5013 - val_loss: 1.2217 - val_acc: 0.4578\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2223 - acc: 0.5081 - val_loss: 1.1969 - val_acc: 0.4699\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2110 - acc: 0.5189 - val_loss: 1.1854 - val_acc: 0.4458\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1990 - acc: 0.5377 - val_loss: 1.1904 - val_acc: 0.4699\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1886 - acc: 0.5296 - val_loss: 1.1752 - val_acc: 0.4699\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1748 - acc: 0.5418 - val_loss: 1.1802 - val_acc: 0.4819\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1686 - acc: 0.5323 - val_loss: 1.1765 - val_acc: 0.4940\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1556 - acc: 0.5323 - val_loss: 1.1662 - val_acc: 0.4819\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1472 - acc: 0.5323 - val_loss: 1.1559 - val_acc: 0.5060\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.1356 - acc: 0.5580 - val_loss: 1.1598 - val_acc: 0.5301\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1238 - acc: 0.5499 - val_loss: 1.1413 - val_acc: 0.4819\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1177 - acc: 0.5674 - val_loss: 1.1424 - val_acc: 0.5301\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1088 - acc: 0.5526 - val_loss: 1.1420 - val_acc: 0.4699\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.0954 - acc: 0.5553 - val_loss: 1.1423 - val_acc: 0.4819\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0873 - acc: 0.5580 - val_loss: 1.1323 - val_acc: 0.5181\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0769 - acc: 0.5728 - val_loss: 1.1280 - val_acc: 0.5181\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0727 - acc: 0.5566 - val_loss: 1.1305 - val_acc: 0.4940\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0619 - acc: 0.5606 - val_loss: 1.1232 - val_acc: 0.5060\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0543 - acc: 0.5714 - val_loss: 1.1143 - val_acc: 0.5181\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.0479 - acc: 0.5863 - val_loss: 1.1013 - val_acc: 0.5542\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0409 - acc: 0.5863 - val_loss: 1.1091 - val_acc: 0.5542\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0361 - acc: 0.5782 - val_loss: 1.1154 - val_acc: 0.5181\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.0287 - acc: 0.5943 - val_loss: 1.0873 - val_acc: 0.5542\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0256 - acc: 0.5836 - val_loss: 1.0939 - val_acc: 0.5542\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0202 - acc: 0.5822 - val_loss: 1.0767 - val_acc: 0.5783\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.0110 - acc: 0.5916 - val_loss: 1.0875 - val_acc: 0.5301\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0046 - acc: 0.5984 - val_loss: 1.0824 - val_acc: 0.5904\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9989 - acc: 0.6078 - val_loss: 1.0818 - val_acc: 0.5422\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.9938 - acc: 0.6159 - val_loss: 1.0624 - val_acc: 0.5663\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9896 - acc: 0.6065 - val_loss: 1.0705 - val_acc: 0.5663\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9830 - acc: 0.6051 - val_loss: 1.0605 - val_acc: 0.5542\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9773 - acc: 0.6186 - val_loss: 1.0659 - val_acc: 0.5783\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9723 - acc: 0.6038 - val_loss: 1.0604 - val_acc: 0.5542\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9681 - acc: 0.6240 - val_loss: 1.0391 - val_acc: 0.5904\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9651 - acc: 0.6199 - val_loss: 1.0531 - val_acc: 0.6024\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9593 - acc: 0.6321 - val_loss: 1.0421 - val_acc: 0.5783\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9566 - acc: 0.6186 - val_loss: 1.0280 - val_acc: 0.6265\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9484 - acc: 0.6361 - val_loss: 1.0318 - val_acc: 0.5904\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9449 - acc: 0.6267 - val_loss: 1.0265 - val_acc: 0.6145\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9414 - acc: 0.6388 - val_loss: 1.0233 - val_acc: 0.6265\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9371 - acc: 0.6415 - val_loss: 1.0193 - val_acc: 0.6386\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 30us/step - loss: 0.9316 - acc: 0.6415 - val_loss: 1.0272 - val_acc: 0.5783\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9274 - acc: 0.6550 - val_loss: 1.0168 - val_acc: 0.6024\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9278 - acc: 0.6361 - val_loss: 1.0172 - val_acc: 0.6145\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9214 - acc: 0.6469 - val_loss: 0.9994 - val_acc: 0.5904\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9160 - acc: 0.6523 - val_loss: 1.0043 - val_acc: 0.6145\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9112 - acc: 0.6536 - val_loss: 1.0038 - val_acc: 0.6024\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9088 - acc: 0.6604 - val_loss: 0.9915 - val_acc: 0.5904\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9059 - acc: 0.6509 - val_loss: 0.9945 - val_acc: 0.6145\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9040 - acc: 0.6523 - val_loss: 0.9960 - val_acc: 0.5904\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9005 - acc: 0.6617 - val_loss: 0.9859 - val_acc: 0.5904\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8921 - acc: 0.6644 - val_loss: 0.9827 - val_acc: 0.6024\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.6604 - val_loss: 0.9854 - val_acc: 0.6024\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8870 - acc: 0.6739 - val_loss: 0.9793 - val_acc: 0.5904\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8872 - acc: 0.6604 - val_loss: 0.9619 - val_acc: 0.6386\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8810 - acc: 0.6536 - val_loss: 1.0029 - val_acc: 0.6386\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8759 - acc: 0.6685 - val_loss: 0.9556 - val_acc: 0.6386\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8740 - acc: 0.6725 - val_loss: 0.9712 - val_acc: 0.6386\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8721 - acc: 0.6631 - val_loss: 0.9707 - val_acc: 0.6024\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8703 - acc: 0.6658 - val_loss: 0.9613 - val_acc: 0.6145\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8643 - acc: 0.6873 - val_loss: 0.9565 - val_acc: 0.6265\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8646 - acc: 0.6685 - val_loss: 0.9635 - val_acc: 0.6145\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8614 - acc: 0.6806 - val_loss: 0.9611 - val_acc: 0.6265\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8604 - acc: 0.6577 - val_loss: 0.9374 - val_acc: 0.6506\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8556 - acc: 0.6819 - val_loss: 0.9467 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8499 - acc: 0.6752 - val_loss: 0.9416 - val_acc: 0.6506\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8438 - acc: 0.6819 - val_loss: 0.9380 - val_acc: 0.6386\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8458 - acc: 0.6752 - val_loss: 0.9407 - val_acc: 0.6506\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8377 - acc: 0.6887 - val_loss: 0.9372 - val_acc: 0.6506\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8368 - acc: 0.6833 - val_loss: 0.9241 - val_acc: 0.6506\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8359 - acc: 0.6927 - val_loss: 0.9237 - val_acc: 0.6506\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8294 - acc: 0.6941 - val_loss: 0.9291 - val_acc: 0.6506\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8280 - acc: 0.6941 - val_loss: 0.9219 - val_acc: 0.6506\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8242 - acc: 0.6927 - val_loss: 0.9285 - val_acc: 0.6506\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8217 - acc: 0.7022 - val_loss: 0.9163 - val_acc: 0.6506\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8187 - acc: 0.6954 - val_loss: 0.9268 - val_acc: 0.6386\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8174 - acc: 0.7075 - val_loss: 0.9092 - val_acc: 0.6506\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8126 - acc: 0.6914 - val_loss: 0.9207 - val_acc: 0.6506\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8144 - acc: 0.6981 - val_loss: 0.9141 - val_acc: 0.6867\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8070 - acc: 0.6995 - val_loss: 0.9124 - val_acc: 0.6627\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8051 - acc: 0.7022 - val_loss: 0.9077 - val_acc: 0.6627\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8085 - acc: 0.7022 - val_loss: 0.9097 - val_acc: 0.6747\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8027 - acc: 0.7008 - val_loss: 0.9149 - val_acc: 0.6627\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7977 - acc: 0.6968 - val_loss: 0.9020 - val_acc: 0.6747\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7976 - acc: 0.6981 - val_loss: 0.9153 - val_acc: 0.6506\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7964 - acc: 0.7116 - val_loss: 0.9045 - val_acc: 0.6627\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7903 - acc: 0.7116 - val_loss: 0.8941 - val_acc: 0.6627\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7891 - acc: 0.7075 - val_loss: 0.8993 - val_acc: 0.6747\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7879 - acc: 0.6995 - val_loss: 0.9028 - val_acc: 0.6627\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7826 - acc: 0.7089 - val_loss: 0.8888 - val_acc: 0.6627\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7836 - acc: 0.7129 - val_loss: 0.8921 - val_acc: 0.6747\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7804 - acc: 0.7075 - val_loss: 0.9061 - val_acc: 0.6867\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7779 - acc: 0.7062 - val_loss: 0.8872 - val_acc: 0.6627\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7746 - acc: 0.7156 - val_loss: 0.8828 - val_acc: 0.6988\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7723 - acc: 0.7264 - val_loss: 0.8915 - val_acc: 0.6867\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7728 - acc: 0.7075 - val_loss: 0.8893 - val_acc: 0.6627\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7764 - acc: 0.7089 - val_loss: 0.8912 - val_acc: 0.6747\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7670 - acc: 0.7183 - val_loss: 0.8862 - val_acc: 0.6627\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7677 - acc: 0.7210 - val_loss: 0.8914 - val_acc: 0.6747\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7649 - acc: 0.7264 - val_loss: 0.8779 - val_acc: 0.6867\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7627 - acc: 0.7183 - val_loss: 0.8814 - val_acc: 0.6988\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7609 - acc: 0.7305 - val_loss: 0.8782 - val_acc: 0.6747\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7606 - acc: 0.7210 - val_loss: 0.8918 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7651 - acc: 0.7116 - val_loss: 0.8679 - val_acc: 0.6988\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7561 - acc: 0.7210 - val_loss: 0.8971 - val_acc: 0.6867\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7491 - acc: 0.7305 - val_loss: 0.8741 - val_acc: 0.6867\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7524 - acc: 0.7183 - val_loss: 0.8708 - val_acc: 0.6867\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7551 - acc: 0.7116 - val_loss: 0.8792 - val_acc: 0.6867\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7470 - acc: 0.7170 - val_loss: 0.8751 - val_acc: 0.6867\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7438 - acc: 0.7358 - val_loss: 0.8708 - val_acc: 0.6988\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7436 - acc: 0.7291 - val_loss: 0.8785 - val_acc: 0.6747\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7420 - acc: 0.7251 - val_loss: 0.8721 - val_acc: 0.6867\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7359 - acc: 0.7399 - val_loss: 0.8729 - val_acc: 0.6747\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7369 - acc: 0.7318 - val_loss: 0.8701 - val_acc: 0.6747\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7360 - acc: 0.7385 - val_loss: 0.8595 - val_acc: 0.6988\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7346 - acc: 0.7305 - val_loss: 0.8708 - val_acc: 0.6506\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 55us/step - loss: 0.7320 - acc: 0.7358 - val_loss: 0.8739 - val_acc: 0.6747\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7352 - acc: 0.7237 - val_loss: 0.8609 - val_acc: 0.6506\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7321 - acc: 0.7264 - val_loss: 0.8658 - val_acc: 0.6988\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7268 - acc: 0.7426 - val_loss: 0.8649 - val_acc: 0.6867\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7234 - acc: 0.7358 - val_loss: 0.8575 - val_acc: 0.7108\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.7233 - acc: 0.7399 - val_loss: 0.8665 - val_acc: 0.6627\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7253 - acc: 0.7318 - val_loss: 0.8478 - val_acc: 0.7229\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7263 - acc: 0.7264 - val_loss: 0.8644 - val_acc: 0.6988\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7243 - acc: 0.7291 - val_loss: 0.8594 - val_acc: 0.6867\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7200 - acc: 0.7439 - val_loss: 0.8634 - val_acc: 0.6988\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7160 - acc: 0.7358 - val_loss: 0.8550 - val_acc: 0.7108\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7149 - acc: 0.7426 - val_loss: 0.8502 - val_acc: 0.7108\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7154 - acc: 0.7358 - val_loss: 0.8645 - val_acc: 0.6988\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7141 - acc: 0.7399 - val_loss: 0.8547 - val_acc: 0.6867\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7099 - acc: 0.7399 - val_loss: 0.8507 - val_acc: 0.7108\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7129 - acc: 0.7264 - val_loss: 0.8514 - val_acc: 0.6988\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7076 - acc: 0.7318 - val_loss: 0.8658 - val_acc: 0.6627\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7082 - acc: 0.7493 - val_loss: 0.8571 - val_acc: 0.6867\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7045 - acc: 0.7439 - val_loss: 0.8496 - val_acc: 0.7108\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7054 - acc: 0.7372 - val_loss: 0.8549 - val_acc: 0.6988\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7048 - acc: 0.7493 - val_loss: 0.8514 - val_acc: 0.6988\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7028 - acc: 0.7480 - val_loss: 0.8520 - val_acc: 0.6988\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7001 - acc: 0.7493 - val_loss: 0.8648 - val_acc: 0.6867\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.6981 - acc: 0.7480 - val_loss: 0.8424 - val_acc: 0.7229\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6983 - acc: 0.7399 - val_loss: 0.8457 - val_acc: 0.7229\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6982 - acc: 0.7399 - val_loss: 0.8561 - val_acc: 0.6988\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6953 - acc: 0.7439 - val_loss: 0.8404 - val_acc: 0.6867\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6903 - acc: 0.7561 - val_loss: 0.8578 - val_acc: 0.6747\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6912 - acc: 0.7507 - val_loss: 0.8416 - val_acc: 0.7108\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6906 - acc: 0.7466 - val_loss: 0.8516 - val_acc: 0.6867\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6909 - acc: 0.7520 - val_loss: 0.8329 - val_acc: 0.7229\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6913 - acc: 0.7385 - val_loss: 0.8402 - val_acc: 0.6988\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6884 - acc: 0.7399 - val_loss: 0.8471 - val_acc: 0.6867\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6905 - acc: 0.7345 - val_loss: 0.8474 - val_acc: 0.6867\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6866 - acc: 0.7453 - val_loss: 0.8418 - val_acc: 0.6867\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6858 - acc: 0.7507 - val_loss: 0.8352 - val_acc: 0.6867\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6870 - acc: 0.7439 - val_loss: 0.8413 - val_acc: 0.6867\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6878 - acc: 0.7345 - val_loss: 0.8398 - val_acc: 0.6988\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6802 - acc: 0.7426 - val_loss: 0.8330 - val_acc: 0.6988\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6854 - acc: 0.7453 - val_loss: 0.8386 - val_acc: 0.7108\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6851 - acc: 0.7534 - val_loss: 0.8349 - val_acc: 0.6867\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.7278 - val_loss: 0.8456 - val_acc: 0.6867\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6780 - acc: 0.7358 - val_loss: 0.8382 - val_acc: 0.7108\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6774 - acc: 0.7466 - val_loss: 0.8396 - val_acc: 0.6747\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6791 - acc: 0.7412 - val_loss: 0.8355 - val_acc: 0.6988\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6731 - acc: 0.7453 - val_loss: 0.8502 - val_acc: 0.6627\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6725 - acc: 0.7426 - val_loss: 0.8293 - val_acc: 0.6988\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6726 - acc: 0.7480 - val_loss: 0.8377 - val_acc: 0.6867\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6694 - acc: 0.7466 - val_loss: 0.8301 - val_acc: 0.6867\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6680 - acc: 0.7466 - val_loss: 0.8309 - val_acc: 0.6867\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 1ms/step - loss: 2.3877 - acc: 0.2251 - val_loss: 2.3666 - val_acc: 0.3373\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 2.3510 - acc: 0.2547 - val_loss: 2.3088 - val_acc: 0.3373\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 2.2792 - acc: 0.2547 - val_loss: 2.1976 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 2.1539 - acc: 0.2547 - val_loss: 2.0164 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.9898 - acc: 0.2547 - val_loss: 1.8252 - val_acc: 0.3373\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8613 - acc: 0.2547 - val_loss: 1.7015 - val_acc: 0.3494\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.7767 - acc: 0.2709 - val_loss: 1.6369 - val_acc: 0.3855\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.7079 - acc: 0.3154 - val_loss: 1.5873 - val_acc: 0.3494\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6494 - acc: 0.3342 - val_loss: 1.5235 - val_acc: 0.3494\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.5999 - acc: 0.3464 - val_loss: 1.4988 - val_acc: 0.3614\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.5615 - acc: 0.3504 - val_loss: 1.4681 - val_acc: 0.3614\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.5277 - acc: 0.3706 - val_loss: 1.4288 - val_acc: 0.3976\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.4997 - acc: 0.3639 - val_loss: 1.3985 - val_acc: 0.3614\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 1.4756 - acc: 0.3922 - val_loss: 1.3825 - val_acc: 0.3976\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.4525 - acc: 0.4030 - val_loss: 1.3548 - val_acc: 0.3976\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.4299 - acc: 0.4313 - val_loss: 1.3520 - val_acc: 0.4337\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.4095 - acc: 0.4299 - val_loss: 1.3335 - val_acc: 0.4337\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3899 - acc: 0.4447 - val_loss: 1.3122 - val_acc: 0.4337\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3679 - acc: 0.4636 - val_loss: 1.2891 - val_acc: 0.4819\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3481 - acc: 0.4677 - val_loss: 1.2858 - val_acc: 0.4578\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.3296 - acc: 0.4933 - val_loss: 1.2691 - val_acc: 0.4819\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3080 - acc: 0.5000 - val_loss: 1.2498 - val_acc: 0.4578\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.2885 - acc: 0.4987 - val_loss: 1.2349 - val_acc: 0.4458\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2732 - acc: 0.5040 - val_loss: 1.2292 - val_acc: 0.4458\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2516 - acc: 0.5175 - val_loss: 1.2172 - val_acc: 0.5060\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2344 - acc: 0.5216 - val_loss: 1.2005 - val_acc: 0.4819\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2154 - acc: 0.5296 - val_loss: 1.1961 - val_acc: 0.4578\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2012 - acc: 0.5162 - val_loss: 1.1843 - val_acc: 0.4217\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1857 - acc: 0.5323 - val_loss: 1.1556 - val_acc: 0.5181\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1692 - acc: 0.5418 - val_loss: 1.1802 - val_acc: 0.4337\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1557 - acc: 0.5512 - val_loss: 1.1441 - val_acc: 0.5181\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 1.1404 - acc: 0.5647 - val_loss: 1.1388 - val_acc: 0.4819\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 1.1276 - acc: 0.5566 - val_loss: 1.1631 - val_acc: 0.5060\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1118 - acc: 0.5647 - val_loss: 1.1203 - val_acc: 0.4819\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0994 - acc: 0.5782 - val_loss: 1.1305 - val_acc: 0.5542\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0873 - acc: 0.5809 - val_loss: 1.1193 - val_acc: 0.4940\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0749 - acc: 0.5876 - val_loss: 1.0959 - val_acc: 0.5060\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0640 - acc: 0.5889 - val_loss: 1.0991 - val_acc: 0.5422\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0540 - acc: 0.5957 - val_loss: 1.0923 - val_acc: 0.5060\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0413 - acc: 0.5889 - val_loss: 1.0783 - val_acc: 0.5060\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0319 - acc: 0.5970 - val_loss: 1.0658 - val_acc: 0.5181\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0240 - acc: 0.6078 - val_loss: 1.0638 - val_acc: 0.5542\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0121 - acc: 0.6105 - val_loss: 1.0519 - val_acc: 0.5542\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0016 - acc: 0.6186 - val_loss: 1.0528 - val_acc: 0.5542\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9960 - acc: 0.6226 - val_loss: 1.0465 - val_acc: 0.5904\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9868 - acc: 0.6186 - val_loss: 1.0375 - val_acc: 0.5663\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9773 - acc: 0.6388 - val_loss: 1.0149 - val_acc: 0.6024\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9710 - acc: 0.6334 - val_loss: 1.0244 - val_acc: 0.5904\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9644 - acc: 0.6348 - val_loss: 1.0176 - val_acc: 0.6145\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9579 - acc: 0.6415 - val_loss: 1.0038 - val_acc: 0.6265\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9508 - acc: 0.6482 - val_loss: 1.0006 - val_acc: 0.6386\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9450 - acc: 0.6294 - val_loss: 0.9978 - val_acc: 0.6386\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9357 - acc: 0.6523 - val_loss: 0.9671 - val_acc: 0.6627\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9253 - acc: 0.6590 - val_loss: 0.9887 - val_acc: 0.6265\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9192 - acc: 0.6617 - val_loss: 0.9745 - val_acc: 0.6386\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9131 - acc: 0.6752 - val_loss: 0.9598 - val_acc: 0.6627\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9087 - acc: 0.6671 - val_loss: 0.9694 - val_acc: 0.6386\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9016 - acc: 0.6765 - val_loss: 0.9560 - val_acc: 0.6506\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8980 - acc: 0.6550 - val_loss: 0.9454 - val_acc: 0.6506\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8958 - acc: 0.6536 - val_loss: 0.9565 - val_acc: 0.6506\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8834 - acc: 0.6725 - val_loss: 0.9454 - val_acc: 0.6265\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8774 - acc: 0.6792 - val_loss: 0.9502 - val_acc: 0.6627\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8728 - acc: 0.6779 - val_loss: 0.9193 - val_acc: 0.6867\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8716 - acc: 0.6752 - val_loss: 0.9320 - val_acc: 0.6747\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8661 - acc: 0.6846 - val_loss: 0.9109 - val_acc: 0.6506\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8603 - acc: 0.6765 - val_loss: 0.9379 - val_acc: 0.6747\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8532 - acc: 0.6954 - val_loss: 0.9079 - val_acc: 0.6867\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8459 - acc: 0.6981 - val_loss: 0.9061 - val_acc: 0.6747\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8510 - acc: 0.6941 - val_loss: 0.9115 - val_acc: 0.6627\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8450 - acc: 0.6981 - val_loss: 0.8926 - val_acc: 0.6627\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8341 - acc: 0.6981 - val_loss: 0.9185 - val_acc: 0.6627\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7008 - val_loss: 0.8832 - val_acc: 0.6627\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8281 - acc: 0.7022 - val_loss: 0.9070 - val_acc: 0.6386\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8208 - acc: 0.7062 - val_loss: 0.8855 - val_acc: 0.6627\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8189 - acc: 0.7035 - val_loss: 0.9070 - val_acc: 0.6506\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8141 - acc: 0.7049 - val_loss: 0.8800 - val_acc: 0.6867\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8174 - acc: 0.7075 - val_loss: 0.8927 - val_acc: 0.6506\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8146 - acc: 0.6954 - val_loss: 0.8875 - val_acc: 0.6867\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8111 - acc: 0.7062 - val_loss: 0.8486 - val_acc: 0.7229\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8122 - acc: 0.6833 - val_loss: 0.9036 - val_acc: 0.6627\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7987 - acc: 0.7143 - val_loss: 0.8592 - val_acc: 0.7229\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7924 - acc: 0.7102 - val_loss: 0.8865 - val_acc: 0.6627\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7928 - acc: 0.6914 - val_loss: 0.8660 - val_acc: 0.6747\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7879 - acc: 0.7170 - val_loss: 0.8595 - val_acc: 0.6867\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7957 - acc: 0.6860 - val_loss: 0.8974 - val_acc: 0.6506\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7823 - acc: 0.7170 - val_loss: 0.8502 - val_acc: 0.6867\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7743 - acc: 0.7210 - val_loss: 0.8686 - val_acc: 0.6867\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7705 - acc: 0.7197 - val_loss: 0.8552 - val_acc: 0.6988\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7734 - acc: 0.7183 - val_loss: 0.8747 - val_acc: 0.6747\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7665 - acc: 0.7332 - val_loss: 0.8549 - val_acc: 0.6747\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7655 - acc: 0.7156 - val_loss: 0.8560 - val_acc: 0.7108\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7612 - acc: 0.7183 - val_loss: 0.8601 - val_acc: 0.6988\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7568 - acc: 0.7251 - val_loss: 0.8539 - val_acc: 0.7108\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7587 - acc: 0.7291 - val_loss: 0.8613 - val_acc: 0.6506\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7521 - acc: 0.7291 - val_loss: 0.8582 - val_acc: 0.6988\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7517 - acc: 0.7278 - val_loss: 0.8497 - val_acc: 0.6988\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7472 - acc: 0.7170 - val_loss: 0.8661 - val_acc: 0.6988\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7433 - acc: 0.7332 - val_loss: 0.8472 - val_acc: 0.7108\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7424 - acc: 0.7224 - val_loss: 0.8555 - val_acc: 0.6627\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7424 - acc: 0.7332 - val_loss: 0.8483 - val_acc: 0.6988\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7401 - acc: 0.7399 - val_loss: 0.8619 - val_acc: 0.6747\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7343 - acc: 0.7291 - val_loss: 0.8455 - val_acc: 0.7108\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7317 - acc: 0.7278 - val_loss: 0.8572 - val_acc: 0.6506\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7358 - acc: 0.7251 - val_loss: 0.8314 - val_acc: 0.7229\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7406 - acc: 0.7278 - val_loss: 0.8581 - val_acc: 0.6988\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7330 - acc: 0.7278 - val_loss: 0.8493 - val_acc: 0.6627\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7337 - acc: 0.7251 - val_loss: 0.8402 - val_acc: 0.6988\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7239 - acc: 0.7264 - val_loss: 0.8608 - val_acc: 0.6747\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7255 - acc: 0.7358 - val_loss: 0.8421 - val_acc: 0.7229\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7167 - acc: 0.7332 - val_loss: 0.8581 - val_acc: 0.6747\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7141 - acc: 0.7358 - val_loss: 0.8647 - val_acc: 0.6506\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7105 - acc: 0.7480 - val_loss: 0.8409 - val_acc: 0.6988\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7119 - acc: 0.7129 - val_loss: 0.8562 - val_acc: 0.6988\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7156 - acc: 0.7305 - val_loss: 0.8499 - val_acc: 0.6386\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7140 - acc: 0.7278 - val_loss: 0.8465 - val_acc: 0.6867\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7100 - acc: 0.7210 - val_loss: 0.8543 - val_acc: 0.6988\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7026 - acc: 0.7264 - val_loss: 0.8495 - val_acc: 0.7108\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7005 - acc: 0.7332 - val_loss: 0.8509 - val_acc: 0.6867\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6985 - acc: 0.7426 - val_loss: 0.8440 - val_acc: 0.6867\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6966 - acc: 0.7372 - val_loss: 0.8458 - val_acc: 0.6867\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7001 - acc: 0.7305 - val_loss: 0.8515 - val_acc: 0.6988\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6940 - acc: 0.7466 - val_loss: 0.8363 - val_acc: 0.6988\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6974 - acc: 0.7601 - val_loss: 0.8685 - val_acc: 0.7108\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6902 - acc: 0.7439 - val_loss: 0.8390 - val_acc: 0.6988\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6901 - acc: 0.7385 - val_loss: 0.8700 - val_acc: 0.6386\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.6902 - acc: 0.7466 - val_loss: 0.8423 - val_acc: 0.6867\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6852 - acc: 0.7305 - val_loss: 0.8652 - val_acc: 0.6747\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6844 - acc: 0.7439 - val_loss: 0.8384 - val_acc: 0.6747\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6854 - acc: 0.7358 - val_loss: 0.8706 - val_acc: 0.6627\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6788 - acc: 0.7332 - val_loss: 0.8616 - val_acc: 0.6747\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6779 - acc: 0.7520 - val_loss: 0.8479 - val_acc: 0.6988\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6789 - acc: 0.7466 - val_loss: 0.8408 - val_acc: 0.6867\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6712 - acc: 0.7332 - val_loss: 0.8761 - val_acc: 0.6627\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6763 - acc: 0.7345 - val_loss: 0.8463 - val_acc: 0.6988\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6715 - acc: 0.7453 - val_loss: 0.8630 - val_acc: 0.6506\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6750 - acc: 0.7412 - val_loss: 0.8508 - val_acc: 0.6747\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6715 - acc: 0.7399 - val_loss: 0.8543 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6688 - acc: 0.7372 - val_loss: 0.8613 - val_acc: 0.6506\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6623 - acc: 0.7480 - val_loss: 0.8498 - val_acc: 0.6386\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6642 - acc: 0.7345 - val_loss: 0.8765 - val_acc: 0.6867\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6676 - acc: 0.7507 - val_loss: 0.8548 - val_acc: 0.6386\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6615 - acc: 0.7493 - val_loss: 0.8545 - val_acc: 0.6867\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6560 - acc: 0.7480 - val_loss: 0.8525 - val_acc: 0.6386\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6594 - acc: 0.7345 - val_loss: 0.8681 - val_acc: 0.6627\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6575 - acc: 0.7547 - val_loss: 0.8573 - val_acc: 0.6627\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6599 - acc: 0.7493 - val_loss: 0.8675 - val_acc: 0.6627\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6586 - acc: 0.7412 - val_loss: 0.8705 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6496 - acc: 0.7412 - val_loss: 0.8627 - val_acc: 0.6506\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6502 - acc: 0.7453 - val_loss: 0.8638 - val_acc: 0.6506\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6472 - acc: 0.7561 - val_loss: 0.8645 - val_acc: 0.6747\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6573 - acc: 0.7372 - val_loss: 0.8486 - val_acc: 0.6747\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6577 - acc: 0.7493 - val_loss: 0.8562 - val_acc: 0.6506\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6504 - acc: 0.7547 - val_loss: 0.8798 - val_acc: 0.6627\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6442 - acc: 0.7466 - val_loss: 0.8645 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6534 - acc: 0.7466 - val_loss: 0.8702 - val_acc: 0.6506\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6464 - acc: 0.7493 - val_loss: 0.8752 - val_acc: 0.6386\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6475 - acc: 0.7399 - val_loss: 0.8529 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6431 - acc: 0.7493 - val_loss: 0.8566 - val_acc: 0.6627\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6397 - acc: 0.7439 - val_loss: 0.8584 - val_acc: 0.6627\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6378 - acc: 0.7507 - val_loss: 0.8623 - val_acc: 0.6506\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6384 - acc: 0.7574 - val_loss: 0.8538 - val_acc: 0.6386\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6367 - acc: 0.7534 - val_loss: 0.8687 - val_acc: 0.6506\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6410 - acc: 0.7561 - val_loss: 0.8533 - val_acc: 0.6506\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6351 - acc: 0.7561 - val_loss: 0.8669 - val_acc: 0.6506\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6374 - acc: 0.7426 - val_loss: 0.8800 - val_acc: 0.6627\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6376 - acc: 0.7520 - val_loss: 0.8597 - val_acc: 0.6747\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6278 - acc: 0.7642 - val_loss: 0.8671 - val_acc: 0.6386\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6279 - acc: 0.7588 - val_loss: 0.8571 - val_acc: 0.6627\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6267 - acc: 0.7668 - val_loss: 0.8838 - val_acc: 0.6627\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6297 - acc: 0.7642 - val_loss: 0.8518 - val_acc: 0.6627\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6353 - acc: 0.7493 - val_loss: 0.8690 - val_acc: 0.6506\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6262 - acc: 0.7561 - val_loss: 0.8554 - val_acc: 0.6747\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6274 - acc: 0.7642 - val_loss: 0.8786 - val_acc: 0.6627\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6269 - acc: 0.7574 - val_loss: 0.8588 - val_acc: 0.6627\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6179 - acc: 0.7615 - val_loss: 0.8871 - val_acc: 0.6627\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6246 - acc: 0.7722 - val_loss: 0.8658 - val_acc: 0.6747\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6256 - acc: 0.7642 - val_loss: 0.8566 - val_acc: 0.6627\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6257 - acc: 0.7534 - val_loss: 0.8897 - val_acc: 0.6386\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6171 - acc: 0.7561 - val_loss: 0.8571 - val_acc: 0.6627\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6153 - acc: 0.7507 - val_loss: 0.8690 - val_acc: 0.6627\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6163 - acc: 0.7736 - val_loss: 0.8778 - val_acc: 0.6867\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6160 - acc: 0.7628 - val_loss: 0.8622 - val_acc: 0.6627\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6167 - acc: 0.7628 - val_loss: 0.8810 - val_acc: 0.6506\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6152 - acc: 0.7561 - val_loss: 0.8666 - val_acc: 0.6506\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6125 - acc: 0.7628 - val_loss: 0.8730 - val_acc: 0.6867\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6142 - acc: 0.7615 - val_loss: 0.8767 - val_acc: 0.6506\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6151 - acc: 0.7655 - val_loss: 0.8708 - val_acc: 0.6747\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6109 - acc: 0.7628 - val_loss: 0.8972 - val_acc: 0.6265\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6194 - acc: 0.7628 - val_loss: 0.8618 - val_acc: 0.6867\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6139 - acc: 0.7628 - val_loss: 0.8625 - val_acc: 0.6627\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6143 - acc: 0.7561 - val_loss: 0.8846 - val_acc: 0.6265\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6039 - acc: 0.7574 - val_loss: 0.8770 - val_acc: 0.6627\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6097 - acc: 0.7642 - val_loss: 0.8710 - val_acc: 0.6506\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6073 - acc: 0.7547 - val_loss: 0.8612 - val_acc: 0.6506\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6049 - acc: 0.7642 - val_loss: 0.8844 - val_acc: 0.6506\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6095 - acc: 0.7574 - val_loss: 0.8688 - val_acc: 0.6627\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6061 - acc: 0.7601 - val_loss: 0.8758 - val_acc: 0.6506\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6044 - acc: 0.7668 - val_loss: 0.8911 - val_acc: 0.6747\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6059 - acc: 0.7695 - val_loss: 0.8659 - val_acc: 0.6747\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6013 - acc: 0.7722 - val_loss: 0.9158 - val_acc: 0.6506\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3835 - acc: 0.1712 - val_loss: 2.3577 - val_acc: 0.2169\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 2.3273 - acc: 0.2588 - val_loss: 2.2675 - val_acc: 0.2530\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.2113 - acc: 0.2466 - val_loss: 2.0891 - val_acc: 0.2892\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.0338 - acc: 0.2682 - val_loss: 1.8811 - val_acc: 0.3253\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.8803 - acc: 0.2588 - val_loss: 1.7351 - val_acc: 0.3373\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.7873 - acc: 0.2642 - val_loss: 1.6607 - val_acc: 0.3253\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.7095 - acc: 0.2992 - val_loss: 1.5970 - val_acc: 0.3494\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.6431 - acc: 0.3100 - val_loss: 1.5439 - val_acc: 0.3133\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5876 - acc: 0.3181 - val_loss: 1.4971 - val_acc: 0.3133\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5415 - acc: 0.3733 - val_loss: 1.4691 - val_acc: 0.3614\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.5016 - acc: 0.4016 - val_loss: 1.4122 - val_acc: 0.4217\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 1.4669 - acc: 0.4259 - val_loss: 1.3961 - val_acc: 0.3855\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.4358 - acc: 0.4380 - val_loss: 1.3621 - val_acc: 0.4337\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4075 - acc: 0.4340 - val_loss: 1.3449 - val_acc: 0.4458\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3830 - acc: 0.4555 - val_loss: 1.3231 - val_acc: 0.4458\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.3561 - acc: 0.4542 - val_loss: 1.3036 - val_acc: 0.4699\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.3321 - acc: 0.4771 - val_loss: 1.2876 - val_acc: 0.3976\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 1.3076 - acc: 0.4892 - val_loss: 1.2716 - val_acc: 0.4458\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.2853 - acc: 0.4717 - val_loss: 1.2475 - val_acc: 0.4699\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2621 - acc: 0.5094 - val_loss: 1.2425 - val_acc: 0.4217\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2424 - acc: 0.5040 - val_loss: 1.2424 - val_acc: 0.4458\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.2308 - acc: 0.4946 - val_loss: 1.2140 - val_acc: 0.4819\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2043 - acc: 0.5229 - val_loss: 1.2215 - val_acc: 0.4337\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.1829 - acc: 0.5310 - val_loss: 1.1990 - val_acc: 0.4819\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1666 - acc: 0.5270 - val_loss: 1.1931 - val_acc: 0.4337\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1491 - acc: 0.5377 - val_loss: 1.1827 - val_acc: 0.4940\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1351 - acc: 0.5418 - val_loss: 1.1707 - val_acc: 0.4699\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1160 - acc: 0.5580 - val_loss: 1.1588 - val_acc: 0.5060\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1068 - acc: 0.5553 - val_loss: 1.1626 - val_acc: 0.4578\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0928 - acc: 0.5458 - val_loss: 1.1389 - val_acc: 0.4819\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0753 - acc: 0.5701 - val_loss: 1.1451 - val_acc: 0.4699\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0631 - acc: 0.5822 - val_loss: 1.1310 - val_acc: 0.4699\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0502 - acc: 0.5863 - val_loss: 1.1200 - val_acc: 0.5301\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0377 - acc: 0.5903 - val_loss: 1.1265 - val_acc: 0.4578\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0300 - acc: 0.5795 - val_loss: 1.1061 - val_acc: 0.5301\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0157 - acc: 0.5943 - val_loss: 1.1053 - val_acc: 0.5301\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.0015 - acc: 0.5984 - val_loss: 1.0949 - val_acc: 0.5422\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9917 - acc: 0.6173 - val_loss: 1.0868 - val_acc: 0.5181\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9808 - acc: 0.6199 - val_loss: 1.0818 - val_acc: 0.5181\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9762 - acc: 0.6146 - val_loss: 1.0746 - val_acc: 0.5542\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9666 - acc: 0.6321 - val_loss: 1.0605 - val_acc: 0.5542\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9580 - acc: 0.6159 - val_loss: 1.0555 - val_acc: 0.5422\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9502 - acc: 0.6307 - val_loss: 1.0594 - val_acc: 0.5301\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9336 - acc: 0.6482 - val_loss: 1.0383 - val_acc: 0.6506\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9291 - acc: 0.6469 - val_loss: 1.0501 - val_acc: 0.5663\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9197 - acc: 0.6536 - val_loss: 1.0410 - val_acc: 0.5783\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9210 - acc: 0.6509 - val_loss: 1.0086 - val_acc: 0.5663\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9099 - acc: 0.6496 - val_loss: 1.0509 - val_acc: 0.6265\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8978 - acc: 0.6617 - val_loss: 1.0106 - val_acc: 0.5663\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8921 - acc: 0.6604 - val_loss: 1.0075 - val_acc: 0.6145\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.6550 - val_loss: 1.0146 - val_acc: 0.5904\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8781 - acc: 0.6590 - val_loss: 0.9940 - val_acc: 0.6265\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8710 - acc: 0.6631 - val_loss: 1.0032 - val_acc: 0.5904\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8637 - acc: 0.6752 - val_loss: 1.0007 - val_acc: 0.6024\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8636 - acc: 0.6631 - val_loss: 0.9897 - val_acc: 0.6145\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8560 - acc: 0.6712 - val_loss: 0.9794 - val_acc: 0.5904\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8468 - acc: 0.6887 - val_loss: 0.9955 - val_acc: 0.5904\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8441 - acc: 0.6900 - val_loss: 0.9847 - val_acc: 0.6265\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8377 - acc: 0.6900 - val_loss: 0.9724 - val_acc: 0.5783\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8307 - acc: 0.6873 - val_loss: 0.9819 - val_acc: 0.6024\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8246 - acc: 0.6927 - val_loss: 0.9728 - val_acc: 0.6386\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8199 - acc: 0.7049 - val_loss: 0.9628 - val_acc: 0.6024\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8143 - acc: 0.7116 - val_loss: 0.9599 - val_acc: 0.6024\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8110 - acc: 0.6954 - val_loss: 0.9625 - val_acc: 0.6024\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8060 - acc: 0.7143 - val_loss: 0.9548 - val_acc: 0.6145\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7997 - acc: 0.7075 - val_loss: 0.9579 - val_acc: 0.5783\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7973 - acc: 0.7116 - val_loss: 0.9620 - val_acc: 0.6386\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7931 - acc: 0.6954 - val_loss: 0.9551 - val_acc: 0.6506\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7902 - acc: 0.7156 - val_loss: 0.9359 - val_acc: 0.6145\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7801 - acc: 0.7237 - val_loss: 0.9483 - val_acc: 0.6145\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7737 - acc: 0.7197 - val_loss: 0.9534 - val_acc: 0.6145\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7782 - acc: 0.7210 - val_loss: 0.9383 - val_acc: 0.6145\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7730 - acc: 0.7278 - val_loss: 0.9642 - val_acc: 0.6024\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7661 - acc: 0.7332 - val_loss: 0.9326 - val_acc: 0.6265\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7664 - acc: 0.7237 - val_loss: 0.9490 - val_acc: 0.6265\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7626 - acc: 0.7129 - val_loss: 0.9614 - val_acc: 0.6145\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7618 - acc: 0.6968 - val_loss: 0.9237 - val_acc: 0.6506\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7496 - acc: 0.7278 - val_loss: 0.9565 - val_acc: 0.6265\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7450 - acc: 0.7372 - val_loss: 0.9353 - val_acc: 0.6024\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7463 - acc: 0.7089 - val_loss: 0.9507 - val_acc: 0.6145\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7453 - acc: 0.7183 - val_loss: 0.9283 - val_acc: 0.5904\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7373 - acc: 0.7210 - val_loss: 0.9324 - val_acc: 0.6145\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7314 - acc: 0.7332 - val_loss: 0.9348 - val_acc: 0.6024\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7265 - acc: 0.7278 - val_loss: 0.9306 - val_acc: 0.6265\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7235 - acc: 0.7385 - val_loss: 0.9334 - val_acc: 0.6024\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7213 - acc: 0.7224 - val_loss: 0.9266 - val_acc: 0.6265\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7222 - acc: 0.7278 - val_loss: 0.9402 - val_acc: 0.6024\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7223 - acc: 0.7210 - val_loss: 0.9346 - val_acc: 0.5663\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7187 - acc: 0.7372 - val_loss: 0.9242 - val_acc: 0.6024\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7188 - acc: 0.7237 - val_loss: 0.9496 - val_acc: 0.6145\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7102 - acc: 0.7156 - val_loss: 0.9259 - val_acc: 0.5904\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7018 - acc: 0.7412 - val_loss: 0.9369 - val_acc: 0.6145\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7011 - acc: 0.7439 - val_loss: 0.9308 - val_acc: 0.6024\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6982 - acc: 0.7305 - val_loss: 0.9696 - val_acc: 0.5904\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6945 - acc: 0.7534 - val_loss: 0.9255 - val_acc: 0.5663\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7015 - acc: 0.7305 - val_loss: 0.9497 - val_acc: 0.6265\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6969 - acc: 0.7291 - val_loss: 0.9571 - val_acc: 0.6145\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7009 - acc: 0.7143 - val_loss: 0.9395 - val_acc: 0.6265\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6966 - acc: 0.7332 - val_loss: 0.9275 - val_acc: 0.6145\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 60us/step - loss: 0.6823 - acc: 0.7426 - val_loss: 0.9465 - val_acc: 0.6145\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 61us/step - loss: 0.6811 - acc: 0.7426 - val_loss: 0.9422 - val_acc: 0.5904\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6829 - acc: 0.7385 - val_loss: 0.9589 - val_acc: 0.6024\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6802 - acc: 0.7453 - val_loss: 0.9426 - val_acc: 0.6145\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6736 - acc: 0.7480 - val_loss: 0.9401 - val_acc: 0.6145\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6730 - acc: 0.7520 - val_loss: 0.9411 - val_acc: 0.6024\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6687 - acc: 0.7520 - val_loss: 0.9466 - val_acc: 0.6145\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6664 - acc: 0.7507 - val_loss: 0.9412 - val_acc: 0.5904\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6682 - acc: 0.7480 - val_loss: 0.9527 - val_acc: 0.6386\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6614 - acc: 0.7453 - val_loss: 0.9367 - val_acc: 0.6145\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6612 - acc: 0.7574 - val_loss: 0.9502 - val_acc: 0.6145\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6611 - acc: 0.7412 - val_loss: 0.9477 - val_acc: 0.6386\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.6551 - acc: 0.7439 - val_loss: 0.9513 - val_acc: 0.6386\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6548 - acc: 0.7588 - val_loss: 0.9490 - val_acc: 0.6386\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6521 - acc: 0.7507 - val_loss: 0.9484 - val_acc: 0.6386\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6550 - acc: 0.7574 - val_loss: 0.9314 - val_acc: 0.6386\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6505 - acc: 0.7480 - val_loss: 0.9367 - val_acc: 0.6145\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6451 - acc: 0.7480 - val_loss: 0.9722 - val_acc: 0.6145\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6519 - acc: 0.7480 - val_loss: 0.9433 - val_acc: 0.6145\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6466 - acc: 0.7480 - val_loss: 0.9369 - val_acc: 0.6145\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.6477 - acc: 0.7534 - val_loss: 0.9431 - val_acc: 0.6265\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6446 - acc: 0.7493 - val_loss: 0.9484 - val_acc: 0.6024\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6423 - acc: 0.7493 - val_loss: 0.9494 - val_acc: 0.6265\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6373 - acc: 0.7588 - val_loss: 0.9620 - val_acc: 0.6265\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6374 - acc: 0.7655 - val_loss: 0.9749 - val_acc: 0.6145\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.6335 - acc: 0.7574 - val_loss: 0.9392 - val_acc: 0.6024\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6415 - acc: 0.7507 - val_loss: 0.9509 - val_acc: 0.6024\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6415 - acc: 0.7453 - val_loss: 0.9692 - val_acc: 0.6145\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6348 - acc: 0.7507 - val_loss: 0.9528 - val_acc: 0.6024\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.6307 - acc: 0.7534 - val_loss: 0.9335 - val_acc: 0.6145\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6297 - acc: 0.7507 - val_loss: 0.9708 - val_acc: 0.6145\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6350 - acc: 0.7547 - val_loss: 0.9330 - val_acc: 0.6265\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6342 - acc: 0.7534 - val_loss: 0.9708 - val_acc: 0.6024\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6311 - acc: 0.7561 - val_loss: 0.9608 - val_acc: 0.6265\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6249 - acc: 0.7426 - val_loss: 0.9609 - val_acc: 0.5904\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6203 - acc: 0.7574 - val_loss: 0.9402 - val_acc: 0.5904\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6201 - acc: 0.7561 - val_loss: 0.9592 - val_acc: 0.6265\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6212 - acc: 0.7615 - val_loss: 0.9673 - val_acc: 0.6145\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6218 - acc: 0.7628 - val_loss: 0.9604 - val_acc: 0.6024\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6206 - acc: 0.7574 - val_loss: 0.9637 - val_acc: 0.5904\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6195 - acc: 0.7547 - val_loss: 0.9640 - val_acc: 0.6145\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6148 - acc: 0.7588 - val_loss: 0.9903 - val_acc: 0.6024\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6129 - acc: 0.7588 - val_loss: 0.9484 - val_acc: 0.6024\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6159 - acc: 0.7655 - val_loss: 0.9450 - val_acc: 0.6024\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6164 - acc: 0.7628 - val_loss: 0.9977 - val_acc: 0.5904\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6129 - acc: 0.7628 - val_loss: 0.9591 - val_acc: 0.6145\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6116 - acc: 0.7520 - val_loss: 0.9711 - val_acc: 0.6145\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6124 - acc: 0.7615 - val_loss: 0.9639 - val_acc: 0.6145\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6052 - acc: 0.7642 - val_loss: 0.9568 - val_acc: 0.6145\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6041 - acc: 0.7601 - val_loss: 0.9955 - val_acc: 0.6145\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6102 - acc: 0.7655 - val_loss: 0.9987 - val_acc: 0.6024\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6138 - acc: 0.7412 - val_loss: 0.9609 - val_acc: 0.6145\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5914 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.5663\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6008 - acc: 0.7642 - val_loss: 0.9777 - val_acc: 0.6024\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5971 - acc: 0.7668 - val_loss: 0.9611 - val_acc: 0.6145\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5917 - acc: 0.7642 - val_loss: 0.9759 - val_acc: 0.6145\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5891 - acc: 0.7682 - val_loss: 0.9692 - val_acc: 0.6145\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5906 - acc: 0.7709 - val_loss: 0.9822 - val_acc: 0.6145\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5938 - acc: 0.7655 - val_loss: 0.9730 - val_acc: 0.6145\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5855 - acc: 0.7682 - val_loss: 0.9976 - val_acc: 0.6386\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5902 - acc: 0.7561 - val_loss: 0.9867 - val_acc: 0.6145\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7682 - val_loss: 0.9775 - val_acc: 0.6145\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5816 - acc: 0.7695 - val_loss: 0.9794 - val_acc: 0.6145\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5849 - acc: 0.7655 - val_loss: 0.9627 - val_acc: 0.6145\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5872 - acc: 0.7709 - val_loss: 1.0042 - val_acc: 0.6265\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5845 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.6024\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5884 - acc: 0.7601 - val_loss: 0.9810 - val_acc: 0.6145\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5785 - acc: 0.7655 - val_loss: 1.0145 - val_acc: 0.6265\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5797 - acc: 0.7655 - val_loss: 0.9894 - val_acc: 0.6145\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5848 - acc: 0.7668 - val_loss: 1.0116 - val_acc: 0.6024\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5797 - acc: 0.7655 - val_loss: 0.9586 - val_acc: 0.6265\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5800 - acc: 0.7776 - val_loss: 1.0057 - val_acc: 0.6024\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5751 - acc: 0.7668 - val_loss: 0.9826 - val_acc: 0.6145\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5676 - acc: 0.7830 - val_loss: 0.9837 - val_acc: 0.6386\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5702 - acc: 0.7830 - val_loss: 1.0107 - val_acc: 0.6024\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5773 - acc: 0.7520 - val_loss: 0.9736 - val_acc: 0.6265\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5672 - acc: 0.7817 - val_loss: 0.9817 - val_acc: 0.6145\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5687 - acc: 0.7749 - val_loss: 0.9941 - val_acc: 0.6145\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5695 - acc: 0.7736 - val_loss: 0.9987 - val_acc: 0.6145\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5713 - acc: 0.7682 - val_loss: 0.9938 - val_acc: 0.6265\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5667 - acc: 0.7790 - val_loss: 1.0098 - val_acc: 0.6386\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5648 - acc: 0.7763 - val_loss: 0.9970 - val_acc: 0.6265\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5619 - acc: 0.7695 - val_loss: 0.9841 - val_acc: 0.6145\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5668 - acc: 0.7776 - val_loss: 0.9944 - val_acc: 0.6024\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5644 - acc: 0.7736 - val_loss: 1.0368 - val_acc: 0.5904\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5704 - acc: 0.7857 - val_loss: 0.9752 - val_acc: 0.6265\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5599 - acc: 0.7830 - val_loss: 1.0107 - val_acc: 0.6265\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5563 - acc: 0.7722 - val_loss: 0.9913 - val_acc: 0.6265\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5596 - acc: 0.7803 - val_loss: 1.0054 - val_acc: 0.6265\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5571 - acc: 0.7790 - val_loss: 1.0229 - val_acc: 0.6265\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5551 - acc: 0.7871 - val_loss: 0.9904 - val_acc: 0.6265\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5577 - acc: 0.7938 - val_loss: 1.0210 - val_acc: 0.6265\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5472 - acc: 0.7898 - val_loss: 1.0167 - val_acc: 0.6506\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5574 - acc: 0.7736 - val_loss: 1.0077 - val_acc: 0.6265\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5552 - acc: 0.7749 - val_loss: 1.0137 - val_acc: 0.6265\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5601 - acc: 0.7830 - val_loss: 1.0570 - val_acc: 0.6265\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5565 - acc: 0.7776 - val_loss: 1.0046 - val_acc: 0.6265\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5503 - acc: 0.7884 - val_loss: 1.0077 - val_acc: 0.6265\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5480 - acc: 0.7817 - val_loss: 1.0294 - val_acc: 0.6265\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5488 - acc: 0.7871 - val_loss: 1.0069 - val_acc: 0.6145\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5541 - acc: 0.7749 - val_loss: 1.0060 - val_acc: 0.6627\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3889 - acc: 0.2520 - val_loss: 2.3748 - val_acc: 0.3012\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 2.3597 - acc: 0.2588 - val_loss: 2.3348 - val_acc: 0.3012\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.3089 - acc: 0.2588 - val_loss: 2.2620 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 2.2196 - acc: 0.2547 - val_loss: 2.1392 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 2.0908 - acc: 0.2547 - val_loss: 1.9755 - val_acc: 0.3373\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.9518 - acc: 0.2547 - val_loss: 1.8378 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.8682 - acc: 0.2547 - val_loss: 1.7570 - val_acc: 0.3373\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8221 - acc: 0.2588 - val_loss: 1.7128 - val_acc: 0.3494\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.7836 - acc: 0.2682 - val_loss: 1.6929 - val_acc: 0.3614\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.7514 - acc: 0.2911 - val_loss: 1.6645 - val_acc: 0.3494\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.7224 - acc: 0.3059 - val_loss: 1.6429 - val_acc: 0.3735\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.6906 - acc: 0.3261 - val_loss: 1.6176 - val_acc: 0.3373\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.6623 - acc: 0.3356 - val_loss: 1.5925 - val_acc: 0.3494\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6340 - acc: 0.3464 - val_loss: 1.5760 - val_acc: 0.3976\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6060 - acc: 0.3841 - val_loss: 1.5487 - val_acc: 0.3614\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.5795 - acc: 0.3895 - val_loss: 1.5208 - val_acc: 0.3614\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.5547 - acc: 0.3922 - val_loss: 1.4903 - val_acc: 0.3494\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5323 - acc: 0.3868 - val_loss: 1.4725 - val_acc: 0.3735\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.5102 - acc: 0.4043 - val_loss: 1.4500 - val_acc: 0.3614\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.4891 - acc: 0.4070 - val_loss: 1.4274 - val_acc: 0.3494\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 1.4705 - acc: 0.4164 - val_loss: 1.4055 - val_acc: 0.4096\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.4519 - acc: 0.4178 - val_loss: 1.3819 - val_acc: 0.4337\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4349 - acc: 0.4178 - val_loss: 1.3711 - val_acc: 0.4096\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4210 - acc: 0.4299 - val_loss: 1.3366 - val_acc: 0.4578\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4010 - acc: 0.4326 - val_loss: 1.3421 - val_acc: 0.3855\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3873 - acc: 0.4461 - val_loss: 1.3283 - val_acc: 0.3855\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3725 - acc: 0.4623 - val_loss: 1.3089 - val_acc: 0.4699\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3596 - acc: 0.4596 - val_loss: 1.2884 - val_acc: 0.4699\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.3436 - acc: 0.4717 - val_loss: 1.2932 - val_acc: 0.4096\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3342 - acc: 0.4596 - val_loss: 1.2764 - val_acc: 0.4337\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3168 - acc: 0.4771 - val_loss: 1.2629 - val_acc: 0.4578\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.3059 - acc: 0.4704 - val_loss: 1.2603 - val_acc: 0.4337\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2902 - acc: 0.4838 - val_loss: 1.2427 - val_acc: 0.4458\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.2799 - acc: 0.4852 - val_loss: 1.2315 - val_acc: 0.4819\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2663 - acc: 0.5000 - val_loss: 1.2290 - val_acc: 0.4337\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2548 - acc: 0.5040 - val_loss: 1.2242 - val_acc: 0.4578\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2429 - acc: 0.5040 - val_loss: 1.2134 - val_acc: 0.4819\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2323 - acc: 0.5054 - val_loss: 1.2023 - val_acc: 0.4458\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.2196 - acc: 0.5135 - val_loss: 1.2049 - val_acc: 0.4699\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2117 - acc: 0.5189 - val_loss: 1.1972 - val_acc: 0.4578\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2026 - acc: 0.5135 - val_loss: 1.1923 - val_acc: 0.4819\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 1.1909 - acc: 0.5175 - val_loss: 1.1739 - val_acc: 0.4578\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1757 - acc: 0.5256 - val_loss: 1.1852 - val_acc: 0.4578\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1658 - acc: 0.5350 - val_loss: 1.1656 - val_acc: 0.4578\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1555 - acc: 0.5404 - val_loss: 1.1623 - val_acc: 0.4458\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1460 - acc: 0.5445 - val_loss: 1.1658 - val_acc: 0.4699\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1374 - acc: 0.5485 - val_loss: 1.1444 - val_acc: 0.4578\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1263 - acc: 0.5512 - val_loss: 1.1527 - val_acc: 0.5060\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.1159 - acc: 0.5539 - val_loss: 1.1342 - val_acc: 0.4819\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1053 - acc: 0.5566 - val_loss: 1.1416 - val_acc: 0.4940\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0961 - acc: 0.5647 - val_loss: 1.1342 - val_acc: 0.5181\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0882 - acc: 0.5701 - val_loss: 1.1241 - val_acc: 0.5181\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0792 - acc: 0.5755 - val_loss: 1.1148 - val_acc: 0.5181\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0704 - acc: 0.5728 - val_loss: 1.1140 - val_acc: 0.5181\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0651 - acc: 0.5809 - val_loss: 1.1115 - val_acc: 0.5301\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0564 - acc: 0.5876 - val_loss: 1.1057 - val_acc: 0.4819\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0483 - acc: 0.5755 - val_loss: 1.1039 - val_acc: 0.5301\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0409 - acc: 0.5849 - val_loss: 1.0930 - val_acc: 0.5301\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0340 - acc: 0.5916 - val_loss: 1.0890 - val_acc: 0.5181\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0266 - acc: 0.5809 - val_loss: 1.0920 - val_acc: 0.5181\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0212 - acc: 0.5863 - val_loss: 1.0821 - val_acc: 0.5663\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0153 - acc: 0.5822 - val_loss: 1.0786 - val_acc: 0.5181\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0080 - acc: 0.5876 - val_loss: 1.0762 - val_acc: 0.5301\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9991 - acc: 0.5809 - val_loss: 1.0691 - val_acc: 0.5422\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9930 - acc: 0.5903 - val_loss: 1.0656 - val_acc: 0.5542\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9892 - acc: 0.5970 - val_loss: 1.0611 - val_acc: 0.5422\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9812 - acc: 0.6038 - val_loss: 1.0606 - val_acc: 0.5663\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9785 - acc: 0.6051 - val_loss: 1.0537 - val_acc: 0.5422\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9721 - acc: 0.6092 - val_loss: 1.0540 - val_acc: 0.5301\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9683 - acc: 0.6011 - val_loss: 1.0390 - val_acc: 0.5783\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9639 - acc: 0.6105 - val_loss: 1.0601 - val_acc: 0.5301\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9589 - acc: 0.6105 - val_loss: 1.0349 - val_acc: 0.5663\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.9518 - acc: 0.6213 - val_loss: 1.0349 - val_acc: 0.5663\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9457 - acc: 0.6119 - val_loss: 1.0320 - val_acc: 0.5542\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9428 - acc: 0.6051 - val_loss: 1.0302 - val_acc: 0.5663\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9367 - acc: 0.6186 - val_loss: 1.0262 - val_acc: 0.5663\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9313 - acc: 0.6253 - val_loss: 1.0247 - val_acc: 0.5542\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9290 - acc: 0.6334 - val_loss: 1.0198 - val_acc: 0.5422\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9242 - acc: 0.6429 - val_loss: 1.0114 - val_acc: 0.5663\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.9218 - acc: 0.6361 - val_loss: 1.0080 - val_acc: 0.5783\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9145 - acc: 0.6348 - val_loss: 1.0055 - val_acc: 0.5904\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9104 - acc: 0.6321 - val_loss: 1.0053 - val_acc: 0.5663\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9078 - acc: 0.6469 - val_loss: 1.0014 - val_acc: 0.5663\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9043 - acc: 0.6361 - val_loss: 0.9936 - val_acc: 0.5663\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9018 - acc: 0.6321 - val_loss: 1.0080 - val_acc: 0.5663\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8942 - acc: 0.6469 - val_loss: 0.9930 - val_acc: 0.5783\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8927 - acc: 0.6415 - val_loss: 0.9812 - val_acc: 0.6024\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.6456 - val_loss: 0.9959 - val_acc: 0.5904\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8855 - acc: 0.6496 - val_loss: 0.9965 - val_acc: 0.5904\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8791 - acc: 0.6685 - val_loss: 0.9761 - val_acc: 0.6145\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8804 - acc: 0.6617 - val_loss: 0.9797 - val_acc: 0.5783\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8763 - acc: 0.6415 - val_loss: 0.9784 - val_acc: 0.6145\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8696 - acc: 0.6456 - val_loss: 0.9728 - val_acc: 0.5783\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8673 - acc: 0.6563 - val_loss: 0.9714 - val_acc: 0.5904\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8632 - acc: 0.6671 - val_loss: 0.9699 - val_acc: 0.5783\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8591 - acc: 0.6712 - val_loss: 0.9677 - val_acc: 0.5904\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8576 - acc: 0.6698 - val_loss: 0.9724 - val_acc: 0.6024\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8538 - acc: 0.6739 - val_loss: 0.9525 - val_acc: 0.6386\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8504 - acc: 0.6671 - val_loss: 0.9690 - val_acc: 0.6024\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8461 - acc: 0.6739 - val_loss: 0.9604 - val_acc: 0.5783\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8460 - acc: 0.6752 - val_loss: 0.9463 - val_acc: 0.6145\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.8437 - acc: 0.6712 - val_loss: 0.9448 - val_acc: 0.6747\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8382 - acc: 0.6725 - val_loss: 0.9513 - val_acc: 0.5904\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8367 - acc: 0.6873 - val_loss: 0.9540 - val_acc: 0.6024\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8330 - acc: 0.6846 - val_loss: 0.9402 - val_acc: 0.6506\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8318 - acc: 0.6739 - val_loss: 0.9521 - val_acc: 0.5783\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8272 - acc: 0.6954 - val_loss: 0.9335 - val_acc: 0.6627\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8247 - acc: 0.6725 - val_loss: 0.9410 - val_acc: 0.5904\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8208 - acc: 0.6887 - val_loss: 0.9369 - val_acc: 0.6024\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8226 - acc: 0.6792 - val_loss: 0.9348 - val_acc: 0.6265\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8172 - acc: 0.6806 - val_loss: 0.9455 - val_acc: 0.5783\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.8178 - acc: 0.6779 - val_loss: 0.9255 - val_acc: 0.6386\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8109 - acc: 0.6887 - val_loss: 0.9285 - val_acc: 0.6145\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.8161 - acc: 0.6833 - val_loss: 0.9237 - val_acc: 0.6145\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8059 - acc: 0.6927 - val_loss: 0.9249 - val_acc: 0.6024\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8033 - acc: 0.6914 - val_loss: 0.9223 - val_acc: 0.6265\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7993 - acc: 0.6900 - val_loss: 0.9252 - val_acc: 0.6506\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7992 - acc: 0.7062 - val_loss: 0.9074 - val_acc: 0.6386\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7955 - acc: 0.6995 - val_loss: 0.9273 - val_acc: 0.6265\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7927 - acc: 0.6887 - val_loss: 0.9178 - val_acc: 0.6024\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7912 - acc: 0.7062 - val_loss: 0.9026 - val_acc: 0.6627\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7866 - acc: 0.6954 - val_loss: 0.9131 - val_acc: 0.6386\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7842 - acc: 0.7075 - val_loss: 0.9153 - val_acc: 0.6386\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7840 - acc: 0.6927 - val_loss: 0.9052 - val_acc: 0.6265\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7814 - acc: 0.6995 - val_loss: 0.9019 - val_acc: 0.6506\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7774 - acc: 0.7022 - val_loss: 0.9091 - val_acc: 0.6506\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7748 - acc: 0.6941 - val_loss: 0.9110 - val_acc: 0.6747\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7724 - acc: 0.7089 - val_loss: 0.8982 - val_acc: 0.6506\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7702 - acc: 0.7035 - val_loss: 0.9026 - val_acc: 0.6747\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7665 - acc: 0.7062 - val_loss: 0.8999 - val_acc: 0.6265\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7693 - acc: 0.7156 - val_loss: 0.8919 - val_acc: 0.6386\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7629 - acc: 0.7008 - val_loss: 0.9095 - val_acc: 0.6627\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7605 - acc: 0.7116 - val_loss: 0.8956 - val_acc: 0.6506\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7601 - acc: 0.7237 - val_loss: 0.8897 - val_acc: 0.6627\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7588 - acc: 0.7049 - val_loss: 0.8930 - val_acc: 0.6747\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7565 - acc: 0.7102 - val_loss: 0.9081 - val_acc: 0.6506\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7574 - acc: 0.7156 - val_loss: 0.8736 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7555 - acc: 0.6995 - val_loss: 0.8977 - val_acc: 0.6627\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7521 - acc: 0.7075 - val_loss: 0.8882 - val_acc: 0.6506\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7498 - acc: 0.7116 - val_loss: 0.9076 - val_acc: 0.6506\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7512 - acc: 0.6981 - val_loss: 0.8775 - val_acc: 0.6747\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7477 - acc: 0.7143 - val_loss: 0.8975 - val_acc: 0.6506\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7431 - acc: 0.7183 - val_loss: 0.8875 - val_acc: 0.6747\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7412 - acc: 0.7116 - val_loss: 0.8809 - val_acc: 0.6627\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7424 - acc: 0.7224 - val_loss: 0.8859 - val_acc: 0.6506\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7387 - acc: 0.7156 - val_loss: 0.8887 - val_acc: 0.6747\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7411 - acc: 0.7035 - val_loss: 0.8787 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.7341 - acc: 0.7224 - val_loss: 0.8788 - val_acc: 0.6627\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7352 - acc: 0.7089 - val_loss: 0.8709 - val_acc: 0.6747\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7352 - acc: 0.7143 - val_loss: 0.8765 - val_acc: 0.6747\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7320 - acc: 0.7116 - val_loss: 0.8930 - val_acc: 0.6988\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7300 - acc: 0.7197 - val_loss: 0.8693 - val_acc: 0.6867\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7265 - acc: 0.7156 - val_loss: 0.8857 - val_acc: 0.6627\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7240 - acc: 0.7129 - val_loss: 0.8745 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7245 - acc: 0.7210 - val_loss: 0.8697 - val_acc: 0.6506\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7240 - acc: 0.7210 - val_loss: 0.8732 - val_acc: 0.6988\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7253 - acc: 0.7062 - val_loss: 0.8726 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7212 - acc: 0.7264 - val_loss: 0.8772 - val_acc: 0.7108\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7202 - acc: 0.7102 - val_loss: 0.8753 - val_acc: 0.6867\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7181 - acc: 0.7102 - val_loss: 0.8664 - val_acc: 0.6988\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7134 - acc: 0.7358 - val_loss: 0.8719 - val_acc: 0.6747\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7168 - acc: 0.7251 - val_loss: 0.8830 - val_acc: 0.6747\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7123 - acc: 0.7183 - val_loss: 0.8632 - val_acc: 0.7229\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7092 - acc: 0.7278 - val_loss: 0.8642 - val_acc: 0.6747\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7110 - acc: 0.7129 - val_loss: 0.8822 - val_acc: 0.6747\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7078 - acc: 0.7399 - val_loss: 0.8649 - val_acc: 0.6747\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7051 - acc: 0.7251 - val_loss: 0.8626 - val_acc: 0.7229\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7061 - acc: 0.7210 - val_loss: 0.8677 - val_acc: 0.6747\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7042 - acc: 0.7345 - val_loss: 0.8632 - val_acc: 0.6747\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7014 - acc: 0.7291 - val_loss: 0.8668 - val_acc: 0.6506\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7001 - acc: 0.7439 - val_loss: 0.8616 - val_acc: 0.6627\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7019 - acc: 0.7237 - val_loss: 0.8713 - val_acc: 0.7349\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7015 - acc: 0.7426 - val_loss: 0.8673 - val_acc: 0.6627\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7003 - acc: 0.7170 - val_loss: 0.8610 - val_acc: 0.6988\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6983 - acc: 0.7332 - val_loss: 0.8664 - val_acc: 0.6627\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6971 - acc: 0.7183 - val_loss: 0.8621 - val_acc: 0.7229\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6938 - acc: 0.7143 - val_loss: 0.8720 - val_acc: 0.6988\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6926 - acc: 0.7372 - val_loss: 0.8612 - val_acc: 0.6747\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6930 - acc: 0.7358 - val_loss: 0.8537 - val_acc: 0.6747\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6893 - acc: 0.7237 - val_loss: 0.8748 - val_acc: 0.6988\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6933 - acc: 0.7291 - val_loss: 0.8612 - val_acc: 0.6747\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6924 - acc: 0.7251 - val_loss: 0.8694 - val_acc: 0.6988\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6879 - acc: 0.7305 - val_loss: 0.8700 - val_acc: 0.6747\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6842 - acc: 0.7332 - val_loss: 0.8700 - val_acc: 0.6747\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6835 - acc: 0.7345 - val_loss: 0.8515 - val_acc: 0.6627\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6819 - acc: 0.7358 - val_loss: 0.8749 - val_acc: 0.6988\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6839 - acc: 0.7251 - val_loss: 0.8521 - val_acc: 0.6988\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6815 - acc: 0.7439 - val_loss: 0.8653 - val_acc: 0.6627\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6778 - acc: 0.7318 - val_loss: 0.8586 - val_acc: 0.6867\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6832 - acc: 0.7305 - val_loss: 0.8517 - val_acc: 0.6747\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6811 - acc: 0.7224 - val_loss: 0.8771 - val_acc: 0.6747\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6779 - acc: 0.7399 - val_loss: 0.8541 - val_acc: 0.6988\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.6757 - acc: 0.7358 - val_loss: 0.8662 - val_acc: 0.6747\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6735 - acc: 0.7318 - val_loss: 0.8677 - val_acc: 0.6627\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6742 - acc: 0.7439 - val_loss: 0.8532 - val_acc: 0.7108\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6751 - acc: 0.7358 - val_loss: 0.8702 - val_acc: 0.6627\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6732 - acc: 0.7399 - val_loss: 0.8689 - val_acc: 0.6747\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6743 - acc: 0.7264 - val_loss: 0.8557 - val_acc: 0.6988\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6680 - acc: 0.7439 - val_loss: 0.8571 - val_acc: 0.6747\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6672 - acc: 0.7251 - val_loss: 0.8725 - val_acc: 0.6988\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3891 - acc: 0.1846 - val_loss: 2.3722 - val_acc: 0.3133\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.3555 - acc: 0.2615 - val_loss: 2.3239 - val_acc: 0.3494\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.2934 - acc: 0.2790 - val_loss: 2.2335 - val_acc: 0.3253\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 2.1853 - acc: 0.2844 - val_loss: 2.0845 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.0321 - acc: 0.2871 - val_loss: 1.9054 - val_acc: 0.3494\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8987 - acc: 0.2588 - val_loss: 1.7739 - val_acc: 0.3373\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.8270 - acc: 0.2668 - val_loss: 1.7018 - val_acc: 0.3494\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.7710 - acc: 0.2844 - val_loss: 1.6636 - val_acc: 0.3735\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.7183 - acc: 0.3046 - val_loss: 1.6185 - val_acc: 0.3855\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.6643 - acc: 0.3221 - val_loss: 1.5768 - val_acc: 0.3614\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.6110 - acc: 0.3450 - val_loss: 1.5227 - val_acc: 0.3614\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.5685 - acc: 0.3827 - val_loss: 1.4865 - val_acc: 0.3133\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.5336 - acc: 0.3841 - val_loss: 1.4585 - val_acc: 0.3614\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.5041 - acc: 0.4016 - val_loss: 1.4234 - val_acc: 0.3735\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4779 - acc: 0.4084 - val_loss: 1.4027 - val_acc: 0.4217\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4550 - acc: 0.4124 - val_loss: 1.3886 - val_acc: 0.3735\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4333 - acc: 0.4191 - val_loss: 1.3587 - val_acc: 0.3735\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4108 - acc: 0.4272 - val_loss: 1.3532 - val_acc: 0.4217\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3913 - acc: 0.4367 - val_loss: 1.3376 - val_acc: 0.4096\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3747 - acc: 0.4501 - val_loss: 1.3134 - val_acc: 0.4217\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3551 - acc: 0.4447 - val_loss: 1.3139 - val_acc: 0.4217\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3370 - acc: 0.4569 - val_loss: 1.2949 - val_acc: 0.4337\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.3209 - acc: 0.4690 - val_loss: 1.2767 - val_acc: 0.4458\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3062 - acc: 0.4704 - val_loss: 1.2739 - val_acc: 0.4337\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2890 - acc: 0.4677 - val_loss: 1.2666 - val_acc: 0.3976\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2712 - acc: 0.4906 - val_loss: 1.2445 - val_acc: 0.4578\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2552 - acc: 0.4879 - val_loss: 1.2274 - val_acc: 0.4578\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 1.2411 - acc: 0.4906 - val_loss: 1.2408 - val_acc: 0.4337\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.2253 - acc: 0.5013 - val_loss: 1.2273 - val_acc: 0.4578\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2100 - acc: 0.4973 - val_loss: 1.2146 - val_acc: 0.4458\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1968 - acc: 0.5094 - val_loss: 1.2012 - val_acc: 0.4458\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1821 - acc: 0.5148 - val_loss: 1.2098 - val_acc: 0.4578\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1695 - acc: 0.5202 - val_loss: 1.1937 - val_acc: 0.4578\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1569 - acc: 0.5256 - val_loss: 1.1954 - val_acc: 0.4940\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1425 - acc: 0.5337 - val_loss: 1.1756 - val_acc: 0.4458\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.1302 - acc: 0.5418 - val_loss: 1.1739 - val_acc: 0.4578\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.1172 - acc: 0.5431 - val_loss: 1.1705 - val_acc: 0.4819\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1101 - acc: 0.5364 - val_loss: 1.1542 - val_acc: 0.4699\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0954 - acc: 0.5539 - val_loss: 1.1479 - val_acc: 0.4940\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0920 - acc: 0.5458 - val_loss: 1.1512 - val_acc: 0.4819\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0762 - acc: 0.5512 - val_loss: 1.1353 - val_acc: 0.4819\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 1.0684 - acc: 0.5728 - val_loss: 1.1250 - val_acc: 0.5181\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0566 - acc: 0.5876 - val_loss: 1.1282 - val_acc: 0.5181\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0480 - acc: 0.5660 - val_loss: 1.1220 - val_acc: 0.5181\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0396 - acc: 0.5660 - val_loss: 1.1072 - val_acc: 0.5060\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0299 - acc: 0.5863 - val_loss: 1.0996 - val_acc: 0.5422\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0213 - acc: 0.5889 - val_loss: 1.1001 - val_acc: 0.5181\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.0122 - acc: 0.5876 - val_loss: 1.0892 - val_acc: 0.5542\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.0052 - acc: 0.5943 - val_loss: 1.0891 - val_acc: 0.5301\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9950 - acc: 0.5903 - val_loss: 1.0633 - val_acc: 0.5663\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9898 - acc: 0.5984 - val_loss: 1.0808 - val_acc: 0.6024\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.9837 - acc: 0.5970 - val_loss: 1.0604 - val_acc: 0.5663\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9771 - acc: 0.5876 - val_loss: 1.0632 - val_acc: 0.5542\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9675 - acc: 0.6119 - val_loss: 1.0395 - val_acc: 0.5783\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9640 - acc: 0.5957 - val_loss: 1.0689 - val_acc: 0.5542\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9561 - acc: 0.6146 - val_loss: 1.0396 - val_acc: 0.6145\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9456 - acc: 0.6213 - val_loss: 1.0414 - val_acc: 0.6024\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9396 - acc: 0.6280 - val_loss: 1.0372 - val_acc: 0.6145\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9332 - acc: 0.6240 - val_loss: 1.0210 - val_acc: 0.6265\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9260 - acc: 0.6361 - val_loss: 1.0137 - val_acc: 0.6024\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9199 - acc: 0.6334 - val_loss: 1.0228 - val_acc: 0.5783\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9168 - acc: 0.6375 - val_loss: 1.0107 - val_acc: 0.6265\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9125 - acc: 0.6415 - val_loss: 0.9929 - val_acc: 0.6386\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9051 - acc: 0.6429 - val_loss: 1.0017 - val_acc: 0.6265\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.9003 - acc: 0.6456 - val_loss: 0.9915 - val_acc: 0.6506\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.6442 - val_loss: 0.9763 - val_acc: 0.6265\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.6456 - val_loss: 0.9815 - val_acc: 0.6386\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8817 - acc: 0.6550 - val_loss: 0.9808 - val_acc: 0.6386\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8759 - acc: 0.6523 - val_loss: 0.9646 - val_acc: 0.6386\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8704 - acc: 0.6685 - val_loss: 0.9610 - val_acc: 0.6506\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8672 - acc: 0.6658 - val_loss: 0.9612 - val_acc: 0.6627\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8704 - acc: 0.6509 - val_loss: 0.9585 - val_acc: 0.6265\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8582 - acc: 0.6536 - val_loss: 0.9525 - val_acc: 0.6506\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8544 - acc: 0.6765 - val_loss: 0.9446 - val_acc: 0.6265\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8465 - acc: 0.6846 - val_loss: 0.9471 - val_acc: 0.6386\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8401 - acc: 0.6941 - val_loss: 0.9381 - val_acc: 0.6506\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8372 - acc: 0.6833 - val_loss: 0.9292 - val_acc: 0.6506\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8367 - acc: 0.6765 - val_loss: 0.9527 - val_acc: 0.6627\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.6833 - val_loss: 0.9158 - val_acc: 0.6988\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8222 - acc: 0.6900 - val_loss: 0.9396 - val_acc: 0.6386\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8200 - acc: 0.6981 - val_loss: 0.9230 - val_acc: 0.6506\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8170 - acc: 0.6968 - val_loss: 0.9260 - val_acc: 0.6506\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8083 - acc: 0.6981 - val_loss: 0.9204 - val_acc: 0.6506\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8092 - acc: 0.7049 - val_loss: 0.9074 - val_acc: 0.6747\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8045 - acc: 0.7049 - val_loss: 0.9067 - val_acc: 0.6627\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7964 - acc: 0.7170 - val_loss: 0.9244 - val_acc: 0.6386\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7947 - acc: 0.7035 - val_loss: 0.9100 - val_acc: 0.6627\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7936 - acc: 0.7129 - val_loss: 0.8978 - val_acc: 0.6867\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7921 - acc: 0.7170 - val_loss: 0.8946 - val_acc: 0.6867\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7837 - acc: 0.7116 - val_loss: 0.9038 - val_acc: 0.6145\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7818 - acc: 0.7278 - val_loss: 0.8877 - val_acc: 0.6867\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7911 - acc: 0.6968 - val_loss: 0.8958 - val_acc: 0.6627\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.7744 - acc: 0.7170 - val_loss: 0.8942 - val_acc: 0.6627\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7696 - acc: 0.7183 - val_loss: 0.8932 - val_acc: 0.6867\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7666 - acc: 0.7291 - val_loss: 0.8748 - val_acc: 0.6627\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7635 - acc: 0.7318 - val_loss: 0.8915 - val_acc: 0.6747\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7611 - acc: 0.7183 - val_loss: 0.8631 - val_acc: 0.6988\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7601 - acc: 0.7197 - val_loss: 0.8814 - val_acc: 0.6988\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7590 - acc: 0.7345 - val_loss: 0.8764 - val_acc: 0.6747\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7490 - acc: 0.7237 - val_loss: 0.8611 - val_acc: 0.6867\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7470 - acc: 0.7332 - val_loss: 0.8691 - val_acc: 0.6627\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7458 - acc: 0.7183 - val_loss: 0.8550 - val_acc: 0.6988\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7426 - acc: 0.7332 - val_loss: 0.8637 - val_acc: 0.6627\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7374 - acc: 0.7197 - val_loss: 0.8647 - val_acc: 0.6867\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7337 - acc: 0.7305 - val_loss: 0.8602 - val_acc: 0.6988\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7359 - acc: 0.7143 - val_loss: 0.8708 - val_acc: 0.6747\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7339 - acc: 0.7264 - val_loss: 0.8511 - val_acc: 0.6867\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7262 - acc: 0.7453 - val_loss: 0.8457 - val_acc: 0.6988\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7289 - acc: 0.7197 - val_loss: 0.8590 - val_acc: 0.6867\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7250 - acc: 0.7332 - val_loss: 0.8637 - val_acc: 0.6747\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7259 - acc: 0.7197 - val_loss: 0.8380 - val_acc: 0.6988\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7199 - acc: 0.7439 - val_loss: 0.8452 - val_acc: 0.7229\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7279 - acc: 0.7156 - val_loss: 0.8586 - val_acc: 0.7229\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7162 - acc: 0.7197 - val_loss: 0.8359 - val_acc: 0.6867\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7139 - acc: 0.7480 - val_loss: 0.8448 - val_acc: 0.7108\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7171 - acc: 0.7224 - val_loss: 0.8597 - val_acc: 0.6867\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7086 - acc: 0.7399 - val_loss: 0.8409 - val_acc: 0.6988\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7028 - acc: 0.7358 - val_loss: 0.8324 - val_acc: 0.6988\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7037 - acc: 0.7493 - val_loss: 0.8364 - val_acc: 0.6988\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6988 - acc: 0.7466 - val_loss: 0.8373 - val_acc: 0.6988\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7001 - acc: 0.7332 - val_loss: 0.8319 - val_acc: 0.7108\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6949 - acc: 0.7507 - val_loss: 0.8306 - val_acc: 0.6867\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6946 - acc: 0.7332 - val_loss: 0.8498 - val_acc: 0.6747\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6929 - acc: 0.7561 - val_loss: 0.8331 - val_acc: 0.6988\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6903 - acc: 0.7385 - val_loss: 0.8474 - val_acc: 0.6867\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6853 - acc: 0.7412 - val_loss: 0.8237 - val_acc: 0.6867\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6870 - acc: 0.7345 - val_loss: 0.8374 - val_acc: 0.6867\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6848 - acc: 0.7466 - val_loss: 0.8279 - val_acc: 0.6988\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6823 - acc: 0.7507 - val_loss: 0.8213 - val_acc: 0.7108\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6816 - acc: 0.7318 - val_loss: 0.8521 - val_acc: 0.6867\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6851 - acc: 0.7305 - val_loss: 0.8317 - val_acc: 0.6988\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6786 - acc: 0.7453 - val_loss: 0.8322 - val_acc: 0.6867\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6767 - acc: 0.7561 - val_loss: 0.8383 - val_acc: 0.6988\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6791 - acc: 0.7412 - val_loss: 0.8214 - val_acc: 0.6988\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6730 - acc: 0.7412 - val_loss: 0.8467 - val_acc: 0.6867\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6700 - acc: 0.7574 - val_loss: 0.8168 - val_acc: 0.6988\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6724 - acc: 0.7439 - val_loss: 0.8338 - val_acc: 0.7108\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6653 - acc: 0.7480 - val_loss: 0.8346 - val_acc: 0.6627\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6674 - acc: 0.7453 - val_loss: 0.8178 - val_acc: 0.7108\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6653 - acc: 0.7399 - val_loss: 0.8266 - val_acc: 0.6627\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6658 - acc: 0.7588 - val_loss: 0.8234 - val_acc: 0.6867\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6685 - acc: 0.7372 - val_loss: 0.8214 - val_acc: 0.6988\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6625 - acc: 0.7412 - val_loss: 0.8308 - val_acc: 0.6747\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6627 - acc: 0.7466 - val_loss: 0.8283 - val_acc: 0.6867\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6558 - acc: 0.7426 - val_loss: 0.8303 - val_acc: 0.6627\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6539 - acc: 0.7466 - val_loss: 0.8202 - val_acc: 0.6627\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6507 - acc: 0.7507 - val_loss: 0.8242 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6507 - acc: 0.7520 - val_loss: 0.8292 - val_acc: 0.6867\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6474 - acc: 0.7507 - val_loss: 0.8113 - val_acc: 0.6747\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6433 - acc: 0.7601 - val_loss: 0.8339 - val_acc: 0.6627\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6432 - acc: 0.7561 - val_loss: 0.8335 - val_acc: 0.6988\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6475 - acc: 0.7412 - val_loss: 0.8421 - val_acc: 0.6627\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6456 - acc: 0.7480 - val_loss: 0.8185 - val_acc: 0.6747\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.6440 - acc: 0.7534 - val_loss: 0.8188 - val_acc: 0.6867\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6464 - acc: 0.7453 - val_loss: 0.8433 - val_acc: 0.6747\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6382 - acc: 0.7561 - val_loss: 0.8228 - val_acc: 0.6988\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6377 - acc: 0.7547 - val_loss: 0.8228 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6395 - acc: 0.7493 - val_loss: 0.8261 - val_acc: 0.6867\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6373 - acc: 0.7534 - val_loss: 0.8245 - val_acc: 0.6867\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6341 - acc: 0.7574 - val_loss: 0.8370 - val_acc: 0.6747\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6360 - acc: 0.7493 - val_loss: 0.8122 - val_acc: 0.6747\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6258 - acc: 0.7507 - val_loss: 0.8157 - val_acc: 0.7108\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6295 - acc: 0.7561 - val_loss: 0.8226 - val_acc: 0.6747\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6307 - acc: 0.7520 - val_loss: 0.8262 - val_acc: 0.6988\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.6322 - acc: 0.7547 - val_loss: 0.8046 - val_acc: 0.6867\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 31us/step - loss: 0.6315 - acc: 0.7466 - val_loss: 0.8378 - val_acc: 0.6867\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6263 - acc: 0.7574 - val_loss: 0.8177 - val_acc: 0.6506\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6235 - acc: 0.7655 - val_loss: 0.8269 - val_acc: 0.6747\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6259 - acc: 0.7615 - val_loss: 0.8188 - val_acc: 0.6627\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6301 - acc: 0.7507 - val_loss: 0.8218 - val_acc: 0.6867\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6262 - acc: 0.7534 - val_loss: 0.8229 - val_acc: 0.6627\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6243 - acc: 0.7480 - val_loss: 0.8330 - val_acc: 0.6867\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6220 - acc: 0.7628 - val_loss: 0.8040 - val_acc: 0.6627\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6178 - acc: 0.7547 - val_loss: 0.8157 - val_acc: 0.6988\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6164 - acc: 0.7547 - val_loss: 0.8237 - val_acc: 0.6747\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6164 - acc: 0.7642 - val_loss: 0.8226 - val_acc: 0.6867\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6195 - acc: 0.7574 - val_loss: 0.8243 - val_acc: 0.6988\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6199 - acc: 0.7547 - val_loss: 0.8345 - val_acc: 0.6627\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6148 - acc: 0.7534 - val_loss: 0.8201 - val_acc: 0.6988\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6123 - acc: 0.7642 - val_loss: 0.8223 - val_acc: 0.6747\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6093 - acc: 0.7574 - val_loss: 0.8108 - val_acc: 0.7349\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6088 - acc: 0.7561 - val_loss: 0.8283 - val_acc: 0.6627\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6091 - acc: 0.7642 - val_loss: 0.8200 - val_acc: 0.6988\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6154 - acc: 0.7520 - val_loss: 0.8115 - val_acc: 0.7108\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6090 - acc: 0.7588 - val_loss: 0.8177 - val_acc: 0.6747\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6064 - acc: 0.7709 - val_loss: 0.8321 - val_acc: 0.6988\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6090 - acc: 0.7601 - val_loss: 0.8232 - val_acc: 0.6386\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6026 - acc: 0.7601 - val_loss: 0.8185 - val_acc: 0.6747\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6017 - acc: 0.7601 - val_loss: 0.8228 - val_acc: 0.6627\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6012 - acc: 0.7574 - val_loss: 0.8116 - val_acc: 0.6867\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6073 - acc: 0.7588 - val_loss: 0.8255 - val_acc: 0.6747\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6089 - acc: 0.7615 - val_loss: 0.8388 - val_acc: 0.6988\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6015 - acc: 0.7749 - val_loss: 0.8135 - val_acc: 0.6747\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6026 - acc: 0.7615 - val_loss: 0.8206 - val_acc: 0.6988\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5954 - acc: 0.7601 - val_loss: 0.8206 - val_acc: 0.6867\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5975 - acc: 0.7547 - val_loss: 0.8308 - val_acc: 0.6506\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5972 - acc: 0.7547 - val_loss: 0.8252 - val_acc: 0.6747\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5961 - acc: 0.7655 - val_loss: 0.8265 - val_acc: 0.6747\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5965 - acc: 0.7574 - val_loss: 0.8135 - val_acc: 0.6988\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5945 - acc: 0.7628 - val_loss: 0.8309 - val_acc: 0.6867\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3841 - acc: 0.2251 - val_loss: 2.3548 - val_acc: 0.3494\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 2.3284 - acc: 0.2830 - val_loss: 2.2693 - val_acc: 0.3614\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 2.2166 - acc: 0.2871 - val_loss: 2.1014 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.0355 - acc: 0.2898 - val_loss: 1.8705 - val_acc: 0.3253\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.8523 - acc: 0.2898 - val_loss: 1.6934 - val_acc: 0.3614\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.7451 - acc: 0.2938 - val_loss: 1.6018 - val_acc: 0.3855\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6668 - acc: 0.3235 - val_loss: 1.5442 - val_acc: 0.3494\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.6037 - acc: 0.3437 - val_loss: 1.4954 - val_acc: 0.3614\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5538 - acc: 0.3598 - val_loss: 1.4598 - val_acc: 0.3855\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.5121 - acc: 0.3706 - val_loss: 1.4196 - val_acc: 0.3133\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.4786 - acc: 0.4016 - val_loss: 1.3882 - val_acc: 0.3855\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.4496 - acc: 0.4164 - val_loss: 1.3620 - val_acc: 0.3976\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.4208 - acc: 0.4380 - val_loss: 1.3457 - val_acc: 0.4096\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.3956 - acc: 0.4394 - val_loss: 1.3061 - val_acc: 0.4458\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3708 - acc: 0.4569 - val_loss: 1.2965 - val_acc: 0.4337\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.3466 - acc: 0.4704 - val_loss: 1.2709 - val_acc: 0.4458\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3238 - acc: 0.4717 - val_loss: 1.2663 - val_acc: 0.4578\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.2999 - acc: 0.4811 - val_loss: 1.2353 - val_acc: 0.4337\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.2777 - acc: 0.4865 - val_loss: 1.2381 - val_acc: 0.4699\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2627 - acc: 0.5013 - val_loss: 1.2307 - val_acc: 0.4337\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2384 - acc: 0.5175 - val_loss: 1.2131 - val_acc: 0.4819\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2166 - acc: 0.5175 - val_loss: 1.2010 - val_acc: 0.4578\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1955 - acc: 0.5148 - val_loss: 1.1927 - val_acc: 0.4819\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1787 - acc: 0.5458 - val_loss: 1.1888 - val_acc: 0.4458\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1586 - acc: 0.5323 - val_loss: 1.1661 - val_acc: 0.4458\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 60us/step - loss: 1.1379 - acc: 0.5526 - val_loss: 1.1726 - val_acc: 0.4699\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1213 - acc: 0.5633 - val_loss: 1.1512 - val_acc: 0.4699\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1069 - acc: 0.5741 - val_loss: 1.1313 - val_acc: 0.5181\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0860 - acc: 0.5930 - val_loss: 1.1283 - val_acc: 0.4699\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0724 - acc: 0.5836 - val_loss: 1.1373 - val_acc: 0.4940\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0572 - acc: 0.5970 - val_loss: 1.1114 - val_acc: 0.5301\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0426 - acc: 0.5889 - val_loss: 1.1106 - val_acc: 0.5060\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 1.0335 - acc: 0.5903 - val_loss: 1.0810 - val_acc: 0.5422\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0212 - acc: 0.5970 - val_loss: 1.1067 - val_acc: 0.5060\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.0045 - acc: 0.6119 - val_loss: 1.0717 - val_acc: 0.5783\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9978 - acc: 0.6024 - val_loss: 1.0696 - val_acc: 0.5542\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9838 - acc: 0.6186 - val_loss: 1.0830 - val_acc: 0.5181\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9702 - acc: 0.6253 - val_loss: 1.0484 - val_acc: 0.5663\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 59us/step - loss: 0.9663 - acc: 0.6186 - val_loss: 1.0480 - val_acc: 0.5422\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9530 - acc: 0.6280 - val_loss: 1.0446 - val_acc: 0.5301\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9433 - acc: 0.6442 - val_loss: 1.0493 - val_acc: 0.5422\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9329 - acc: 0.6415 - val_loss: 1.0315 - val_acc: 0.5663\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9226 - acc: 0.6415 - val_loss: 1.0413 - val_acc: 0.5663\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9199 - acc: 0.6523 - val_loss: 1.0142 - val_acc: 0.5542\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9112 - acc: 0.6415 - val_loss: 1.0275 - val_acc: 0.5663\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8991 - acc: 0.6671 - val_loss: 0.9996 - val_acc: 0.5904\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8921 - acc: 0.6563 - val_loss: 1.0095 - val_acc: 0.5663\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8817 - acc: 0.6685 - val_loss: 0.9992 - val_acc: 0.5542\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8732 - acc: 0.6644 - val_loss: 0.9900 - val_acc: 0.5663\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8721 - acc: 0.6644 - val_loss: 1.0002 - val_acc: 0.5783\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.8621 - acc: 0.6725 - val_loss: 0.9588 - val_acc: 0.5783\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8583 - acc: 0.6765 - val_loss: 0.9936 - val_acc: 0.5783\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8454 - acc: 0.6914 - val_loss: 0.9647 - val_acc: 0.5542\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8417 - acc: 0.6887 - val_loss: 0.9703 - val_acc: 0.5783\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8295 - acc: 0.6954 - val_loss: 0.9638 - val_acc: 0.5542\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8257 - acc: 0.7022 - val_loss: 0.9688 - val_acc: 0.5663\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8198 - acc: 0.6914 - val_loss: 0.9585 - val_acc: 0.6024\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.8146 - acc: 0.6995 - val_loss: 0.9594 - val_acc: 0.6024\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8113 - acc: 0.7022 - val_loss: 0.9491 - val_acc: 0.6024\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8043 - acc: 0.7022 - val_loss: 0.9422 - val_acc: 0.5904\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8110 - acc: 0.6968 - val_loss: 0.9610 - val_acc: 0.6145\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7967 - acc: 0.6887 - val_loss: 0.9504 - val_acc: 0.6145\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8000 - acc: 0.6968 - val_loss: 0.9402 - val_acc: 0.6024\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7851 - acc: 0.7224 - val_loss: 0.9246 - val_acc: 0.6265\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7798 - acc: 0.7089 - val_loss: 0.9231 - val_acc: 0.5904\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7738 - acc: 0.7143 - val_loss: 0.9350 - val_acc: 0.5783\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7703 - acc: 0.7102 - val_loss: 0.9271 - val_acc: 0.6506\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7707 - acc: 0.7197 - val_loss: 0.9296 - val_acc: 0.6024\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7608 - acc: 0.7143 - val_loss: 0.9188 - val_acc: 0.5904\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7607 - acc: 0.7089 - val_loss: 0.9302 - val_acc: 0.6265\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7569 - acc: 0.7143 - val_loss: 0.9223 - val_acc: 0.6265\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7513 - acc: 0.7197 - val_loss: 0.9171 - val_acc: 0.6265\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7473 - acc: 0.7143 - val_loss: 0.9118 - val_acc: 0.6386\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7402 - acc: 0.7210 - val_loss: 0.9046 - val_acc: 0.6145\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7364 - acc: 0.7291 - val_loss: 0.9142 - val_acc: 0.6265\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7296 - acc: 0.7210 - val_loss: 0.8833 - val_acc: 0.6145\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7248 - acc: 0.7385 - val_loss: 0.9148 - val_acc: 0.6024\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7282 - acc: 0.7291 - val_loss: 0.8765 - val_acc: 0.6024\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7248 - acc: 0.7143 - val_loss: 0.9266 - val_acc: 0.6145\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7180 - acc: 0.7197 - val_loss: 0.8972 - val_acc: 0.6145\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7168 - acc: 0.7332 - val_loss: 0.8948 - val_acc: 0.6506\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7207 - acc: 0.7264 - val_loss: 0.9051 - val_acc: 0.6386\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7115 - acc: 0.7291 - val_loss: 0.8936 - val_acc: 0.6386\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7064 - acc: 0.7291 - val_loss: 0.9097 - val_acc: 0.6265\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7079 - acc: 0.7264 - val_loss: 0.8935 - val_acc: 0.6265\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6989 - acc: 0.7358 - val_loss: 0.8997 - val_acc: 0.6265\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6975 - acc: 0.7358 - val_loss: 0.8820 - val_acc: 0.6265\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6934 - acc: 0.7264 - val_loss: 0.8901 - val_acc: 0.6386\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6902 - acc: 0.7318 - val_loss: 0.8911 - val_acc: 0.6386\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6909 - acc: 0.7183 - val_loss: 0.9045 - val_acc: 0.6265\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6949 - acc: 0.7251 - val_loss: 0.8685 - val_acc: 0.6386\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6909 - acc: 0.7305 - val_loss: 0.8882 - val_acc: 0.6145\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6806 - acc: 0.7439 - val_loss: 0.8688 - val_acc: 0.6265\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6755 - acc: 0.7412 - val_loss: 0.8904 - val_acc: 0.6506\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6757 - acc: 0.7278 - val_loss: 0.8795 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6767 - acc: 0.7399 - val_loss: 0.8767 - val_acc: 0.6386\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6729 - acc: 0.7358 - val_loss: 0.8778 - val_acc: 0.6386\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6715 - acc: 0.7412 - val_loss: 0.8761 - val_acc: 0.6145\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6680 - acc: 0.7385 - val_loss: 0.8705 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6689 - acc: 0.7493 - val_loss: 0.8678 - val_acc: 0.6024\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6629 - acc: 0.7466 - val_loss: 0.8844 - val_acc: 0.6386\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6640 - acc: 0.7345 - val_loss: 0.8660 - val_acc: 0.6024\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6573 - acc: 0.7480 - val_loss: 0.8880 - val_acc: 0.6506\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6552 - acc: 0.7466 - val_loss: 0.8705 - val_acc: 0.6265\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6566 - acc: 0.7493 - val_loss: 0.8776 - val_acc: 0.6506\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6600 - acc: 0.7372 - val_loss: 0.8638 - val_acc: 0.6265\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6531 - acc: 0.7399 - val_loss: 0.8674 - val_acc: 0.6506\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6557 - acc: 0.7332 - val_loss: 0.8777 - val_acc: 0.6024\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6552 - acc: 0.7426 - val_loss: 0.8733 - val_acc: 0.6265\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6448 - acc: 0.7372 - val_loss: 0.8755 - val_acc: 0.6506\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6389 - acc: 0.7493 - val_loss: 0.8642 - val_acc: 0.6145\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6494 - acc: 0.7480 - val_loss: 0.8687 - val_acc: 0.6627\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6361 - acc: 0.7507 - val_loss: 0.8653 - val_acc: 0.6386\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6360 - acc: 0.7534 - val_loss: 0.8630 - val_acc: 0.6386\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6393 - acc: 0.7520 - val_loss: 0.8640 - val_acc: 0.6627\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6359 - acc: 0.7493 - val_loss: 0.8904 - val_acc: 0.6145\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6361 - acc: 0.7547 - val_loss: 0.8520 - val_acc: 0.6024\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6358 - acc: 0.7466 - val_loss: 0.8776 - val_acc: 0.6506\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6323 - acc: 0.7385 - val_loss: 0.8798 - val_acc: 0.6145\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6278 - acc: 0.7547 - val_loss: 0.8584 - val_acc: 0.6265\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6267 - acc: 0.7561 - val_loss: 0.8635 - val_acc: 0.6506\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6440 - acc: 0.7507 - val_loss: 0.9085 - val_acc: 0.6265\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6233 - acc: 0.7480 - val_loss: 0.8689 - val_acc: 0.6627\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6274 - acc: 0.7493 - val_loss: 0.8743 - val_acc: 0.6265\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6240 - acc: 0.7588 - val_loss: 0.8682 - val_acc: 0.6145\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6213 - acc: 0.7561 - val_loss: 0.9024 - val_acc: 0.6627\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6319 - acc: 0.7412 - val_loss: 0.8957 - val_acc: 0.6145\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6269 - acc: 0.7588 - val_loss: 0.8613 - val_acc: 0.6506\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6203 - acc: 0.7466 - val_loss: 0.8574 - val_acc: 0.6386\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6328 - acc: 0.7561 - val_loss: 0.8729 - val_acc: 0.6386\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6162 - acc: 0.7534 - val_loss: 0.8989 - val_acc: 0.6265\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6082 - acc: 0.7628 - val_loss: 0.8493 - val_acc: 0.6386\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6135 - acc: 0.7520 - val_loss: 0.8785 - val_acc: 0.6386\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6131 - acc: 0.7655 - val_loss: 0.8802 - val_acc: 0.6386\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6055 - acc: 0.7655 - val_loss: 0.8591 - val_acc: 0.6506\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6042 - acc: 0.7561 - val_loss: 0.8960 - val_acc: 0.6145\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6100 - acc: 0.7655 - val_loss: 0.8859 - val_acc: 0.6265\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6171 - acc: 0.7561 - val_loss: 0.8770 - val_acc: 0.6265\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6189 - acc: 0.7466 - val_loss: 0.8674 - val_acc: 0.6386\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6029 - acc: 0.7547 - val_loss: 0.8830 - val_acc: 0.6506\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6022 - acc: 0.7588 - val_loss: 0.8647 - val_acc: 0.6506\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.5992 - acc: 0.7655 - val_loss: 0.8757 - val_acc: 0.6386\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6001 - acc: 0.7695 - val_loss: 0.8816 - val_acc: 0.6145\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5976 - acc: 0.7722 - val_loss: 0.8833 - val_acc: 0.6265\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5976 - acc: 0.7695 - val_loss: 0.8773 - val_acc: 0.6627\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5943 - acc: 0.7682 - val_loss: 0.8750 - val_acc: 0.6627\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5968 - acc: 0.7561 - val_loss: 0.8817 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5976 - acc: 0.7736 - val_loss: 0.8819 - val_acc: 0.6145\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5994 - acc: 0.7736 - val_loss: 0.8632 - val_acc: 0.6627\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5951 - acc: 0.7682 - val_loss: 0.8752 - val_acc: 0.6506\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5967 - acc: 0.7682 - val_loss: 0.8967 - val_acc: 0.6386\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6079 - acc: 0.7615 - val_loss: 0.8667 - val_acc: 0.6386\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6131 - acc: 0.7682 - val_loss: 0.8978 - val_acc: 0.6024\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5881 - acc: 0.7736 - val_loss: 0.8679 - val_acc: 0.6506\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5946 - acc: 0.7709 - val_loss: 0.8786 - val_acc: 0.6506\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5853 - acc: 0.7722 - val_loss: 0.8935 - val_acc: 0.6506\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5896 - acc: 0.7790 - val_loss: 0.8783 - val_acc: 0.6506\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5872 - acc: 0.7601 - val_loss: 0.8685 - val_acc: 0.6506\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5830 - acc: 0.7668 - val_loss: 0.8808 - val_acc: 0.6506\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5820 - acc: 0.7709 - val_loss: 0.8875 - val_acc: 0.6627\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5773 - acc: 0.7844 - val_loss: 0.8866 - val_acc: 0.6386\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5813 - acc: 0.7763 - val_loss: 0.8860 - val_acc: 0.6506\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5815 - acc: 0.7695 - val_loss: 0.8999 - val_acc: 0.6386\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5773 - acc: 0.7709 - val_loss: 0.8778 - val_acc: 0.6506\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5772 - acc: 0.7682 - val_loss: 0.8808 - val_acc: 0.6506\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5740 - acc: 0.7776 - val_loss: 0.8811 - val_acc: 0.6386\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5701 - acc: 0.7736 - val_loss: 0.8947 - val_acc: 0.6627\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5726 - acc: 0.7830 - val_loss: 0.8769 - val_acc: 0.6506\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5898 - acc: 0.7574 - val_loss: 0.9036 - val_acc: 0.6024\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5710 - acc: 0.7722 - val_loss: 0.8985 - val_acc: 0.6627\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5788 - acc: 0.7830 - val_loss: 0.8993 - val_acc: 0.6265\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5674 - acc: 0.7736 - val_loss: 0.8907 - val_acc: 0.6627\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5723 - acc: 0.7830 - val_loss: 0.8932 - val_acc: 0.6506\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5636 - acc: 0.7642 - val_loss: 0.8982 - val_acc: 0.6265\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5740 - acc: 0.7722 - val_loss: 0.8987 - val_acc: 0.6506\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5743 - acc: 0.7763 - val_loss: 0.8705 - val_acc: 0.6627\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5794 - acc: 0.7749 - val_loss: 0.8735 - val_acc: 0.6627\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5707 - acc: 0.7709 - val_loss: 0.9128 - val_acc: 0.6024\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5669 - acc: 0.7830 - val_loss: 0.9014 - val_acc: 0.7108\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5642 - acc: 0.7722 - val_loss: 0.8891 - val_acc: 0.6386\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5645 - acc: 0.7803 - val_loss: 0.9007 - val_acc: 0.6024\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5730 - acc: 0.7749 - val_loss: 0.8898 - val_acc: 0.6747\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5662 - acc: 0.7803 - val_loss: 0.9073 - val_acc: 0.6386\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5732 - acc: 0.7763 - val_loss: 0.8771 - val_acc: 0.6386\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5647 - acc: 0.7763 - val_loss: 0.9129 - val_acc: 0.6867\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5599 - acc: 0.7749 - val_loss: 0.8819 - val_acc: 0.6747\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5614 - acc: 0.7763 - val_loss: 0.9157 - val_acc: 0.6506\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.5627 - acc: 0.7790 - val_loss: 0.8700 - val_acc: 0.6627\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5553 - acc: 0.7911 - val_loss: 0.8994 - val_acc: 0.6506\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5548 - acc: 0.7817 - val_loss: 0.8868 - val_acc: 0.6506\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5553 - acc: 0.7817 - val_loss: 0.9123 - val_acc: 0.6386\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5585 - acc: 0.7776 - val_loss: 0.9031 - val_acc: 0.6627\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5602 - acc: 0.7830 - val_loss: 0.8858 - val_acc: 0.6386\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5620 - acc: 0.7695 - val_loss: 0.9023 - val_acc: 0.6747\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5540 - acc: 0.7844 - val_loss: 0.8940 - val_acc: 0.6627\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5602 - acc: 0.7830 - val_loss: 0.9062 - val_acc: 0.6747\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5566 - acc: 0.7884 - val_loss: 0.9111 - val_acc: 0.6627\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5513 - acc: 0.7938 - val_loss: 0.8889 - val_acc: 0.6988\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5422 - acc: 0.7857 - val_loss: 0.9131 - val_acc: 0.6747\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5475 - acc: 0.7830 - val_loss: 0.8949 - val_acc: 0.6506\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3700 - acc: 0.2372 - val_loss: 2.3178 - val_acc: 0.3373\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 2.2747 - acc: 0.2547 - val_loss: 2.1609 - val_acc: 0.3373\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.0984 - acc: 0.2547 - val_loss: 1.9101 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.9020 - acc: 0.2547 - val_loss: 1.7328 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.7822 - acc: 0.2749 - val_loss: 1.6409 - val_acc: 0.3614\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.6876 - acc: 0.3154 - val_loss: 1.5793 - val_acc: 0.3494\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.6092 - acc: 0.3518 - val_loss: 1.5065 - val_acc: 0.3373\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.5467 - acc: 0.3827 - val_loss: 1.4662 - val_acc: 0.3253\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.5041 - acc: 0.3989 - val_loss: 1.4204 - val_acc: 0.3855\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.4657 - acc: 0.4137 - val_loss: 1.3853 - val_acc: 0.4217\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.4324 - acc: 0.4420 - val_loss: 1.3533 - val_acc: 0.4217\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.4027 - acc: 0.4434 - val_loss: 1.3346 - val_acc: 0.4337\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3727 - acc: 0.4609 - val_loss: 1.3103 - val_acc: 0.4217\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3485 - acc: 0.4501 - val_loss: 1.2793 - val_acc: 0.4337\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.3187 - acc: 0.4811 - val_loss: 1.2600 - val_acc: 0.4217\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2965 - acc: 0.4865 - val_loss: 1.2645 - val_acc: 0.4458\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2675 - acc: 0.4960 - val_loss: 1.2437 - val_acc: 0.4217\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.2453 - acc: 0.5067 - val_loss: 1.2235 - val_acc: 0.4337\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 1.2166 - acc: 0.5054 - val_loss: 1.2188 - val_acc: 0.4217\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.2006 - acc: 0.5175 - val_loss: 1.2030 - val_acc: 0.4337\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1782 - acc: 0.5350 - val_loss: 1.1865 - val_acc: 0.4337\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.1536 - acc: 0.5270 - val_loss: 1.1880 - val_acc: 0.4578\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1396 - acc: 0.5512 - val_loss: 1.1667 - val_acc: 0.4699\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.1241 - acc: 0.5539 - val_loss: 1.1605 - val_acc: 0.4699\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1057 - acc: 0.5553 - val_loss: 1.1459 - val_acc: 0.4819\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0864 - acc: 0.5701 - val_loss: 1.1419 - val_acc: 0.4458\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0696 - acc: 0.5863 - val_loss: 1.1454 - val_acc: 0.4699\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 56us/step - loss: 1.0582 - acc: 0.5930 - val_loss: 1.1188 - val_acc: 0.4819\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 1.0406 - acc: 0.5930 - val_loss: 1.1169 - val_acc: 0.4578\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 1.0285 - acc: 0.5768 - val_loss: 1.1143 - val_acc: 0.4819\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0157 - acc: 0.6065 - val_loss: 1.0994 - val_acc: 0.5181\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9967 - acc: 0.6226 - val_loss: 1.0898 - val_acc: 0.5060\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9868 - acc: 0.6294 - val_loss: 1.0826 - val_acc: 0.5181\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9769 - acc: 0.6240 - val_loss: 1.0779 - val_acc: 0.5301\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9644 - acc: 0.6388 - val_loss: 1.0713 - val_acc: 0.5422\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9566 - acc: 0.6388 - val_loss: 1.0474 - val_acc: 0.5904\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9449 - acc: 0.6469 - val_loss: 1.0650 - val_acc: 0.5301\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9439 - acc: 0.6267 - val_loss: 1.0411 - val_acc: 0.5542\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.9311 - acc: 0.6577 - val_loss: 1.0374 - val_acc: 0.5542\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9186 - acc: 0.6577 - val_loss: 1.0423 - val_acc: 0.5783\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9093 - acc: 0.6698 - val_loss: 1.0286 - val_acc: 0.5542\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9043 - acc: 0.6442 - val_loss: 1.0239 - val_acc: 0.5422\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8926 - acc: 0.6712 - val_loss: 1.0112 - val_acc: 0.5783\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.6846 - val_loss: 1.0158 - val_acc: 0.5422\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.8686 - acc: 0.6765 - val_loss: 1.0164 - val_acc: 0.5904\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.8625 - acc: 0.6887 - val_loss: 1.0068 - val_acc: 0.5542\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8508 - acc: 0.6887 - val_loss: 0.9847 - val_acc: 0.6024\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.6819 - val_loss: 0.9859 - val_acc: 0.6024\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8407 - acc: 0.6927 - val_loss: 0.9926 - val_acc: 0.5542\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8285 - acc: 0.6941 - val_loss: 0.9782 - val_acc: 0.5542\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8202 - acc: 0.7008 - val_loss: 0.9719 - val_acc: 0.5783\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8142 - acc: 0.6968 - val_loss: 0.9937 - val_acc: 0.5783\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8053 - acc: 0.7102 - val_loss: 0.9731 - val_acc: 0.5904\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.8036 - acc: 0.6927 - val_loss: 0.9709 - val_acc: 0.5663\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7989 - acc: 0.6954 - val_loss: 0.9907 - val_acc: 0.5904\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7879 - acc: 0.7210 - val_loss: 0.9762 - val_acc: 0.5663\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7819 - acc: 0.7156 - val_loss: 0.9562 - val_acc: 0.6747\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7812 - acc: 0.7116 - val_loss: 0.9551 - val_acc: 0.5542\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7707 - acc: 0.7183 - val_loss: 0.9771 - val_acc: 0.6386\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7635 - acc: 0.7197 - val_loss: 0.9636 - val_acc: 0.5663\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7577 - acc: 0.7210 - val_loss: 0.9459 - val_acc: 0.6024\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7558 - acc: 0.7183 - val_loss: 0.9609 - val_acc: 0.6627\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7435 - acc: 0.7224 - val_loss: 0.9519 - val_acc: 0.6265\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7437 - acc: 0.7332 - val_loss: 0.9568 - val_acc: 0.6265\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7345 - acc: 0.7345 - val_loss: 0.9380 - val_acc: 0.6024\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7332 - acc: 0.7264 - val_loss: 0.9477 - val_acc: 0.6506\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7288 - acc: 0.7385 - val_loss: 0.9526 - val_acc: 0.6024\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.7272 - acc: 0.7210 - val_loss: 0.9638 - val_acc: 0.6627\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7262 - acc: 0.7197 - val_loss: 0.9568 - val_acc: 0.6145\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7168 - acc: 0.7332 - val_loss: 0.9383 - val_acc: 0.6627\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7126 - acc: 0.7278 - val_loss: 0.9570 - val_acc: 0.6386\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.7054 - acc: 0.7345 - val_loss: 0.9205 - val_acc: 0.6145\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.7025 - acc: 0.7412 - val_loss: 0.9483 - val_acc: 0.5783\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6999 - acc: 0.7426 - val_loss: 0.9428 - val_acc: 0.6145\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.7305 - val_loss: 0.9433 - val_acc: 0.6145\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6934 - acc: 0.7358 - val_loss: 0.9434 - val_acc: 0.6265\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6864 - acc: 0.7412 - val_loss: 0.9338 - val_acc: 0.6386\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6840 - acc: 0.7507 - val_loss: 0.9256 - val_acc: 0.6386\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6826 - acc: 0.7466 - val_loss: 0.9434 - val_acc: 0.6386\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6803 - acc: 0.7358 - val_loss: 0.9359 - val_acc: 0.6386\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6751 - acc: 0.7372 - val_loss: 0.9897 - val_acc: 0.6145\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6811 - acc: 0.7372 - val_loss: 0.9611 - val_acc: 0.6024\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6748 - acc: 0.7385 - val_loss: 0.9197 - val_acc: 0.6265\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6686 - acc: 0.7453 - val_loss: 0.9639 - val_acc: 0.6386\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6661 - acc: 0.7480 - val_loss: 0.9227 - val_acc: 0.6386\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6704 - acc: 0.7453 - val_loss: 0.9377 - val_acc: 0.6386\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6566 - acc: 0.7547 - val_loss: 0.9415 - val_acc: 0.6265\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6564 - acc: 0.7507 - val_loss: 0.9479 - val_acc: 0.6265\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6505 - acc: 0.7493 - val_loss: 0.9103 - val_acc: 0.6386\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6585 - acc: 0.7466 - val_loss: 0.9399 - val_acc: 0.6024\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6497 - acc: 0.7588 - val_loss: 0.9383 - val_acc: 0.6627\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6413 - acc: 0.7507 - val_loss: 0.9432 - val_acc: 0.6386\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 67us/step - loss: 0.6402 - acc: 0.7439 - val_loss: 0.9209 - val_acc: 0.6265\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 0.6421 - acc: 0.7642 - val_loss: 0.9312 - val_acc: 0.6024\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6427 - acc: 0.7466 - val_loss: 0.9428 - val_acc: 0.6627\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6358 - acc: 0.7615 - val_loss: 0.9363 - val_acc: 0.6386\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6298 - acc: 0.7520 - val_loss: 0.9484 - val_acc: 0.6145\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6250 - acc: 0.7655 - val_loss: 0.9474 - val_acc: 0.6386\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6288 - acc: 0.7547 - val_loss: 0.9304 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6170 - acc: 0.7655 - val_loss: 0.9758 - val_acc: 0.6265\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6269 - acc: 0.7561 - val_loss: 0.9090 - val_acc: 0.6386\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6231 - acc: 0.7642 - val_loss: 0.9390 - val_acc: 0.6265\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6125 - acc: 0.7574 - val_loss: 0.9311 - val_acc: 0.6265\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6159 - acc: 0.7642 - val_loss: 0.9476 - val_acc: 0.6145\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6145 - acc: 0.7615 - val_loss: 0.9383 - val_acc: 0.6145\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6116 - acc: 0.7642 - val_loss: 0.9270 - val_acc: 0.6627\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6127 - acc: 0.7682 - val_loss: 0.9792 - val_acc: 0.6265\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6133 - acc: 0.7615 - val_loss: 0.9411 - val_acc: 0.6386\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6091 - acc: 0.7790 - val_loss: 0.9226 - val_acc: 0.6386\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6068 - acc: 0.7668 - val_loss: 0.9412 - val_acc: 0.6145\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.6062 - acc: 0.7601 - val_loss: 0.9476 - val_acc: 0.6024\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6059 - acc: 0.7628 - val_loss: 0.9418 - val_acc: 0.6867\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6096 - acc: 0.7695 - val_loss: 0.9578 - val_acc: 0.6145\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5955 - acc: 0.7682 - val_loss: 0.9218 - val_acc: 0.6506\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5995 - acc: 0.7655 - val_loss: 0.9481 - val_acc: 0.6145\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5902 - acc: 0.7817 - val_loss: 0.9555 - val_acc: 0.6627\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5917 - acc: 0.7642 - val_loss: 0.9198 - val_acc: 0.6265\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5975 - acc: 0.7588 - val_loss: 0.9541 - val_acc: 0.6145\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5934 - acc: 0.7695 - val_loss: 0.9235 - val_acc: 0.6265\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5895 - acc: 0.7817 - val_loss: 0.9647 - val_acc: 0.6265\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5959 - acc: 0.7709 - val_loss: 0.9463 - val_acc: 0.6265\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5886 - acc: 0.7709 - val_loss: 0.9303 - val_acc: 0.5904\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5929 - acc: 0.7749 - val_loss: 0.9529 - val_acc: 0.6265\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5868 - acc: 0.7655 - val_loss: 0.9591 - val_acc: 0.6386\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5788 - acc: 0.7857 - val_loss: 0.9518 - val_acc: 0.6024\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5804 - acc: 0.7776 - val_loss: 0.9242 - val_acc: 0.6386\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5767 - acc: 0.7871 - val_loss: 0.9645 - val_acc: 0.6145\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5819 - acc: 0.7749 - val_loss: 0.9244 - val_acc: 0.6506\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5784 - acc: 0.7911 - val_loss: 0.9543 - val_acc: 0.6024\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5927 - acc: 0.7709 - val_loss: 0.9294 - val_acc: 0.6145\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5718 - acc: 0.7803 - val_loss: 0.9524 - val_acc: 0.6265\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.5800 - acc: 0.7682 - val_loss: 0.9996 - val_acc: 0.6265\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5865 - acc: 0.7763 - val_loss: 0.9366 - val_acc: 0.6506\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5750 - acc: 0.7736 - val_loss: 0.9529 - val_acc: 0.6627\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5656 - acc: 0.7803 - val_loss: 0.9429 - val_acc: 0.5904\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5618 - acc: 0.7830 - val_loss: 0.9466 - val_acc: 0.6265\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5605 - acc: 0.7925 - val_loss: 0.9618 - val_acc: 0.5904\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.5630 - acc: 0.7803 - val_loss: 0.9410 - val_acc: 0.6145\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5593 - acc: 0.7857 - val_loss: 0.9580 - val_acc: 0.6145\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5523 - acc: 0.7898 - val_loss: 0.9413 - val_acc: 0.6386\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5568 - acc: 0.7951 - val_loss: 0.9531 - val_acc: 0.6265\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5604 - acc: 0.7790 - val_loss: 0.9413 - val_acc: 0.6265\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5611 - acc: 0.7911 - val_loss: 0.9716 - val_acc: 0.6506\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5564 - acc: 0.7817 - val_loss: 0.9661 - val_acc: 0.6386\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5542 - acc: 0.7898 - val_loss: 0.9348 - val_acc: 0.6265\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5636 - acc: 0.7830 - val_loss: 0.9461 - val_acc: 0.6386\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5535 - acc: 0.7951 - val_loss: 0.9348 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5435 - acc: 0.7938 - val_loss: 0.9810 - val_acc: 0.6145\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5579 - acc: 0.7749 - val_loss: 0.9445 - val_acc: 0.6627\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.5611 - acc: 0.7790 - val_loss: 0.9739 - val_acc: 0.6386\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5405 - acc: 0.7965 - val_loss: 0.9550 - val_acc: 0.6386\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5486 - acc: 0.7790 - val_loss: 0.9649 - val_acc: 0.6265\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5412 - acc: 0.7992 - val_loss: 0.9550 - val_acc: 0.6145\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5470 - acc: 0.7844 - val_loss: 0.9746 - val_acc: 0.6627\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5479 - acc: 0.7884 - val_loss: 0.9938 - val_acc: 0.6145\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5375 - acc: 0.7844 - val_loss: 0.9376 - val_acc: 0.6386\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5424 - acc: 0.7992 - val_loss: 0.9805 - val_acc: 0.6265\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5450 - acc: 0.7938 - val_loss: 0.9309 - val_acc: 0.6386\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5406 - acc: 0.7911 - val_loss: 0.9561 - val_acc: 0.6145\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5387 - acc: 0.7978 - val_loss: 1.0174 - val_acc: 0.6265\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5368 - acc: 0.7871 - val_loss: 0.9626 - val_acc: 0.6265\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.5463 - acc: 0.7884 - val_loss: 0.9719 - val_acc: 0.6145\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5485 - acc: 0.7830 - val_loss: 0.9695 - val_acc: 0.6145\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5338 - acc: 0.7938 - val_loss: 0.9874 - val_acc: 0.6265\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5335 - acc: 0.7790 - val_loss: 1.0009 - val_acc: 0.6265\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5330 - acc: 0.7938 - val_loss: 0.9608 - val_acc: 0.6506\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5285 - acc: 0.8046 - val_loss: 0.9770 - val_acc: 0.6386\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5305 - acc: 0.7992 - val_loss: 0.9968 - val_acc: 0.6386\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5346 - acc: 0.7951 - val_loss: 0.9644 - val_acc: 0.6145\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5332 - acc: 0.7938 - val_loss: 0.9871 - val_acc: 0.6145\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5359 - acc: 0.8019 - val_loss: 0.9634 - val_acc: 0.6386\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5197 - acc: 0.8140 - val_loss: 0.9524 - val_acc: 0.6386\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5162 - acc: 0.8100 - val_loss: 0.9811 - val_acc: 0.6145\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5143 - acc: 0.8100 - val_loss: 0.9795 - val_acc: 0.6145\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5150 - acc: 0.8032 - val_loss: 0.9726 - val_acc: 0.6265\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5239 - acc: 0.8019 - val_loss: 0.9805 - val_acc: 0.6145\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5152 - acc: 0.7911 - val_loss: 0.9836 - val_acc: 0.6265\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5156 - acc: 0.8073 - val_loss: 1.0095 - val_acc: 0.6145\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5196 - acc: 0.8019 - val_loss: 1.0201 - val_acc: 0.6265\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5309 - acc: 0.8005 - val_loss: 0.9675 - val_acc: 0.6024\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5152 - acc: 0.7978 - val_loss: 0.9821 - val_acc: 0.6386\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5163 - acc: 0.8019 - val_loss: 0.9946 - val_acc: 0.6386\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5059 - acc: 0.8019 - val_loss: 1.0046 - val_acc: 0.6145\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5186 - acc: 0.8032 - val_loss: 1.0139 - val_acc: 0.5904\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5069 - acc: 0.8100 - val_loss: 0.9782 - val_acc: 0.6386\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5083 - acc: 0.8100 - val_loss: 0.9801 - val_acc: 0.6145\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5014 - acc: 0.8073 - val_loss: 1.0087 - val_acc: 0.6145\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5046 - acc: 0.8005 - val_loss: 0.9916 - val_acc: 0.6024\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5059 - acc: 0.8086 - val_loss: 0.9759 - val_acc: 0.6145\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4953 - acc: 0.8261 - val_loss: 1.0103 - val_acc: 0.6265\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.4985 - acc: 0.8046 - val_loss: 1.0073 - val_acc: 0.6145\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5009 - acc: 0.8113 - val_loss: 0.9830 - val_acc: 0.6747\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.4988 - acc: 0.8221 - val_loss: 1.0042 - val_acc: 0.6265\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4991 - acc: 0.7978 - val_loss: 1.0187 - val_acc: 0.6024\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5026 - acc: 0.8194 - val_loss: 0.9879 - val_acc: 0.6506\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.4939 - acc: 0.8086 - val_loss: 1.0044 - val_acc: 0.6265\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.4923 - acc: 0.8154 - val_loss: 1.0088 - val_acc: 0.6386\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.4946 - acc: 0.8181 - val_loss: 1.0045 - val_acc: 0.6265\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.4872 - acc: 0.8154 - val_loss: 1.0092 - val_acc: 0.6265\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.4968 - acc: 0.8086 - val_loss: 0.9978 - val_acc: 0.6265\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 1s 2ms/step - loss: 2.3903 - acc: 0.2129 - val_loss: 2.3730 - val_acc: 0.3373\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.3588 - acc: 0.2561 - val_loss: 2.3207 - val_acc: 0.3373\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 2.2952 - acc: 0.2547 - val_loss: 2.2201 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 2.1811 - acc: 0.2547 - val_loss: 2.0485 - val_acc: 0.3373\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 2.0174 - acc: 0.2547 - val_loss: 1.8520 - val_acc: 0.3494\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.8837 - acc: 0.2642 - val_loss: 1.7263 - val_acc: 0.3614\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.8004 - acc: 0.2736 - val_loss: 1.6684 - val_acc: 0.3494\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.7355 - acc: 0.3167 - val_loss: 1.6147 - val_acc: 0.3855\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.6709 - acc: 0.3329 - val_loss: 1.5653 - val_acc: 0.3494\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 1.6162 - acc: 0.3410 - val_loss: 1.5239 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.5663 - acc: 0.3491 - val_loss: 1.4785 - val_acc: 0.3494\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.5307 - acc: 0.3949 - val_loss: 1.4483 - val_acc: 0.3735\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4958 - acc: 0.3935 - val_loss: 1.3938 - val_acc: 0.3735\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.4647 - acc: 0.4245 - val_loss: 1.3938 - val_acc: 0.3735\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.4418 - acc: 0.4232 - val_loss: 1.3745 - val_acc: 0.3855\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.4163 - acc: 0.4272 - val_loss: 1.3411 - val_acc: 0.3855\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3960 - acc: 0.4474 - val_loss: 1.3259 - val_acc: 0.3855\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.3759 - acc: 0.4434 - val_loss: 1.3125 - val_acc: 0.4096\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.3562 - acc: 0.4434 - val_loss: 1.2974 - val_acc: 0.4337\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3353 - acc: 0.4528 - val_loss: 1.2921 - val_acc: 0.4096\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3163 - acc: 0.4609 - val_loss: 1.2810 - val_acc: 0.4096\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.3002 - acc: 0.4717 - val_loss: 1.2515 - val_acc: 0.4337\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.2803 - acc: 0.4757 - val_loss: 1.2541 - val_acc: 0.4337\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.2620 - acc: 0.4825 - val_loss: 1.2396 - val_acc: 0.4217\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2431 - acc: 0.4798 - val_loss: 1.2260 - val_acc: 0.4337\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2274 - acc: 0.4852 - val_loss: 1.2265 - val_acc: 0.4217\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.2102 - acc: 0.5013 - val_loss: 1.2089 - val_acc: 0.4578\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.1935 - acc: 0.5081 - val_loss: 1.1971 - val_acc: 0.4096\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1791 - acc: 0.5081 - val_loss: 1.1948 - val_acc: 0.4458\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.1635 - acc: 0.5162 - val_loss: 1.1881 - val_acc: 0.4217\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.1514 - acc: 0.5310 - val_loss: 1.1670 - val_acc: 0.4699\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.1347 - acc: 0.5404 - val_loss: 1.1627 - val_acc: 0.4337\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1209 - acc: 0.5458 - val_loss: 1.1613 - val_acc: 0.4699\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1085 - acc: 0.5512 - val_loss: 1.1491 - val_acc: 0.4819\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.0950 - acc: 0.5553 - val_loss: 1.1395 - val_acc: 0.5060\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0828 - acc: 0.5647 - val_loss: 1.1222 - val_acc: 0.4940\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0731 - acc: 0.5687 - val_loss: 1.1230 - val_acc: 0.5060\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.0620 - acc: 0.5768 - val_loss: 1.1187 - val_acc: 0.5060\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.0519 - acc: 0.5795 - val_loss: 1.1011 - val_acc: 0.5301\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.0386 - acc: 0.5782 - val_loss: 1.0918 - val_acc: 0.5542\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.0288 - acc: 0.5863 - val_loss: 1.0953 - val_acc: 0.5301\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0179 - acc: 0.5943 - val_loss: 1.0749 - val_acc: 0.5301\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0111 - acc: 0.5930 - val_loss: 1.0776 - val_acc: 0.5783\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0038 - acc: 0.5970 - val_loss: 1.0755 - val_acc: 0.5422\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9923 - acc: 0.5984 - val_loss: 1.0518 - val_acc: 0.5904\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.9835 - acc: 0.6092 - val_loss: 1.0503 - val_acc: 0.6024\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9739 - acc: 0.6065 - val_loss: 1.0494 - val_acc: 0.5783\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9698 - acc: 0.6119 - val_loss: 1.0409 - val_acc: 0.6265\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9598 - acc: 0.6119 - val_loss: 1.0338 - val_acc: 0.5904\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.9515 - acc: 0.6280 - val_loss: 1.0146 - val_acc: 0.6024\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9489 - acc: 0.6253 - val_loss: 1.0388 - val_acc: 0.5783\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.9396 - acc: 0.6173 - val_loss: 1.0022 - val_acc: 0.6265\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9289 - acc: 0.6469 - val_loss: 1.0121 - val_acc: 0.6024\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9242 - acc: 0.6509 - val_loss: 0.9988 - val_acc: 0.6506\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9197 - acc: 0.6442 - val_loss: 0.9927 - val_acc: 0.6265\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.6523 - val_loss: 0.9895 - val_acc: 0.6506\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9003 - acc: 0.6536 - val_loss: 0.9915 - val_acc: 0.6386\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8968 - acc: 0.6482 - val_loss: 0.9867 - val_acc: 0.6627\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8853 - acc: 0.6765 - val_loss: 0.9568 - val_acc: 0.6627\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8814 - acc: 0.6658 - val_loss: 0.9705 - val_acc: 0.5904\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8811 - acc: 0.6631 - val_loss: 0.9691 - val_acc: 0.6265\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.6887 - val_loss: 0.9523 - val_acc: 0.6627\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.8647 - acc: 0.6739 - val_loss: 0.9588 - val_acc: 0.6265\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8616 - acc: 0.6792 - val_loss: 0.9490 - val_acc: 0.6145\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.6927 - val_loss: 0.9401 - val_acc: 0.6627\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8484 - acc: 0.6833 - val_loss: 0.9339 - val_acc: 0.6506\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8417 - acc: 0.6900 - val_loss: 0.9394 - val_acc: 0.6145\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8384 - acc: 0.7062 - val_loss: 0.9274 - val_acc: 0.6506\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8306 - acc: 0.6900 - val_loss: 0.9335 - val_acc: 0.6145\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.8250 - acc: 0.7089 - val_loss: 0.9242 - val_acc: 0.6506\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8247 - acc: 0.6914 - val_loss: 0.9159 - val_acc: 0.6506\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8205 - acc: 0.7035 - val_loss: 0.9114 - val_acc: 0.6627\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.8120 - acc: 0.7049 - val_loss: 0.9142 - val_acc: 0.6506\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8043 - acc: 0.7075 - val_loss: 0.9079 - val_acc: 0.6506\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8023 - acc: 0.7035 - val_loss: 0.9125 - val_acc: 0.6988\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8005 - acc: 0.7170 - val_loss: 0.8992 - val_acc: 0.6506\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7990 - acc: 0.7022 - val_loss: 0.9047 - val_acc: 0.7108\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7942 - acc: 0.7183 - val_loss: 0.8874 - val_acc: 0.6867\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7919 - acc: 0.7278 - val_loss: 0.9116 - val_acc: 0.6386\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7826 - acc: 0.7332 - val_loss: 0.8928 - val_acc: 0.6867\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7815 - acc: 0.7210 - val_loss: 0.8766 - val_acc: 0.6867\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7750 - acc: 0.7170 - val_loss: 0.8754 - val_acc: 0.6867\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7723 - acc: 0.7156 - val_loss: 0.8829 - val_acc: 0.6627\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7716 - acc: 0.7210 - val_loss: 0.9040 - val_acc: 0.6747\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7643 - acc: 0.7291 - val_loss: 0.8799 - val_acc: 0.6627\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7626 - acc: 0.7305 - val_loss: 0.8759 - val_acc: 0.6627\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7543 - acc: 0.7291 - val_loss: 0.8794 - val_acc: 0.6867\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7517 - acc: 0.7358 - val_loss: 0.8732 - val_acc: 0.6506\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7500 - acc: 0.7345 - val_loss: 0.8721 - val_acc: 0.6506\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7502 - acc: 0.7183 - val_loss: 0.8768 - val_acc: 0.7108\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.7453 - acc: 0.7278 - val_loss: 0.8771 - val_acc: 0.6627\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7368 - acc: 0.7385 - val_loss: 0.8739 - val_acc: 0.6988\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7366 - acc: 0.7372 - val_loss: 0.8465 - val_acc: 0.6627\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7299 - acc: 0.7412 - val_loss: 0.8744 - val_acc: 0.6988\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7267 - acc: 0.7332 - val_loss: 0.8557 - val_acc: 0.6867\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7260 - acc: 0.7318 - val_loss: 0.8667 - val_acc: 0.6506\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7259 - acc: 0.7480 - val_loss: 0.8550 - val_acc: 0.6747\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7165 - acc: 0.7561 - val_loss: 0.8591 - val_acc: 0.6627\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7166 - acc: 0.7412 - val_loss: 0.8514 - val_acc: 0.6747\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7243 - acc: 0.7345 - val_loss: 0.8644 - val_acc: 0.6747\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7166 - acc: 0.7318 - val_loss: 0.8890 - val_acc: 0.6386\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7092 - acc: 0.7453 - val_loss: 0.8522 - val_acc: 0.6506\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7044 - acc: 0.7439 - val_loss: 0.8587 - val_acc: 0.6867\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7035 - acc: 0.7439 - val_loss: 0.8388 - val_acc: 0.6627\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7022 - acc: 0.7534 - val_loss: 0.8601 - val_acc: 0.6627\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6973 - acc: 0.7480 - val_loss: 0.8539 - val_acc: 0.6627\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6983 - acc: 0.7385 - val_loss: 0.8457 - val_acc: 0.6386\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6936 - acc: 0.7426 - val_loss: 0.8626 - val_acc: 0.6506\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6906 - acc: 0.7493 - val_loss: 0.8384 - val_acc: 0.6627\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6893 - acc: 0.7466 - val_loss: 0.8378 - val_acc: 0.6747\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6841 - acc: 0.7520 - val_loss: 0.8484 - val_acc: 0.6627\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6865 - acc: 0.7507 - val_loss: 0.8476 - val_acc: 0.6506\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6848 - acc: 0.7507 - val_loss: 0.8456 - val_acc: 0.6867\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6821 - acc: 0.7426 - val_loss: 0.8504 - val_acc: 0.6867\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6798 - acc: 0.7412 - val_loss: 0.8554 - val_acc: 0.6867\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6759 - acc: 0.7642 - val_loss: 0.8463 - val_acc: 0.6506\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6872 - acc: 0.7426 - val_loss: 0.8498 - val_acc: 0.6747\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6820 - acc: 0.7224 - val_loss: 0.8449 - val_acc: 0.7108\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6756 - acc: 0.7466 - val_loss: 0.8434 - val_acc: 0.6867\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6696 - acc: 0.7466 - val_loss: 0.8517 - val_acc: 0.6747\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6656 - acc: 0.7507 - val_loss: 0.8310 - val_acc: 0.6867\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6689 - acc: 0.7520 - val_loss: 0.8278 - val_acc: 0.6867\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6591 - acc: 0.7574 - val_loss: 0.8319 - val_acc: 0.6988\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6611 - acc: 0.7507 - val_loss: 0.8376 - val_acc: 0.6867\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6594 - acc: 0.7520 - val_loss: 0.8309 - val_acc: 0.6506\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6538 - acc: 0.7588 - val_loss: 0.8233 - val_acc: 0.6627\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6529 - acc: 0.7480 - val_loss: 0.8267 - val_acc: 0.6988\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6533 - acc: 0.7520 - val_loss: 0.8312 - val_acc: 0.6867\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6529 - acc: 0.7588 - val_loss: 0.8313 - val_acc: 0.6627\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6548 - acc: 0.7628 - val_loss: 0.8229 - val_acc: 0.6867\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6583 - acc: 0.7466 - val_loss: 0.8093 - val_acc: 0.6747\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6524 - acc: 0.7547 - val_loss: 0.8557 - val_acc: 0.6506\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6456 - acc: 0.7547 - val_loss: 0.8182 - val_acc: 0.6627\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6430 - acc: 0.7561 - val_loss: 0.8219 - val_acc: 0.6988\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6388 - acc: 0.7588 - val_loss: 0.8338 - val_acc: 0.6627\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6421 - acc: 0.7601 - val_loss: 0.8151 - val_acc: 0.6747\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6454 - acc: 0.7547 - val_loss: 0.8319 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6401 - acc: 0.7561 - val_loss: 0.8216 - val_acc: 0.7108\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6369 - acc: 0.7547 - val_loss: 0.8334 - val_acc: 0.6747\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6329 - acc: 0.7615 - val_loss: 0.8196 - val_acc: 0.6988\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6307 - acc: 0.7588 - val_loss: 0.8233 - val_acc: 0.6747\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6282 - acc: 0.7588 - val_loss: 0.8190 - val_acc: 0.6988\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6305 - acc: 0.7588 - val_loss: 0.8135 - val_acc: 0.6506\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6353 - acc: 0.7763 - val_loss: 0.8201 - val_acc: 0.6386\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6469 - acc: 0.7453 - val_loss: 0.8446 - val_acc: 0.6627\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6273 - acc: 0.7709 - val_loss: 0.8083 - val_acc: 0.6867\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6315 - acc: 0.7601 - val_loss: 0.8337 - val_acc: 0.6386\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6283 - acc: 0.7655 - val_loss: 0.8290 - val_acc: 0.6867\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 32us/step - loss: 0.6254 - acc: 0.7668 - val_loss: 0.8215 - val_acc: 0.6867\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6203 - acc: 0.7668 - val_loss: 0.8097 - val_acc: 0.6506\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6248 - acc: 0.7695 - val_loss: 0.8623 - val_acc: 0.6145\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6252 - acc: 0.7561 - val_loss: 0.8248 - val_acc: 0.6627\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6147 - acc: 0.7776 - val_loss: 0.8146 - val_acc: 0.6627\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6152 - acc: 0.7601 - val_loss: 0.8203 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6164 - acc: 0.7628 - val_loss: 0.8239 - val_acc: 0.6747\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.6134 - acc: 0.7722 - val_loss: 0.8179 - val_acc: 0.6867\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 56us/step - loss: 0.6085 - acc: 0.7588 - val_loss: 0.8311 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6065 - acc: 0.7682 - val_loss: 0.8156 - val_acc: 0.6867\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.6163 - acc: 0.7574 - val_loss: 0.8285 - val_acc: 0.6867\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6072 - acc: 0.7642 - val_loss: 0.8274 - val_acc: 0.6867\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6106 - acc: 0.7601 - val_loss: 0.8390 - val_acc: 0.6386\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6126 - acc: 0.7642 - val_loss: 0.8204 - val_acc: 0.6747\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 33us/step - loss: 0.6032 - acc: 0.7628 - val_loss: 0.8292 - val_acc: 0.6867\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6070 - acc: 0.7803 - val_loss: 0.8041 - val_acc: 0.6627\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5994 - acc: 0.7642 - val_loss: 0.8305 - val_acc: 0.6867\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5998 - acc: 0.7749 - val_loss: 0.8131 - val_acc: 0.6747\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5982 - acc: 0.7763 - val_loss: 0.8239 - val_acc: 0.6867\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6029 - acc: 0.7615 - val_loss: 0.8253 - val_acc: 0.6747\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6000 - acc: 0.7668 - val_loss: 0.8273 - val_acc: 0.6627\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6016 - acc: 0.7695 - val_loss: 0.8205 - val_acc: 0.6747\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5963 - acc: 0.7682 - val_loss: 0.8399 - val_acc: 0.6627\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5892 - acc: 0.7709 - val_loss: 0.8138 - val_acc: 0.6627\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5957 - acc: 0.7722 - val_loss: 0.8130 - val_acc: 0.6627\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 0.5916 - acc: 0.7722 - val_loss: 0.8274 - val_acc: 0.6747\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5983 - acc: 0.7749 - val_loss: 0.8191 - val_acc: 0.6627\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5893 - acc: 0.7817 - val_loss: 0.8294 - val_acc: 0.6627\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5944 - acc: 0.7709 - val_loss: 0.8229 - val_acc: 0.6627\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5952 - acc: 0.7655 - val_loss: 0.8123 - val_acc: 0.6988\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5846 - acc: 0.7763 - val_loss: 0.8268 - val_acc: 0.6747\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5850 - acc: 0.7736 - val_loss: 0.8100 - val_acc: 0.6627\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5897 - acc: 0.7817 - val_loss: 0.8211 - val_acc: 0.6747\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5882 - acc: 0.7642 - val_loss: 0.8295 - val_acc: 0.6627\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5816 - acc: 0.7790 - val_loss: 0.8424 - val_acc: 0.6747\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5982 - acc: 0.7534 - val_loss: 0.8191 - val_acc: 0.6867\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5946 - acc: 0.7709 - val_loss: 0.8227 - val_acc: 0.6627\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6014 - acc: 0.7817 - val_loss: 0.8048 - val_acc: 0.6506\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5903 - acc: 0.7830 - val_loss: 0.8410 - val_acc: 0.6867\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5819 - acc: 0.7695 - val_loss: 0.8269 - val_acc: 0.6506\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5807 - acc: 0.7830 - val_loss: 0.8296 - val_acc: 0.6747\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5761 - acc: 0.7803 - val_loss: 0.8288 - val_acc: 0.6747\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5758 - acc: 0.7844 - val_loss: 0.8237 - val_acc: 0.6747\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5725 - acc: 0.7776 - val_loss: 0.8257 - val_acc: 0.6627\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5725 - acc: 0.7803 - val_loss: 0.8236 - val_acc: 0.6747\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5794 - acc: 0.7722 - val_loss: 0.8496 - val_acc: 0.6867\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5728 - acc: 0.7722 - val_loss: 0.8288 - val_acc: 0.6867\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5730 - acc: 0.7884 - val_loss: 0.8294 - val_acc: 0.6867\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5718 - acc: 0.7776 - val_loss: 0.8353 - val_acc: 0.6747\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5672 - acc: 0.7898 - val_loss: 0.8325 - val_acc: 0.6747\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5692 - acc: 0.7790 - val_loss: 0.8300 - val_acc: 0.6506\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5769 - acc: 0.7736 - val_loss: 0.8256 - val_acc: 0.6747\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 2s 2ms/step - loss: 2.3836 - acc: 0.1954 - val_loss: 2.3622 - val_acc: 0.1446\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 2.3374 - acc: 0.2305 - val_loss: 2.2959 - val_acc: 0.1205\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.2491 - acc: 0.2008 - val_loss: 2.1657 - val_acc: 0.1205\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 2.0913 - acc: 0.2305 - val_loss: 1.9639 - val_acc: 0.2289\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.9099 - acc: 0.2817 - val_loss: 1.7593 - val_acc: 0.2892\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.7777 - acc: 0.2992 - val_loss: 1.6115 - val_acc: 0.3494\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.6832 - acc: 0.3154 - val_loss: 1.5526 - val_acc: 0.3494\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.6162 - acc: 0.3275 - val_loss: 1.5087 - val_acc: 0.3494\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.5635 - acc: 0.3437 - val_loss: 1.4605 - val_acc: 0.3494\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 1.5202 - acc: 0.3625 - val_loss: 1.4150 - val_acc: 0.3373\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.4855 - acc: 0.3976 - val_loss: 1.3981 - val_acc: 0.3976\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 1.4572 - acc: 0.4003 - val_loss: 1.3491 - val_acc: 0.3855\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.4258 - acc: 0.4178 - val_loss: 1.3545 - val_acc: 0.3855\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.4020 - acc: 0.4542 - val_loss: 1.3159 - val_acc: 0.4217\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3792 - acc: 0.4501 - val_loss: 1.3149 - val_acc: 0.4096\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 1.3510 - acc: 0.4636 - val_loss: 1.2713 - val_acc: 0.4458\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.3320 - acc: 0.4663 - val_loss: 1.2731 - val_acc: 0.4337\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.3051 - acc: 0.4919 - val_loss: 1.2486 - val_acc: 0.4096\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2846 - acc: 0.4865 - val_loss: 1.2341 - val_acc: 0.4337\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2636 - acc: 0.4933 - val_loss: 1.2187 - val_acc: 0.4096\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2409 - acc: 0.5054 - val_loss: 1.2076 - val_acc: 0.4699\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 1.2214 - acc: 0.5121 - val_loss: 1.2019 - val_acc: 0.4578\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 1.2004 - acc: 0.5243 - val_loss: 1.1855 - val_acc: 0.4578\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.1829 - acc: 0.5337 - val_loss: 1.1645 - val_acc: 0.4819\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1642 - acc: 0.5391 - val_loss: 1.1643 - val_acc: 0.4699\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1463 - acc: 0.5445 - val_loss: 1.1402 - val_acc: 0.4578\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1292 - acc: 0.5526 - val_loss: 1.1289 - val_acc: 0.4819\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.1139 - acc: 0.5499 - val_loss: 1.1411 - val_acc: 0.4819\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.0988 - acc: 0.5566 - val_loss: 1.1055 - val_acc: 0.4819\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0841 - acc: 0.5741 - val_loss: 1.1073 - val_acc: 0.4940\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0722 - acc: 0.5687 - val_loss: 1.0722 - val_acc: 0.4940\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 64us/step - loss: 1.0526 - acc: 0.5970 - val_loss: 1.0966 - val_acc: 0.4940\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0370 - acc: 0.5916 - val_loss: 1.0570 - val_acc: 0.5422\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 1.0258 - acc: 0.6024 - val_loss: 1.0607 - val_acc: 0.5301\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0112 - acc: 0.5957 - val_loss: 1.0436 - val_acc: 0.5301\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.9991 - acc: 0.6051 - val_loss: 1.0368 - val_acc: 0.5904\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9868 - acc: 0.6388 - val_loss: 1.0420 - val_acc: 0.5181\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9758 - acc: 0.6186 - val_loss: 1.0153 - val_acc: 0.6265\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9620 - acc: 0.6429 - val_loss: 1.0134 - val_acc: 0.5904\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9523 - acc: 0.6429 - val_loss: 0.9966 - val_acc: 0.6145\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.9449 - acc: 0.6456 - val_loss: 0.9838 - val_acc: 0.6024\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.9323 - acc: 0.6617 - val_loss: 0.9841 - val_acc: 0.6145\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.9264 - acc: 0.6577 - val_loss: 0.9870 - val_acc: 0.6024\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.9156 - acc: 0.6563 - val_loss: 0.9638 - val_acc: 0.5422\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9069 - acc: 0.6792 - val_loss: 0.9723 - val_acc: 0.6145\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 57us/step - loss: 0.8942 - acc: 0.6765 - val_loss: 0.9513 - val_acc: 0.5783\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8914 - acc: 0.6631 - val_loss: 0.9605 - val_acc: 0.6024\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8798 - acc: 0.6739 - val_loss: 0.9524 - val_acc: 0.6265\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8806 - acc: 0.6860 - val_loss: 0.9265 - val_acc: 0.6506\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8642 - acc: 0.6927 - val_loss: 0.9488 - val_acc: 0.6265\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.8589 - acc: 0.6860 - val_loss: 0.9320 - val_acc: 0.6506\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8467 - acc: 0.6995 - val_loss: 0.9131 - val_acc: 0.6265\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.8534 - acc: 0.6873 - val_loss: 0.9138 - val_acc: 0.6386\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8390 - acc: 0.6846 - val_loss: 0.9302 - val_acc: 0.6265\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8343 - acc: 0.6860 - val_loss: 0.9028 - val_acc: 0.6506\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8250 - acc: 0.6995 - val_loss: 0.9052 - val_acc: 0.6265\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.8144 - acc: 0.6981 - val_loss: 0.9071 - val_acc: 0.6386\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8094 - acc: 0.7049 - val_loss: 0.9127 - val_acc: 0.6506\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8125 - acc: 0.6873 - val_loss: 0.8925 - val_acc: 0.6506\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8046 - acc: 0.7089 - val_loss: 0.8955 - val_acc: 0.6506\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7953 - acc: 0.7022 - val_loss: 0.8844 - val_acc: 0.6386\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7872 - acc: 0.7049 - val_loss: 0.8937 - val_acc: 0.6386\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.7836 - acc: 0.7116 - val_loss: 0.9037 - val_acc: 0.6506\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7782 - acc: 0.7264 - val_loss: 0.8652 - val_acc: 0.6867\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7734 - acc: 0.7251 - val_loss: 0.8735 - val_acc: 0.6386\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7690 - acc: 0.7210 - val_loss: 0.8731 - val_acc: 0.6386\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7620 - acc: 0.7224 - val_loss: 0.8955 - val_acc: 0.6506\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7592 - acc: 0.7210 - val_loss: 0.8691 - val_acc: 0.6627\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7587 - acc: 0.7156 - val_loss: 0.8540 - val_acc: 0.6386\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7565 - acc: 0.7170 - val_loss: 0.8946 - val_acc: 0.6386\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7513 - acc: 0.7237 - val_loss: 0.8671 - val_acc: 0.6747\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7392 - acc: 0.7264 - val_loss: 0.8639 - val_acc: 0.6506\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7405 - acc: 0.7251 - val_loss: 0.8513 - val_acc: 0.6627\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7337 - acc: 0.7278 - val_loss: 0.8475 - val_acc: 0.6627\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.7310 - acc: 0.7237 - val_loss: 0.9010 - val_acc: 0.6867\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7383 - acc: 0.7291 - val_loss: 0.8466 - val_acc: 0.6386\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7335 - acc: 0.7251 - val_loss: 0.8757 - val_acc: 0.6747\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7194 - acc: 0.7291 - val_loss: 0.8602 - val_acc: 0.6386\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7178 - acc: 0.7372 - val_loss: 0.8464 - val_acc: 0.6627\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.7199 - acc: 0.7318 - val_loss: 0.8583 - val_acc: 0.6747\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7086 - acc: 0.7439 - val_loss: 0.8353 - val_acc: 0.6627\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.7093 - acc: 0.7439 - val_loss: 0.8526 - val_acc: 0.6506\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7015 - acc: 0.7439 - val_loss: 0.8476 - val_acc: 0.6506\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7032 - acc: 0.7318 - val_loss: 0.8368 - val_acc: 0.6506\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7040 - acc: 0.7426 - val_loss: 0.8392 - val_acc: 0.6627\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.7031 - acc: 0.7278 - val_loss: 0.8390 - val_acc: 0.6867\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6976 - acc: 0.7534 - val_loss: 0.8209 - val_acc: 0.6506\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6934 - acc: 0.7318 - val_loss: 0.8585 - val_acc: 0.6747\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6860 - acc: 0.7453 - val_loss: 0.8366 - val_acc: 0.6747\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6822 - acc: 0.7453 - val_loss: 0.8281 - val_acc: 0.6867\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6773 - acc: 0.7493 - val_loss: 0.8334 - val_acc: 0.6506\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6742 - acc: 0.7520 - val_loss: 0.8500 - val_acc: 0.6627\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6762 - acc: 0.7426 - val_loss: 0.8236 - val_acc: 0.6747\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6728 - acc: 0.7466 - val_loss: 0.8193 - val_acc: 0.6867\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6634 - acc: 0.7520 - val_loss: 0.8529 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6759 - acc: 0.7426 - val_loss: 0.8177 - val_acc: 0.6747\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6658 - acc: 0.7520 - val_loss: 0.8229 - val_acc: 0.6747\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6633 - acc: 0.7628 - val_loss: 0.8212 - val_acc: 0.6747\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6576 - acc: 0.7507 - val_loss: 0.8342 - val_acc: 0.6867\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6579 - acc: 0.7534 - val_loss: 0.8174 - val_acc: 0.6867\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6532 - acc: 0.7507 - val_loss: 0.8257 - val_acc: 0.6988\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.6506 - acc: 0.7480 - val_loss: 0.8300 - val_acc: 0.6747\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6474 - acc: 0.7547 - val_loss: 0.8381 - val_acc: 0.6506\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6499 - acc: 0.7547 - val_loss: 0.8142 - val_acc: 0.6988\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6468 - acc: 0.7466 - val_loss: 0.8286 - val_acc: 0.6747\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6529 - acc: 0.7534 - val_loss: 0.8541 - val_acc: 0.6506\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6459 - acc: 0.7453 - val_loss: 0.8291 - val_acc: 0.6867\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6433 - acc: 0.7534 - val_loss: 0.8277 - val_acc: 0.6988\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.6399 - acc: 0.7493 - val_loss: 0.8128 - val_acc: 0.6627\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6383 - acc: 0.7507 - val_loss: 0.8450 - val_acc: 0.6627\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6373 - acc: 0.7588 - val_loss: 0.8222 - val_acc: 0.6627\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6311 - acc: 0.7615 - val_loss: 0.8445 - val_acc: 0.6627\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6318 - acc: 0.7547 - val_loss: 0.8255 - val_acc: 0.6867\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.6288 - acc: 0.7588 - val_loss: 0.8249 - val_acc: 0.6988\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6337 - acc: 0.7574 - val_loss: 0.8030 - val_acc: 0.6747\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6261 - acc: 0.7709 - val_loss: 0.8436 - val_acc: 0.6627\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6211 - acc: 0.7668 - val_loss: 0.8273 - val_acc: 0.6867\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6170 - acc: 0.7601 - val_loss: 0.8203 - val_acc: 0.6988\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6163 - acc: 0.7615 - val_loss: 0.8246 - val_acc: 0.6867\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6140 - acc: 0.7709 - val_loss: 0.8119 - val_acc: 0.6627\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6174 - acc: 0.7615 - val_loss: 0.8434 - val_acc: 0.6747\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6198 - acc: 0.7763 - val_loss: 0.8372 - val_acc: 0.6867\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6177 - acc: 0.7668 - val_loss: 0.8092 - val_acc: 0.6867\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6187 - acc: 0.7682 - val_loss: 0.8223 - val_acc: 0.6867\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6072 - acc: 0.7749 - val_loss: 0.8445 - val_acc: 0.6988\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 0.6079 - acc: 0.7655 - val_loss: 0.8070 - val_acc: 0.6867\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6115 - acc: 0.7453 - val_loss: 0.8362 - val_acc: 0.6867\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6053 - acc: 0.7817 - val_loss: 0.8267 - val_acc: 0.7108\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6038 - acc: 0.7722 - val_loss: 0.8065 - val_acc: 0.6867\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6018 - acc: 0.7803 - val_loss: 0.8225 - val_acc: 0.7229\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5970 - acc: 0.7763 - val_loss: 0.8154 - val_acc: 0.6988\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6039 - acc: 0.7668 - val_loss: 0.8243 - val_acc: 0.7108\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5996 - acc: 0.7763 - val_loss: 0.8481 - val_acc: 0.7108\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5937 - acc: 0.7749 - val_loss: 0.8014 - val_acc: 0.7108\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5959 - acc: 0.7736 - val_loss: 0.8270 - val_acc: 0.6988\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6004 - acc: 0.7722 - val_loss: 0.8565 - val_acc: 0.6867\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5978 - acc: 0.7615 - val_loss: 0.8426 - val_acc: 0.6747\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5938 - acc: 0.7736 - val_loss: 0.8221 - val_acc: 0.7108\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5863 - acc: 0.7803 - val_loss: 0.8118 - val_acc: 0.6988\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5918 - acc: 0.7790 - val_loss: 0.8228 - val_acc: 0.6867\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5894 - acc: 0.7817 - val_loss: 0.8407 - val_acc: 0.6988\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5833 - acc: 0.7776 - val_loss: 0.8641 - val_acc: 0.6747\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7722 - val_loss: 0.8256 - val_acc: 0.6988\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.5814 - acc: 0.7736 - val_loss: 0.8230 - val_acc: 0.6988\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5902 - acc: 0.7642 - val_loss: 0.8469 - val_acc: 0.6747\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5906 - acc: 0.7817 - val_loss: 0.8273 - val_acc: 0.6747\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5809 - acc: 0.7749 - val_loss: 0.8462 - val_acc: 0.7108\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5797 - acc: 0.7817 - val_loss: 0.8336 - val_acc: 0.6627\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5867 - acc: 0.7763 - val_loss: 0.8529 - val_acc: 0.6988\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5791 - acc: 0.7776 - val_loss: 0.8212 - val_acc: 0.6747\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5843 - acc: 0.7776 - val_loss: 0.8293 - val_acc: 0.6988\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5764 - acc: 0.7817 - val_loss: 0.8357 - val_acc: 0.7108\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5687 - acc: 0.7871 - val_loss: 0.8182 - val_acc: 0.7108\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5702 - acc: 0.7668 - val_loss: 0.8431 - val_acc: 0.6988\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5669 - acc: 0.7844 - val_loss: 0.8496 - val_acc: 0.6988\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5817 - acc: 0.7601 - val_loss: 0.8276 - val_acc: 0.6867\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5721 - acc: 0.7844 - val_loss: 0.8279 - val_acc: 0.7108\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5848 - acc: 0.7695 - val_loss: 0.8507 - val_acc: 0.6747\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5765 - acc: 0.7561 - val_loss: 0.8546 - val_acc: 0.6747\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5825 - acc: 0.7722 - val_loss: 0.8433 - val_acc: 0.7108\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5713 - acc: 0.7817 - val_loss: 0.8183 - val_acc: 0.6867\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.5634 - acc: 0.7763 - val_loss: 0.8265 - val_acc: 0.7108\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5626 - acc: 0.7992 - val_loss: 0.8536 - val_acc: 0.6867\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5573 - acc: 0.7871 - val_loss: 0.8370 - val_acc: 0.7108\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5610 - acc: 0.7871 - val_loss: 0.8331 - val_acc: 0.6988\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5530 - acc: 0.7898 - val_loss: 0.8475 - val_acc: 0.7108\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5526 - acc: 0.7871 - val_loss: 0.8310 - val_acc: 0.6867\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5550 - acc: 0.7844 - val_loss: 0.8315 - val_acc: 0.6867\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5548 - acc: 0.7749 - val_loss: 0.8346 - val_acc: 0.7229\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5527 - acc: 0.7844 - val_loss: 0.8300 - val_acc: 0.6988\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.5521 - acc: 0.7857 - val_loss: 0.8510 - val_acc: 0.7108\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5546 - acc: 0.7925 - val_loss: 0.8374 - val_acc: 0.6988\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5502 - acc: 0.7817 - val_loss: 0.8373 - val_acc: 0.7108\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5481 - acc: 0.7925 - val_loss: 0.8652 - val_acc: 0.6747\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5516 - acc: 0.7898 - val_loss: 0.8245 - val_acc: 0.6867\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5481 - acc: 0.7776 - val_loss: 0.8528 - val_acc: 0.7108\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5535 - acc: 0.7898 - val_loss: 0.8408 - val_acc: 0.6988\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5458 - acc: 0.7830 - val_loss: 0.8290 - val_acc: 0.6988\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5437 - acc: 0.7871 - val_loss: 0.8512 - val_acc: 0.7108\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5568 - acc: 0.7736 - val_loss: 0.8420 - val_acc: 0.6988\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5488 - acc: 0.7776 - val_loss: 0.8416 - val_acc: 0.6988\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5492 - acc: 0.7925 - val_loss: 0.8548 - val_acc: 0.6988\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5472 - acc: 0.7965 - val_loss: 0.8341 - val_acc: 0.6988\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5434 - acc: 0.7749 - val_loss: 0.8434 - val_acc: 0.6988\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5406 - acc: 0.7830 - val_loss: 0.8559 - val_acc: 0.6988\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5625 - acc: 0.7682 - val_loss: 0.8404 - val_acc: 0.6988\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5512 - acc: 0.7898 - val_loss: 0.8668 - val_acc: 0.6747\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5425 - acc: 0.7776 - val_loss: 0.8404 - val_acc: 0.7349\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5362 - acc: 0.7871 - val_loss: 0.8392 - val_acc: 0.6867\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5303 - acc: 0.7992 - val_loss: 0.8387 - val_acc: 0.6867\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5334 - acc: 0.8005 - val_loss: 0.8499 - val_acc: 0.6988\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5299 - acc: 0.7978 - val_loss: 0.8464 - val_acc: 0.6747\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5352 - acc: 0.7884 - val_loss: 0.8485 - val_acc: 0.7229\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5278 - acc: 0.8005 - val_loss: 0.8393 - val_acc: 0.6988\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5320 - acc: 0.7978 - val_loss: 0.8427 - val_acc: 0.6867\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5373 - acc: 0.7871 - val_loss: 0.8690 - val_acc: 0.6747\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5383 - acc: 0.7951 - val_loss: 0.8408 - val_acc: 0.6988\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 0.5290 - acc: 0.8032 - val_loss: 0.8478 - val_acc: 0.6867\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 34us/step - loss: 0.5254 - acc: 0.7817 - val_loss: 0.8433 - val_acc: 0.6867\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5240 - acc: 0.7871 - val_loss: 0.8854 - val_acc: 0.6988\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 2s 2ms/step - loss: 2.3711 - acc: 0.2466 - val_loss: 2.3282 - val_acc: 0.3494\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 2.2842 - acc: 0.2668 - val_loss: 2.1901 - val_acc: 0.3494\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 2.1114 - acc: 0.2615 - val_loss: 1.9445 - val_acc: 0.3373\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.8960 - acc: 0.2722 - val_loss: 1.7284 - val_acc: 0.3614\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.7538 - acc: 0.3032 - val_loss: 1.6219 - val_acc: 0.3494\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.6584 - acc: 0.3208 - val_loss: 1.5505 - val_acc: 0.3976\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.5819 - acc: 0.3760 - val_loss: 1.4824 - val_acc: 0.4096\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.5299 - acc: 0.3854 - val_loss: 1.4530 - val_acc: 0.3855\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.4831 - acc: 0.4178 - val_loss: 1.3987 - val_acc: 0.4337\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.4478 - acc: 0.4272 - val_loss: 1.3763 - val_acc: 0.3855\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.4185 - acc: 0.4353 - val_loss: 1.3482 - val_acc: 0.4217\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 1.3838 - acc: 0.4623 - val_loss: 1.3326 - val_acc: 0.3976\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.3592 - acc: 0.4730 - val_loss: 1.3074 - val_acc: 0.4458\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.3366 - acc: 0.4609 - val_loss: 1.2771 - val_acc: 0.4458\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.3060 - acc: 0.4919 - val_loss: 1.2763 - val_acc: 0.4096\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2811 - acc: 0.4946 - val_loss: 1.2516 - val_acc: 0.4337\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 1.2579 - acc: 0.4987 - val_loss: 1.2373 - val_acc: 0.4217\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2344 - acc: 0.5148 - val_loss: 1.2202 - val_acc: 0.4458\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.2113 - acc: 0.5148 - val_loss: 1.2139 - val_acc: 0.4578\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1859 - acc: 0.5256 - val_loss: 1.2041 - val_acc: 0.4217\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.1649 - acc: 0.5337 - val_loss: 1.1788 - val_acc: 0.4699\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.1432 - acc: 0.5256 - val_loss: 1.1730 - val_acc: 0.4578\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.1249 - acc: 0.5512 - val_loss: 1.1553 - val_acc: 0.4578\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.1077 - acc: 0.5633 - val_loss: 1.1257 - val_acc: 0.4819\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0847 - acc: 0.5687 - val_loss: 1.1445 - val_acc: 0.4578\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.0674 - acc: 0.5620 - val_loss: 1.1245 - val_acc: 0.4699\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0564 - acc: 0.5755 - val_loss: 1.1033 - val_acc: 0.5301\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0389 - acc: 0.5916 - val_loss: 1.1092 - val_acc: 0.4940\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0239 - acc: 0.5984 - val_loss: 1.0795 - val_acc: 0.5060\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.0080 - acc: 0.6119 - val_loss: 1.0690 - val_acc: 0.5060\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.9931 - acc: 0.6051 - val_loss: 1.0772 - val_acc: 0.4699\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9775 - acc: 0.6226 - val_loss: 1.0506 - val_acc: 0.5060\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.9693 - acc: 0.6334 - val_loss: 1.0532 - val_acc: 0.5542\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9572 - acc: 0.6280 - val_loss: 1.0459 - val_acc: 0.5301\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9467 - acc: 0.6442 - val_loss: 1.0260 - val_acc: 0.5422\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9356 - acc: 0.6429 - val_loss: 1.0129 - val_acc: 0.5904\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9220 - acc: 0.6388 - val_loss: 1.0492 - val_acc: 0.5301\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.9185 - acc: 0.6388 - val_loss: 1.0004 - val_acc: 0.5663\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.9129 - acc: 0.6482 - val_loss: 1.0075 - val_acc: 0.5542\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8955 - acc: 0.6536 - val_loss: 1.0049 - val_acc: 0.5663\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8922 - acc: 0.6577 - val_loss: 0.9896 - val_acc: 0.5663\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8845 - acc: 0.6536 - val_loss: 0.9756 - val_acc: 0.5904\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8685 - acc: 0.6833 - val_loss: 1.0168 - val_acc: 0.6145\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8718 - acc: 0.6617 - val_loss: 0.9754 - val_acc: 0.5783\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8554 - acc: 0.6833 - val_loss: 0.9549 - val_acc: 0.5904\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.8494 - acc: 0.6806 - val_loss: 0.9581 - val_acc: 0.6145\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.8394 - acc: 0.6779 - val_loss: 0.9706 - val_acc: 0.6506\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8423 - acc: 0.6752 - val_loss: 0.9453 - val_acc: 0.6265\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.6779 - val_loss: 0.9320 - val_acc: 0.6024\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.8181 - acc: 0.6995 - val_loss: 0.9589 - val_acc: 0.6145\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8128 - acc: 0.6927 - val_loss: 0.9352 - val_acc: 0.5904\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.8027 - acc: 0.6981 - val_loss: 0.9321 - val_acc: 0.6024\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.8008 - acc: 0.6968 - val_loss: 0.9224 - val_acc: 0.6265\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7904 - acc: 0.6981 - val_loss: 0.9180 - val_acc: 0.6506\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7861 - acc: 0.6981 - val_loss: 0.9127 - val_acc: 0.6145\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7766 - acc: 0.7049 - val_loss: 0.9418 - val_acc: 0.6386\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7840 - acc: 0.6873 - val_loss: 0.9159 - val_acc: 0.6265\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7711 - acc: 0.7143 - val_loss: 0.8975 - val_acc: 0.6265\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7635 - acc: 0.7129 - val_loss: 0.8974 - val_acc: 0.6265\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7589 - acc: 0.7075 - val_loss: 0.9167 - val_acc: 0.6747\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7540 - acc: 0.7183 - val_loss: 0.8867 - val_acc: 0.6265\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7530 - acc: 0.7170 - val_loss: 0.9032 - val_acc: 0.6145\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.7479 - acc: 0.7143 - val_loss: 0.8991 - val_acc: 0.6627\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7506 - acc: 0.7129 - val_loss: 0.9174 - val_acc: 0.6506\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7445 - acc: 0.7049 - val_loss: 0.8783 - val_acc: 0.6265\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7370 - acc: 0.7035 - val_loss: 0.8710 - val_acc: 0.6506\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7268 - acc: 0.7358 - val_loss: 0.8936 - val_acc: 0.6386\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7232 - acc: 0.7197 - val_loss: 0.8752 - val_acc: 0.6265\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7209 - acc: 0.7291 - val_loss: 0.8886 - val_acc: 0.6627\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7221 - acc: 0.7129 - val_loss: 0.9022 - val_acc: 0.6386\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7129 - acc: 0.7372 - val_loss: 0.8372 - val_acc: 0.6747\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7117 - acc: 0.7264 - val_loss: 0.8988 - val_acc: 0.6386\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.7053 - acc: 0.7345 - val_loss: 0.8544 - val_acc: 0.6988\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.7033 - acc: 0.7237 - val_loss: 0.8732 - val_acc: 0.6265\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6937 - acc: 0.7345 - val_loss: 0.8656 - val_acc: 0.6627\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6948 - acc: 0.7237 - val_loss: 0.9184 - val_acc: 0.6747\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.7008 - acc: 0.7116 - val_loss: 0.8853 - val_acc: 0.6867\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6981 - acc: 0.7358 - val_loss: 0.8559 - val_acc: 0.6145\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6846 - acc: 0.7358 - val_loss: 0.8617 - val_acc: 0.6867\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6852 - acc: 0.7237 - val_loss: 0.8776 - val_acc: 0.6747\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6838 - acc: 0.7305 - val_loss: 0.8670 - val_acc: 0.6867\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6728 - acc: 0.7372 - val_loss: 0.8577 - val_acc: 0.6265\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6759 - acc: 0.7520 - val_loss: 0.8570 - val_acc: 0.6867\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6727 - acc: 0.7372 - val_loss: 0.8656 - val_acc: 0.6506\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6636 - acc: 0.7493 - val_loss: 0.8699 - val_acc: 0.6627\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6640 - acc: 0.7358 - val_loss: 0.8938 - val_acc: 0.6747\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6702 - acc: 0.7385 - val_loss: 0.8582 - val_acc: 0.6506\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.6568 - acc: 0.7399 - val_loss: 0.8506 - val_acc: 0.6627\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6550 - acc: 0.7439 - val_loss: 0.8435 - val_acc: 0.6506\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 56us/step - loss: 0.6504 - acc: 0.7507 - val_loss: 0.8549 - val_acc: 0.6627\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6591 - acc: 0.7251 - val_loss: 0.8794 - val_acc: 0.6506\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6615 - acc: 0.7547 - val_loss: 0.8840 - val_acc: 0.6747\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6537 - acc: 0.7412 - val_loss: 0.8424 - val_acc: 0.6506\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6452 - acc: 0.7453 - val_loss: 0.8643 - val_acc: 0.6627\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6331 - acc: 0.7507 - val_loss: 0.8481 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.6411 - acc: 0.7493 - val_loss: 0.8319 - val_acc: 0.6867\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6430 - acc: 0.7305 - val_loss: 0.8484 - val_acc: 0.6506\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6367 - acc: 0.7372 - val_loss: 0.8463 - val_acc: 0.6747\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6354 - acc: 0.7534 - val_loss: 0.8435 - val_acc: 0.6386\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6387 - acc: 0.7615 - val_loss: 0.8444 - val_acc: 0.6867\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6452 - acc: 0.7439 - val_loss: 0.8913 - val_acc: 0.6506\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6387 - acc: 0.7601 - val_loss: 0.8473 - val_acc: 0.6506\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6260 - acc: 0.7520 - val_loss: 0.8495 - val_acc: 0.6506\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6232 - acc: 0.7628 - val_loss: 0.8360 - val_acc: 0.6747\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6150 - acc: 0.7655 - val_loss: 0.8598 - val_acc: 0.6627\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.6208 - acc: 0.7615 - val_loss: 0.8380 - val_acc: 0.6747\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6197 - acc: 0.7547 - val_loss: 0.8315 - val_acc: 0.6627\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6129 - acc: 0.7682 - val_loss: 0.8354 - val_acc: 0.6506\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6172 - acc: 0.7588 - val_loss: 0.8500 - val_acc: 0.6747\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6210 - acc: 0.7493 - val_loss: 0.8529 - val_acc: 0.6747\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6193 - acc: 0.7534 - val_loss: 0.8145 - val_acc: 0.6747\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6261 - acc: 0.7574 - val_loss: 0.8547 - val_acc: 0.6506\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6177 - acc: 0.7574 - val_loss: 0.8411 - val_acc: 0.6627\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6072 - acc: 0.7642 - val_loss: 0.8524 - val_acc: 0.6747\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 0.6010 - acc: 0.7655 - val_loss: 0.8492 - val_acc: 0.6506\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.6098 - acc: 0.7466 - val_loss: 0.8384 - val_acc: 0.6867\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6046 - acc: 0.7574 - val_loss: 0.8522 - val_acc: 0.6988\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6063 - acc: 0.7547 - val_loss: 0.8337 - val_acc: 0.6867\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5982 - acc: 0.7709 - val_loss: 0.8531 - val_acc: 0.6386\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5944 - acc: 0.7722 - val_loss: 0.8779 - val_acc: 0.6867\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5940 - acc: 0.7682 - val_loss: 0.8415 - val_acc: 0.6747\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5980 - acc: 0.7574 - val_loss: 0.8500 - val_acc: 0.6747\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5942 - acc: 0.7574 - val_loss: 0.8374 - val_acc: 0.6506\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5900 - acc: 0.7628 - val_loss: 0.8431 - val_acc: 0.6386\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6025 - acc: 0.7574 - val_loss: 0.8556 - val_acc: 0.6506\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5991 - acc: 0.7561 - val_loss: 0.8412 - val_acc: 0.6627\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5971 - acc: 0.7480 - val_loss: 0.8361 - val_acc: 0.6627\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5832 - acc: 0.7655 - val_loss: 0.8509 - val_acc: 0.6867\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5863 - acc: 0.7534 - val_loss: 0.8577 - val_acc: 0.6627\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5861 - acc: 0.7722 - val_loss: 0.8557 - val_acc: 0.6747\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5858 - acc: 0.7722 - val_loss: 0.8389 - val_acc: 0.6506\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5773 - acc: 0.7749 - val_loss: 0.8400 - val_acc: 0.6627\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5902 - acc: 0.7628 - val_loss: 0.8385 - val_acc: 0.6627\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5815 - acc: 0.7722 - val_loss: 0.8466 - val_acc: 0.6627\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5768 - acc: 0.7776 - val_loss: 0.8404 - val_acc: 0.6747\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5812 - acc: 0.7736 - val_loss: 0.8512 - val_acc: 0.6867\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5868 - acc: 0.7655 - val_loss: 0.8498 - val_acc: 0.6627\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5788 - acc: 0.7682 - val_loss: 0.8470 - val_acc: 0.6627\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5826 - acc: 0.7776 - val_loss: 0.8584 - val_acc: 0.6506\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5675 - acc: 0.7749 - val_loss: 0.8226 - val_acc: 0.6988\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5663 - acc: 0.7668 - val_loss: 0.8464 - val_acc: 0.6506\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5687 - acc: 0.7830 - val_loss: 0.8652 - val_acc: 0.6627\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5671 - acc: 0.7830 - val_loss: 0.8288 - val_acc: 0.6506\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5632 - acc: 0.7695 - val_loss: 0.8337 - val_acc: 0.6988\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5606 - acc: 0.7830 - val_loss: 0.8464 - val_acc: 0.6627\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5657 - acc: 0.7776 - val_loss: 0.8497 - val_acc: 0.6867\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5642 - acc: 0.7817 - val_loss: 0.8425 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5615 - acc: 0.7776 - val_loss: 0.8419 - val_acc: 0.6747\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5641 - acc: 0.7642 - val_loss: 0.8583 - val_acc: 0.6867\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5707 - acc: 0.7642 - val_loss: 0.8296 - val_acc: 0.6627\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5649 - acc: 0.7844 - val_loss: 0.8407 - val_acc: 0.6747\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5634 - acc: 0.7790 - val_loss: 0.8461 - val_acc: 0.6988\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5594 - acc: 0.7830 - val_loss: 0.8983 - val_acc: 0.6627\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.5785 - acc: 0.7709 - val_loss: 0.8282 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5635 - acc: 0.7749 - val_loss: 0.8532 - val_acc: 0.6627\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5564 - acc: 0.7763 - val_loss: 0.8226 - val_acc: 0.6627\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5467 - acc: 0.7871 - val_loss: 0.8733 - val_acc: 0.6747\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5653 - acc: 0.7803 - val_loss: 0.8508 - val_acc: 0.7108\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5624 - acc: 0.7642 - val_loss: 0.8660 - val_acc: 0.6627\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5498 - acc: 0.7695 - val_loss: 0.8344 - val_acc: 0.6747\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 36us/step - loss: 0.5606 - acc: 0.7695 - val_loss: 0.8566 - val_acc: 0.6627\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5423 - acc: 0.7925 - val_loss: 0.8402 - val_acc: 0.6506\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5424 - acc: 0.7776 - val_loss: 0.8911 - val_acc: 0.6506\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5498 - acc: 0.7844 - val_loss: 0.8678 - val_acc: 0.6627\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5518 - acc: 0.7749 - val_loss: 0.8293 - val_acc: 0.6627\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5414 - acc: 0.7925 - val_loss: 0.8711 - val_acc: 0.6627\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5401 - acc: 0.7749 - val_loss: 0.8411 - val_acc: 0.6867\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5423 - acc: 0.7951 - val_loss: 0.8410 - val_acc: 0.6627\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5385 - acc: 0.7830 - val_loss: 0.8344 - val_acc: 0.6867\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5337 - acc: 0.7844 - val_loss: 0.8457 - val_acc: 0.6627\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5279 - acc: 0.7844 - val_loss: 0.8679 - val_acc: 0.6867\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5308 - acc: 0.7884 - val_loss: 0.8317 - val_acc: 0.6627\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5386 - acc: 0.7803 - val_loss: 0.8407 - val_acc: 0.6988\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5356 - acc: 0.7884 - val_loss: 0.8587 - val_acc: 0.6506\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5323 - acc: 0.7898 - val_loss: 0.8469 - val_acc: 0.6867\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5227 - acc: 0.7844 - val_loss: 0.8571 - val_acc: 0.6747\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5240 - acc: 0.7938 - val_loss: 0.8631 - val_acc: 0.6747\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5286 - acc: 0.7898 - val_loss: 0.8703 - val_acc: 0.6506\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5349 - acc: 0.7911 - val_loss: 0.8348 - val_acc: 0.6747\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5392 - acc: 0.8005 - val_loss: 0.8475 - val_acc: 0.6747\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5310 - acc: 0.7965 - val_loss: 0.8712 - val_acc: 0.6627\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5292 - acc: 0.7938 - val_loss: 0.8615 - val_acc: 0.6988\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5243 - acc: 0.7925 - val_loss: 0.8730 - val_acc: 0.6747\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5187 - acc: 0.7898 - val_loss: 0.8348 - val_acc: 0.6627\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.5243 - acc: 0.7951 - val_loss: 0.8702 - val_acc: 0.6506\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5238 - acc: 0.7925 - val_loss: 0.8609 - val_acc: 0.6747\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.5194 - acc: 0.7871 - val_loss: 0.8633 - val_acc: 0.6747\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.5087 - acc: 0.7992 - val_loss: 0.8536 - val_acc: 0.6747\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 37us/step - loss: 0.5095 - acc: 0.8032 - val_loss: 0.8469 - val_acc: 0.6627\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5226 - acc: 0.7871 - val_loss: 0.8639 - val_acc: 0.6506\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5270 - acc: 0.7951 - val_loss: 0.8431 - val_acc: 0.6627\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5203 - acc: 0.7978 - val_loss: 0.8604 - val_acc: 0.6747\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5096 - acc: 0.8005 - val_loss: 0.8525 - val_acc: 0.6506\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5242 - acc: 0.7844 - val_loss: 0.8464 - val_acc: 0.6747\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5386 - acc: 0.7736 - val_loss: 0.8360 - val_acc: 0.6627\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5367 - acc: 0.7884 - val_loss: 0.8697 - val_acc: 0.6867\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5074 - acc: 0.8032 - val_loss: 0.8578 - val_acc: 0.6627\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5051 - acc: 0.8019 - val_loss: 0.8341 - val_acc: 0.6627\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.4996 - acc: 0.8100 - val_loss: 0.8451 - val_acc: 0.6867\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5043 - acc: 0.8019 - val_loss: 0.8534 - val_acc: 0.6867\n",
            "Train on 742 samples, validate on 83 samples\n",
            "Epoch 1/200\n",
            "742/742 [==============================] - 2s 2ms/step - loss: 2.3591 - acc: 0.2237 - val_loss: 2.2956 - val_acc: 0.2892\n",
            "Epoch 2/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 2.2181 - acc: 0.2763 - val_loss: 2.0787 - val_acc: 0.2892\n",
            "Epoch 3/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.9823 - acc: 0.2790 - val_loss: 1.8081 - val_acc: 0.3133\n",
            "Epoch 4/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.8051 - acc: 0.2925 - val_loss: 1.6651 - val_acc: 0.3494\n",
            "Epoch 5/200\n",
            "742/742 [==============================] - 0s 35us/step - loss: 1.6954 - acc: 0.3221 - val_loss: 1.5810 - val_acc: 0.3614\n",
            "Epoch 6/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 1.6094 - acc: 0.3693 - val_loss: 1.5306 - val_acc: 0.3253\n",
            "Epoch 7/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.5401 - acc: 0.3881 - val_loss: 1.4500 - val_acc: 0.4096\n",
            "Epoch 8/200\n",
            "742/742 [==============================] - 0s 61us/step - loss: 1.4927 - acc: 0.3976 - val_loss: 1.4012 - val_acc: 0.4096\n",
            "Epoch 9/200\n",
            "742/742 [==============================] - 0s 55us/step - loss: 1.4473 - acc: 0.4272 - val_loss: 1.3676 - val_acc: 0.3976\n",
            "Epoch 10/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 1.4102 - acc: 0.4515 - val_loss: 1.3478 - val_acc: 0.4096\n",
            "Epoch 11/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.3804 - acc: 0.4528 - val_loss: 1.3137 - val_acc: 0.4217\n",
            "Epoch 12/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 1.3459 - acc: 0.4677 - val_loss: 1.2971 - val_acc: 0.4217\n",
            "Epoch 13/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.3202 - acc: 0.4730 - val_loss: 1.2764 - val_acc: 0.4578\n",
            "Epoch 14/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.2900 - acc: 0.4852 - val_loss: 1.2541 - val_acc: 0.4217\n",
            "Epoch 15/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.2592 - acc: 0.5013 - val_loss: 1.2447 - val_acc: 0.4337\n",
            "Epoch 16/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 1.2341 - acc: 0.4973 - val_loss: 1.2189 - val_acc: 0.4337\n",
            "Epoch 17/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.2055 - acc: 0.5054 - val_loss: 1.2108 - val_acc: 0.3976\n",
            "Epoch 18/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 1.1816 - acc: 0.5175 - val_loss: 1.1978 - val_acc: 0.4699\n",
            "Epoch 19/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 1.1580 - acc: 0.5229 - val_loss: 1.1910 - val_acc: 0.4699\n",
            "Epoch 20/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.1373 - acc: 0.5310 - val_loss: 1.1482 - val_acc: 0.4699\n",
            "Epoch 21/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.1096 - acc: 0.5687 - val_loss: 1.1568 - val_acc: 0.4699\n",
            "Epoch 22/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 1.0862 - acc: 0.5620 - val_loss: 1.1367 - val_acc: 0.4819\n",
            "Epoch 23/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 1.0678 - acc: 0.5687 - val_loss: 1.1279 - val_acc: 0.4337\n",
            "Epoch 24/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 1.0518 - acc: 0.5863 - val_loss: 1.1205 - val_acc: 0.4819\n",
            "Epoch 25/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0384 - acc: 0.5687 - val_loss: 1.0973 - val_acc: 0.5301\n",
            "Epoch 26/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 1.0140 - acc: 0.6065 - val_loss: 1.0897 - val_acc: 0.5060\n",
            "Epoch 27/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 1.0041 - acc: 0.5943 - val_loss: 1.0783 - val_acc: 0.5422\n",
            "Epoch 28/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.9829 - acc: 0.6119 - val_loss: 1.0969 - val_acc: 0.5422\n",
            "Epoch 29/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.9754 - acc: 0.6105 - val_loss: 1.0491 - val_acc: 0.5422\n",
            "Epoch 30/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.9538 - acc: 0.6415 - val_loss: 1.0559 - val_acc: 0.5422\n",
            "Epoch 31/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.9453 - acc: 0.6334 - val_loss: 1.0354 - val_acc: 0.5663\n",
            "Epoch 32/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9298 - acc: 0.6415 - val_loss: 1.0416 - val_acc: 0.5783\n",
            "Epoch 33/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.9247 - acc: 0.6307 - val_loss: 0.9959 - val_acc: 0.5904\n",
            "Epoch 34/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.9148 - acc: 0.6456 - val_loss: 1.0508 - val_acc: 0.5542\n",
            "Epoch 35/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.8903 - acc: 0.6577 - val_loss: 0.9897 - val_acc: 0.5663\n",
            "Epoch 36/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8760 - acc: 0.6712 - val_loss: 1.0297 - val_acc: 0.6024\n",
            "Epoch 37/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.8699 - acc: 0.6658 - val_loss: 1.0086 - val_acc: 0.5542\n",
            "Epoch 38/200\n",
            "742/742 [==============================] - 0s 54us/step - loss: 0.8559 - acc: 0.6725 - val_loss: 0.9691 - val_acc: 0.5904\n",
            "Epoch 39/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8476 - acc: 0.6860 - val_loss: 0.9860 - val_acc: 0.5663\n",
            "Epoch 40/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8464 - acc: 0.6981 - val_loss: 0.9747 - val_acc: 0.5783\n",
            "Epoch 41/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.8287 - acc: 0.6752 - val_loss: 0.9760 - val_acc: 0.6747\n",
            "Epoch 42/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.8214 - acc: 0.6698 - val_loss: 0.9690 - val_acc: 0.5904\n",
            "Epoch 43/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.8042 - acc: 0.6941 - val_loss: 0.9464 - val_acc: 0.6265\n",
            "Epoch 44/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.7963 - acc: 0.6941 - val_loss: 0.9718 - val_acc: 0.6145\n",
            "Epoch 45/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.7935 - acc: 0.7022 - val_loss: 0.9413 - val_acc: 0.6145\n",
            "Epoch 46/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7809 - acc: 0.7049 - val_loss: 0.9405 - val_acc: 0.6265\n",
            "Epoch 47/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7796 - acc: 0.7089 - val_loss: 0.9325 - val_acc: 0.6145\n",
            "Epoch 48/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.7725 - acc: 0.6981 - val_loss: 0.9338 - val_acc: 0.6265\n",
            "Epoch 49/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.7687 - acc: 0.7156 - val_loss: 0.9417 - val_acc: 0.6024\n",
            "Epoch 50/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7650 - acc: 0.6900 - val_loss: 0.9328 - val_acc: 0.6506\n",
            "Epoch 51/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7466 - acc: 0.7129 - val_loss: 0.9390 - val_acc: 0.6386\n",
            "Epoch 52/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.7448 - acc: 0.7197 - val_loss: 0.9160 - val_acc: 0.6265\n",
            "Epoch 53/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7332 - acc: 0.7399 - val_loss: 0.9354 - val_acc: 0.5904\n",
            "Epoch 54/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.7361 - acc: 0.7291 - val_loss: 0.8979 - val_acc: 0.6265\n",
            "Epoch 55/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7289 - acc: 0.7210 - val_loss: 0.9435 - val_acc: 0.6145\n",
            "Epoch 56/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.7190 - acc: 0.7237 - val_loss: 0.9130 - val_acc: 0.6506\n",
            "Epoch 57/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.7073 - acc: 0.7318 - val_loss: 0.8877 - val_acc: 0.6386\n",
            "Epoch 58/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.7042 - acc: 0.7453 - val_loss: 0.9073 - val_acc: 0.6386\n",
            "Epoch 59/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6985 - acc: 0.7399 - val_loss: 0.9094 - val_acc: 0.6024\n",
            "Epoch 60/200\n",
            "742/742 [==============================] - 0s 38us/step - loss: 0.6983 - acc: 0.7332 - val_loss: 0.9080 - val_acc: 0.6506\n",
            "Epoch 61/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.6927 - acc: 0.7412 - val_loss: 0.9064 - val_acc: 0.6024\n",
            "Epoch 62/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.6805 - acc: 0.7426 - val_loss: 0.8923 - val_acc: 0.6506\n",
            "Epoch 63/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6877 - acc: 0.7385 - val_loss: 0.9104 - val_acc: 0.6145\n",
            "Epoch 64/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.6805 - acc: 0.7305 - val_loss: 0.8922 - val_acc: 0.6265\n",
            "Epoch 65/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6855 - acc: 0.7332 - val_loss: 0.9111 - val_acc: 0.6506\n",
            "Epoch 66/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6734 - acc: 0.7372 - val_loss: 0.8892 - val_acc: 0.6145\n",
            "Epoch 67/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6725 - acc: 0.7439 - val_loss: 0.9150 - val_acc: 0.6265\n",
            "Epoch 68/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6623 - acc: 0.7385 - val_loss: 0.9008 - val_acc: 0.6627\n",
            "Epoch 69/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6653 - acc: 0.7439 - val_loss: 0.8866 - val_acc: 0.6145\n",
            "Epoch 70/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6518 - acc: 0.7547 - val_loss: 0.8923 - val_acc: 0.6386\n",
            "Epoch 71/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.6512 - acc: 0.7480 - val_loss: 0.8909 - val_acc: 0.6265\n",
            "Epoch 72/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.6478 - acc: 0.7426 - val_loss: 0.9016 - val_acc: 0.6024\n",
            "Epoch 73/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6480 - acc: 0.7655 - val_loss: 0.9025 - val_acc: 0.6265\n",
            "Epoch 74/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6511 - acc: 0.7345 - val_loss: 0.9096 - val_acc: 0.6145\n",
            "Epoch 75/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6450 - acc: 0.7615 - val_loss: 0.8711 - val_acc: 0.6386\n",
            "Epoch 76/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6349 - acc: 0.7439 - val_loss: 0.9042 - val_acc: 0.6627\n",
            "Epoch 77/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.6346 - acc: 0.7561 - val_loss: 0.8962 - val_acc: 0.6145\n",
            "Epoch 78/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.6255 - acc: 0.7655 - val_loss: 0.8775 - val_acc: 0.6145\n",
            "Epoch 79/200\n",
            "742/742 [==============================] - 0s 57us/step - loss: 0.6217 - acc: 0.7628 - val_loss: 0.8911 - val_acc: 0.6145\n",
            "Epoch 80/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6271 - acc: 0.7466 - val_loss: 0.9126 - val_acc: 0.5904\n",
            "Epoch 81/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6258 - acc: 0.7534 - val_loss: 0.8935 - val_acc: 0.6747\n",
            "Epoch 82/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.6185 - acc: 0.7642 - val_loss: 0.8996 - val_acc: 0.5904\n",
            "Epoch 83/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6326 - acc: 0.7493 - val_loss: 0.8901 - val_acc: 0.6145\n",
            "Epoch 84/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.6213 - acc: 0.7655 - val_loss: 0.9099 - val_acc: 0.6265\n",
            "Epoch 85/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6083 - acc: 0.7615 - val_loss: 0.8800 - val_acc: 0.6506\n",
            "Epoch 86/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6091 - acc: 0.7763 - val_loss: 0.9129 - val_acc: 0.6145\n",
            "Epoch 87/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.6058 - acc: 0.7655 - val_loss: 0.8969 - val_acc: 0.6506\n",
            "Epoch 88/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6120 - acc: 0.7682 - val_loss: 0.8746 - val_acc: 0.6506\n",
            "Epoch 89/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.6135 - acc: 0.7561 - val_loss: 0.9039 - val_acc: 0.6506\n",
            "Epoch 90/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.6051 - acc: 0.7749 - val_loss: 0.8838 - val_acc: 0.6506\n",
            "Epoch 91/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.6003 - acc: 0.7749 - val_loss: 0.8929 - val_acc: 0.6386\n",
            "Epoch 92/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5935 - acc: 0.7763 - val_loss: 0.8932 - val_acc: 0.6627\n",
            "Epoch 93/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5956 - acc: 0.7722 - val_loss: 0.8892 - val_acc: 0.6265\n",
            "Epoch 94/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5837 - acc: 0.7668 - val_loss: 0.8922 - val_acc: 0.6024\n",
            "Epoch 95/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5943 - acc: 0.7628 - val_loss: 0.8704 - val_acc: 0.6506\n",
            "Epoch 96/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5900 - acc: 0.7736 - val_loss: 0.8844 - val_acc: 0.6145\n",
            "Epoch 97/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.5818 - acc: 0.7722 - val_loss: 0.8975 - val_acc: 0.6265\n",
            "Epoch 98/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5926 - acc: 0.7722 - val_loss: 0.8949 - val_acc: 0.6506\n",
            "Epoch 99/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5907 - acc: 0.7668 - val_loss: 0.8986 - val_acc: 0.6145\n",
            "Epoch 100/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5821 - acc: 0.7722 - val_loss: 0.8657 - val_acc: 0.6386\n",
            "Epoch 101/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5806 - acc: 0.7830 - val_loss: 0.9041 - val_acc: 0.6627\n",
            "Epoch 102/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5725 - acc: 0.7871 - val_loss: 0.8977 - val_acc: 0.6386\n",
            "Epoch 103/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5743 - acc: 0.7776 - val_loss: 0.9286 - val_acc: 0.6265\n",
            "Epoch 104/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5754 - acc: 0.7736 - val_loss: 0.8974 - val_acc: 0.6506\n",
            "Epoch 105/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.5674 - acc: 0.7776 - val_loss: 0.9047 - val_acc: 0.6145\n",
            "Epoch 106/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5674 - acc: 0.7763 - val_loss: 0.9033 - val_acc: 0.6386\n",
            "Epoch 107/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5612 - acc: 0.7844 - val_loss: 0.8707 - val_acc: 0.6386\n",
            "Epoch 108/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5662 - acc: 0.7925 - val_loss: 0.9179 - val_acc: 0.6145\n",
            "Epoch 109/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.5635 - acc: 0.7938 - val_loss: 0.9058 - val_acc: 0.6506\n",
            "Epoch 110/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5588 - acc: 0.7736 - val_loss: 0.9028 - val_acc: 0.6386\n",
            "Epoch 111/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5781 - acc: 0.7736 - val_loss: 0.8874 - val_acc: 0.6506\n",
            "Epoch 112/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5878 - acc: 0.7628 - val_loss: 0.8900 - val_acc: 0.6386\n",
            "Epoch 113/200\n",
            "742/742 [==============================] - 0s 55us/step - loss: 0.5700 - acc: 0.7668 - val_loss: 0.9090 - val_acc: 0.6265\n",
            "Epoch 114/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5536 - acc: 0.7817 - val_loss: 0.8887 - val_acc: 0.6386\n",
            "Epoch 115/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5540 - acc: 0.7830 - val_loss: 0.8915 - val_acc: 0.6386\n",
            "Epoch 116/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5505 - acc: 0.7925 - val_loss: 0.9153 - val_acc: 0.6506\n",
            "Epoch 117/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5436 - acc: 0.7871 - val_loss: 0.8859 - val_acc: 0.6506\n",
            "Epoch 118/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5407 - acc: 0.7938 - val_loss: 0.8966 - val_acc: 0.6386\n",
            "Epoch 119/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5410 - acc: 0.7911 - val_loss: 0.9079 - val_acc: 0.6145\n",
            "Epoch 120/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5600 - acc: 0.7898 - val_loss: 0.9307 - val_acc: 0.6386\n",
            "Epoch 121/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.5660 - acc: 0.7763 - val_loss: 0.8808 - val_acc: 0.6506\n",
            "Epoch 122/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5505 - acc: 0.7763 - val_loss: 0.8950 - val_acc: 0.6386\n",
            "Epoch 123/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5400 - acc: 0.7992 - val_loss: 0.9012 - val_acc: 0.6386\n",
            "Epoch 124/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5348 - acc: 0.7830 - val_loss: 0.8993 - val_acc: 0.6386\n",
            "Epoch 125/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5318 - acc: 0.7951 - val_loss: 0.8792 - val_acc: 0.6506\n",
            "Epoch 126/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5298 - acc: 0.8032 - val_loss: 0.8835 - val_acc: 0.6265\n",
            "Epoch 127/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5326 - acc: 0.7951 - val_loss: 0.9079 - val_acc: 0.6265\n",
            "Epoch 128/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.5250 - acc: 0.8019 - val_loss: 0.9054 - val_acc: 0.6386\n",
            "Epoch 129/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5259 - acc: 0.8059 - val_loss: 0.8835 - val_acc: 0.6145\n",
            "Epoch 130/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5298 - acc: 0.8059 - val_loss: 0.9146 - val_acc: 0.6506\n",
            "Epoch 131/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5361 - acc: 0.7803 - val_loss: 0.9289 - val_acc: 0.6627\n",
            "Epoch 132/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.5338 - acc: 0.8005 - val_loss: 0.9038 - val_acc: 0.6506\n",
            "Epoch 133/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5203 - acc: 0.7951 - val_loss: 0.8957 - val_acc: 0.6506\n",
            "Epoch 134/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5128 - acc: 0.8059 - val_loss: 0.9235 - val_acc: 0.6506\n",
            "Epoch 135/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5168 - acc: 0.8019 - val_loss: 0.9080 - val_acc: 0.6627\n",
            "Epoch 136/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.5158 - acc: 0.8005 - val_loss: 0.9060 - val_acc: 0.6386\n",
            "Epoch 137/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5095 - acc: 0.8032 - val_loss: 0.9068 - val_acc: 0.6627\n",
            "Epoch 138/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5044 - acc: 0.8140 - val_loss: 0.8941 - val_acc: 0.6627\n",
            "Epoch 139/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.5128 - acc: 0.8059 - val_loss: 0.9093 - val_acc: 0.6506\n",
            "Epoch 140/200\n",
            "742/742 [==============================] - 0s 39us/step - loss: 0.5086 - acc: 0.7978 - val_loss: 0.9065 - val_acc: 0.6386\n",
            "Epoch 141/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5060 - acc: 0.8005 - val_loss: 0.9067 - val_acc: 0.6627\n",
            "Epoch 142/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5061 - acc: 0.8154 - val_loss: 0.8973 - val_acc: 0.6506\n",
            "Epoch 143/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4996 - acc: 0.8100 - val_loss: 0.9412 - val_acc: 0.6386\n",
            "Epoch 144/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.5073 - acc: 0.7925 - val_loss: 0.8756 - val_acc: 0.6386\n",
            "Epoch 145/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5124 - acc: 0.8032 - val_loss: 0.9699 - val_acc: 0.6506\n",
            "Epoch 146/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.5166 - acc: 0.7992 - val_loss: 0.9246 - val_acc: 0.6627\n",
            "Epoch 147/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.4958 - acc: 0.8100 - val_loss: 0.9727 - val_acc: 0.6627\n",
            "Epoch 148/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.5034 - acc: 0.8032 - val_loss: 0.9162 - val_acc: 0.6145\n",
            "Epoch 149/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.5003 - acc: 0.8086 - val_loss: 0.9165 - val_acc: 0.6386\n",
            "Epoch 150/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.5017 - acc: 0.8154 - val_loss: 0.8998 - val_acc: 0.6506\n",
            "Epoch 151/200\n",
            "742/742 [==============================] - 0s 51us/step - loss: 0.4966 - acc: 0.8127 - val_loss: 0.9350 - val_acc: 0.6265\n",
            "Epoch 152/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.4896 - acc: 0.8127 - val_loss: 0.9235 - val_acc: 0.6747\n",
            "Epoch 153/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.4880 - acc: 0.8194 - val_loss: 0.9045 - val_acc: 0.6386\n",
            "Epoch 154/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4822 - acc: 0.8221 - val_loss: 0.9177 - val_acc: 0.6747\n",
            "Epoch 155/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.4883 - acc: 0.8100 - val_loss: 0.9175 - val_acc: 0.6506\n",
            "Epoch 156/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.4855 - acc: 0.8154 - val_loss: 0.9438 - val_acc: 0.6265\n",
            "Epoch 157/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.4870 - acc: 0.8140 - val_loss: 0.9294 - val_acc: 0.6506\n",
            "Epoch 158/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.4985 - acc: 0.8032 - val_loss: 0.9471 - val_acc: 0.6145\n",
            "Epoch 159/200\n",
            "742/742 [==============================] - 0s 53us/step - loss: 0.4826 - acc: 0.8127 - val_loss: 0.9027 - val_acc: 0.6506\n",
            "Epoch 160/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.4822 - acc: 0.8181 - val_loss: 0.9294 - val_acc: 0.6627\n",
            "Epoch 161/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4896 - acc: 0.8059 - val_loss: 0.9383 - val_acc: 0.5904\n",
            "Epoch 162/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4992 - acc: 0.8046 - val_loss: 0.9336 - val_acc: 0.6506\n",
            "Epoch 163/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4823 - acc: 0.8248 - val_loss: 0.8986 - val_acc: 0.6506\n",
            "Epoch 164/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4800 - acc: 0.8113 - val_loss: 0.9364 - val_acc: 0.6024\n",
            "Epoch 165/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.4728 - acc: 0.8140 - val_loss: 0.9191 - val_acc: 0.6386\n",
            "Epoch 166/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.4700 - acc: 0.8154 - val_loss: 0.9093 - val_acc: 0.6627\n",
            "Epoch 167/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4696 - acc: 0.8194 - val_loss: 0.9149 - val_acc: 0.6386\n",
            "Epoch 168/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4744 - acc: 0.8329 - val_loss: 0.9241 - val_acc: 0.6145\n",
            "Epoch 169/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4753 - acc: 0.8154 - val_loss: 0.9198 - val_acc: 0.6265\n",
            "Epoch 170/200\n",
            "742/742 [==============================] - 0s 47us/step - loss: 0.4680 - acc: 0.8235 - val_loss: 0.9280 - val_acc: 0.6627\n",
            "Epoch 171/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.4640 - acc: 0.8221 - val_loss: 0.9260 - val_acc: 0.6386\n",
            "Epoch 172/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4640 - acc: 0.8261 - val_loss: 0.9267 - val_acc: 0.6386\n",
            "Epoch 173/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4640 - acc: 0.8288 - val_loss: 0.9306 - val_acc: 0.6386\n",
            "Epoch 174/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4577 - acc: 0.8302 - val_loss: 0.9050 - val_acc: 0.6506\n",
            "Epoch 175/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.4672 - acc: 0.8194 - val_loss: 0.9524 - val_acc: 0.6386\n",
            "Epoch 176/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.4657 - acc: 0.8235 - val_loss: 0.9211 - val_acc: 0.6627\n",
            "Epoch 177/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4556 - acc: 0.8410 - val_loss: 0.9218 - val_acc: 0.6506\n",
            "Epoch 178/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4598 - acc: 0.8356 - val_loss: 0.9451 - val_acc: 0.6386\n",
            "Epoch 179/200\n",
            "742/742 [==============================] - 0s 42us/step - loss: 0.4540 - acc: 0.8329 - val_loss: 0.9087 - val_acc: 0.6265\n",
            "Epoch 180/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.4592 - acc: 0.8248 - val_loss: 0.9607 - val_acc: 0.6747\n",
            "Epoch 181/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4586 - acc: 0.8261 - val_loss: 0.9258 - val_acc: 0.6024\n",
            "Epoch 182/200\n",
            "742/742 [==============================] - 0s 48us/step - loss: 0.4619 - acc: 0.8383 - val_loss: 0.9232 - val_acc: 0.6386\n",
            "Epoch 183/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4646 - acc: 0.8248 - val_loss: 0.9225 - val_acc: 0.6506\n",
            "Epoch 184/200\n",
            "742/742 [==============================] - 0s 46us/step - loss: 0.4522 - acc: 0.8491 - val_loss: 0.9487 - val_acc: 0.6386\n",
            "Epoch 185/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.4513 - acc: 0.8288 - val_loss: 0.9454 - val_acc: 0.6386\n",
            "Epoch 186/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.4520 - acc: 0.8275 - val_loss: 0.9140 - val_acc: 0.6265\n",
            "Epoch 187/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4575 - acc: 0.8329 - val_loss: 0.9366 - val_acc: 0.6506\n",
            "Epoch 188/200\n",
            "742/742 [==============================] - 0s 55us/step - loss: 0.4535 - acc: 0.8302 - val_loss: 0.9494 - val_acc: 0.6506\n",
            "Epoch 189/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4712 - acc: 0.8181 - val_loss: 0.9205 - val_acc: 0.6627\n",
            "Epoch 190/200\n",
            "742/742 [==============================] - 0s 52us/step - loss: 0.4522 - acc: 0.8302 - val_loss: 0.9914 - val_acc: 0.6506\n",
            "Epoch 191/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4593 - acc: 0.8181 - val_loss: 0.9553 - val_acc: 0.6627\n",
            "Epoch 192/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4655 - acc: 0.8235 - val_loss: 0.9199 - val_acc: 0.6747\n",
            "Epoch 193/200\n",
            "742/742 [==============================] - 0s 50us/step - loss: 0.4410 - acc: 0.8423 - val_loss: 0.9291 - val_acc: 0.6386\n",
            "Epoch 194/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4505 - acc: 0.8194 - val_loss: 0.9917 - val_acc: 0.6145\n",
            "Epoch 195/200\n",
            "742/742 [==============================] - 0s 43us/step - loss: 0.4480 - acc: 0.8181 - val_loss: 0.9423 - val_acc: 0.6145\n",
            "Epoch 196/200\n",
            "742/742 [==============================] - 0s 49us/step - loss: 0.4456 - acc: 0.8329 - val_loss: 0.9512 - val_acc: 0.6145\n",
            "Epoch 197/200\n",
            "742/742 [==============================] - 0s 45us/step - loss: 0.4431 - acc: 0.8342 - val_loss: 0.9495 - val_acc: 0.6265\n",
            "Epoch 198/200\n",
            "742/742 [==============================] - 0s 44us/step - loss: 0.4331 - acc: 0.8383 - val_loss: 0.9532 - val_acc: 0.6386\n",
            "Epoch 199/200\n",
            "742/742 [==============================] - 0s 41us/step - loss: 0.4418 - acc: 0.8288 - val_loss: 0.9322 - val_acc: 0.6386\n",
            "Epoch 200/200\n",
            "742/742 [==============================] - 0s 40us/step - loss: 0.4459 - acc: 0.8423 - val_loss: 0.9530 - val_acc: 0.6386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5OheaZjYxZu",
        "colab_type": "code",
        "outputId": "b514644e-9e57-458a-de7c-727662082278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#Printing out last value of the Training. More complicated network *should* be better... \n",
        "\n",
        "print(hist_16_16.history['loss'][199])\n",
        "print(hist_32_16.history['loss'][199])\n",
        "print(hist_64_16.history['loss'][199])\n",
        "print(hist_128_16.history['loss'][199])\n",
        "\n",
        "print(hist_16_32.history['loss'][199])\n",
        "print(hist_32_32.history['loss'][199])\n",
        "print(hist_64_32.history['loss'][199])\n",
        "print(hist_128_32.history['loss'][199])\n",
        "\n",
        "print(hist_16_64.history['loss'][199])\n",
        "print(hist_32_64.history['loss'][199])\n",
        "print(hist_64_64.history['loss'][199])\n",
        "print(hist_128_64.history['loss'][199])\n",
        "\n",
        "print(hist_16_128.history['loss'][199])\n",
        "print(hist_32_128.history['loss'][199])\n",
        "print(hist_64_128.history['loss'][199])\n",
        "print(hist_128_128.history['loss'][199])\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8461064077773184\n",
            "0.7096212711938308\n",
            "0.6671552437977649\n",
            "0.5769424158929172\n",
            "0.7157031167549586\n",
            "0.6680119312355782\n",
            "0.5945279811913112\n",
            "0.5240027141378254\n",
            "0.6875809709016846\n",
            "0.6012813204703626\n",
            "0.5475018544981101\n",
            "0.5042777149825083\n",
            "0.6205842862874671\n",
            "0.5540582140822294\n",
            "0.49682245807185005\n",
            "0.44588878670471377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yFhj2lQ1HE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e29423cc-30a3-4f43-94d5-b597b0340b67"
      },
      "source": [
        "#Printing out last value of the validation. More complicated network *should* be better... \n",
        "\n",
        "print(hist_16_16.history['val_loss'][199])\n",
        "print(hist_32_16.history['val_loss'][199])\n",
        "print(hist_64_16.history['val_loss'][199])\n",
        "print(hist_128_16.history['val_loss'][199])\n",
        "\n",
        "print(hist_16_32.history['val_loss'][199])\n",
        "print(hist_32_32.history['val_loss'][199])\n",
        "print(hist_64_32.history['val_loss'][199])\n",
        "print(hist_128_32.history['val_loss'][199])\n",
        "\n",
        "print(hist_16_64.history['val_loss'][199])\n",
        "print(hist_32_64.history['val_loss'][199])\n",
        "print(hist_64_64.history['val_loss'][199])\n",
        "print(hist_128_64.history['val_loss'][199])\n",
        "\n",
        "print(hist_16_128.history['val_loss'][199])\n",
        "print(hist_32_128.history['val_loss'][199])\n",
        "print(hist_64_128.history['val_loss'][199])\n",
        "print(hist_128_128.history['val_loss'][199])\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9158114747828748\n",
            "0.8319253052573606\n",
            "0.8725389471973282\n",
            "0.8256289765059229\n",
            "0.8381305825279419\n",
            "0.8309252003589308\n",
            "0.830909128648689\n",
            "0.8854168129254536\n",
            "0.7898384009499148\n",
            "0.9157857772815658\n",
            "0.8949352093489773\n",
            "0.8534129542040537\n",
            "0.8429134330117559\n",
            "1.0060248482658203\n",
            "0.9978451542107456\n",
            "0.9529868486415909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxlhqLGRzkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "44930a1c-3c0b-418c-988a-5afef8f046cb"
      },
      "source": [
        "#Printing out last value of the Accuracy. More complicated network *should* be better... \n",
        "\n",
        "print(hist_16_16.history['acc'][199])\n",
        "print(hist_32_16.history['acc'][199])\n",
        "print(hist_64_16.history['acc'][199])\n",
        "print(hist_128_16.history['acc'][199])\n",
        "\n",
        "print(hist_16_32.history['acc'][199])\n",
        "print(hist_32_32.history['acc'][199])\n",
        "print(hist_64_32.history['acc'][199])\n",
        "print(hist_128_32.history['acc'][199])\n",
        "\n",
        "print(hist_16_64.history['acc'][199])\n",
        "print(hist_32_64.history['acc'][199])\n",
        "print(hist_64_64.history['acc'][199])\n",
        "print(hist_128_64.history['acc'][199])\n",
        "\n",
        "print(hist_16_128.history['acc'][199])\n",
        "print(hist_32_128.history['acc'][199])\n",
        "print(hist_64_128.history['acc'][199])\n",
        "print(hist_128_128.history['acc'][199])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.675202156012913\n",
            "0.7277628037164796\n",
            "0.7250673860873816\n",
            "0.7735849071063121\n",
            "0.7277628017885666\n",
            "0.7466307285661004\n",
            "0.7628032340193694\n",
            "0.7870619942878456\n",
            "0.7479784369790008\n",
            "0.7722371962835204\n",
            "0.7830188684065066\n",
            "0.8018867937381056\n",
            "0.7574123994038111\n",
            "0.7749326150372343\n",
            "0.808625338212499\n",
            "0.8423180605844668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXYIWKE5XOOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6b468eb5-e749-4fd7-a953-19a2c652c2df"
      },
      "source": [
        "#Printing out last value of the Accuracy. More complicated network *should* be better... \n",
        "\n",
        "print(hist_16_16.history['val_acc'][199])\n",
        "print(hist_32_16.history['val_acc'][199])\n",
        "print(hist_64_16.history['val_acc'][199])\n",
        "print(hist_128_16.history['val_acc'][199])\n",
        "\n",
        "print(hist_16_32.history['val_acc'][199])\n",
        "print(hist_32_32.history['val_acc'][199])\n",
        "print(hist_64_32.history['val_acc'][199])\n",
        "print(hist_128_32.history['val_acc'][199])\n",
        "\n",
        "print(hist_16_64.history['val_acc'][199])\n",
        "print(hist_32_64.history['val_acc'][199])\n",
        "print(hist_64_64.history['val_acc'][199])\n",
        "print(hist_128_64.history['val_acc'][199])\n",
        "\n",
        "print(hist_16_128.history['val_acc'][199])\n",
        "print(hist_32_128.history['val_acc'][199])\n",
        "print(hist_64_128.history['val_acc'][199])\n",
        "print(hist_128_128.history['val_acc'][199])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6867469822067812\n",
            "0.7710843351950128\n",
            "0.6987951785685068\n",
            "0.6746987980532358\n",
            "0.7349397568817598\n",
            "0.6867469908243202\n",
            "0.6867469908243202\n",
            "0.6987951785685068\n",
            "0.7469879582703832\n",
            "0.6506024074841694\n",
            "0.6506024074841694\n",
            "0.6867469857974224\n",
            "0.6506024074841694\n",
            "0.6626506002552538\n",
            "0.6265060269688986\n",
            "0.6385542183037264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUJpgpQkMfGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "da5d5fcc-c51f-4b8e-8189-37b8ed1474e7"
      },
      "source": [
        "\n",
        "# Storing the evaluation performance \n",
        "\n",
        "[Loss_16_16, Acc_16_16] = model_16_16.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_16_32, Acc_16_32] = model_16_32.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_16_64, Acc_16_64] = model_16_64.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_16_128, Acc_16_128] = model_16_128.evaluate(X_test_scaled, Y_test)\n",
        "\n",
        "[Loss_32_16, Acc_32_16] = model_32_16.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_32_32, Acc_32_32] = model_32_32.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_32_64, Acc_32_64] = model_32_64.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_32_128, Acc_32_128] = model_32_128.evaluate(X_test_scaled, Y_test)\n",
        "\n",
        "[Loss_64_16, Acc_64_16] = model_64_16.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_64_32, Acc_64_32] = model_64_32.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_64_64, Acc_64_64] = model_64_64.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_64_128, Acc_64_128] = model_64_128.evaluate(X_test_scaled, Y_test)\n",
        "\n",
        "[Loss_128_16, Acc_128_16] = model_128_16.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_128_32, Acc_128_32] = model_128_32.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_128_64, Acc_128_64] = model_128_64.evaluate(X_test_scaled, Y_test)\n",
        "[Loss_128_128, Acc_128_128] = model_128_128.evaluate(X_test_scaled, Y_test)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "276/276 [==============================] - 0s 55us/step\n",
            "276/276 [==============================] - 0s 50us/step\n",
            "276/276 [==============================] - 0s 48us/step\n",
            "276/276 [==============================] - 0s 55us/step\n",
            "276/276 [==============================] - 0s 54us/step\n",
            "276/276 [==============================] - 0s 53us/step\n",
            "276/276 [==============================] - 0s 53us/step\n",
            "276/276 [==============================] - 0s 51us/step\n",
            "276/276 [==============================] - 0s 39us/step\n",
            "276/276 [==============================] - 0s 48us/step\n",
            "276/276 [==============================] - 0s 46us/step\n",
            "276/276 [==============================] - 0s 46us/step\n",
            "276/276 [==============================] - 0s 44us/step\n",
            "276/276 [==============================] - 0s 43us/step\n",
            "276/276 [==============================] - 0s 39us/step\n",
            "276/276 [==============================] - 0s 49us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjjuB4cY-8Z",
        "colab_type": "code",
        "outputId": "d4f69d6f-4e51-4328-ab3c-d04b72a012ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Plotting for visual effect\n",
        "\n",
        "#plt.plot(hist_16_16.history['loss'])\n",
        "#plt.plot(hist_16_32.history['val_loss'])\n",
        "#plt.plot(hist_32_16.history['loss'])\n",
        "#plt.plot(hist_32_32.history['val_loss'])\n",
        "#plt.plot(hist_64_16.history['loss'])\n",
        "#plt.plot(hist_64_32.history['val_loss'])\n",
        "#plt.plot(hist_128_16.history['loss'])\n",
        "#plt.plot(hist_128_32.history['val_loss'])\n",
        "\n",
        "# plot the loss\n",
        "plt.plot(hist_16_32.history['loss'])\n",
        "plt.plot(hist_16_32.history['val_loss'])\n",
        "plt.plot(hist_32_32.history['loss'])\n",
        "plt.plot(hist_32_32.history['val_loss'])\n",
        "plt.plot(hist_64_32.history['loss'])\n",
        "plt.plot(hist_64_32.history['val_loss'])\n",
        "plt.plot(hist_128_32.history['loss'])\n",
        "plt.plot(hist_128_32.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train_16_32', 'Val_16_32', 'Train_32_32', 'Val_32_32', 'Train_64_32', 'Val_64_32', 'Train_128_32', 'Val_128_32'], loc = 'upper right')\n",
        "#plt.legend(['Train_16_16', 'Train_32_16', 'Train_64_16', 'Train_128_16', 'Train_16_32', 'Train_32_32', 'Train_64_32', 'Train_128_32'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbUX/H9+AqR7VotUcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf6Tb-nVMKx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "487d768d-1516-474e-f126-7f3c9a9f225b"
      },
      "source": [
        "\n",
        "plt.plot(hist_16_64.history['loss'])\n",
        "#plt.plot(hist_16_32.history['val_loss'])\n",
        "plt.plot(hist_32_64.history['loss'])\n",
        "#plt.plot(hist_32_32.history['val_loss'])\n",
        "plt.plot(hist_64_64.history['loss'])\n",
        "#plt.plot(hist_64_32.history['val_loss'])\n",
        "plt.plot(hist_128_64.history['loss'])\n",
        "#plt.plot(hist_128_32.history['val_loss'])\n",
        "\n",
        "# plot the loss\n",
        "plt.plot(hist_16_128.history['loss'])\n",
        "#plt.plot(hist_16_32.history['val_loss'])\n",
        "plt.plot(hist_32_128.history['loss'])\n",
        "#plt.plot(hist_32_32.history['val_loss'])\n",
        "plt.plot(hist_64_128.history['loss'])\n",
        "#plt.plot(hist_64_32.history['val_loss'])\n",
        "plt.plot(hist_128_128.history['loss'])\n",
        "#plt.plot(hist_128_32.history['val_loss'])\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.legend(['Train', 'Val', 'Train2', 'Val2', 'Train3', 'Val3', 'Train4', 'Val4'], loc = 'upper right')\n",
        "plt.legend(['Train_16_64', 'Train_32_64', 'Train_64_64', 'Train_128_64', 'Train_16_128', 'Train_32_128', 'Train_64_128', 'Train_128_128'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJitLEMYYY7L6f0fNx/5MyVK1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_JDsq7JZC6-",
        "colab_type": "code",
        "outputId": "b78378c7-1e13-44a6-a10d-6e0818c1c4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#plot the accuracy\n",
        "\n",
        "#plt.plot(hist_16_32.history['acc'])\n",
        "#plt.plot(hist_16_16.history['val_acc'])\n",
        "#plt.plot(hist_32_32.history['acc'])\n",
        "#plt.plot(hist_32_16.history['val_acc'])\n",
        "#plt.plot(hist_64_32.history['acc'])\n",
        "#plt.plot(hist_64_16.history['val_acc'])\n",
        "#plt.plot(hist_128_32.history['acc'])\n",
        "#plt.plot(hist_128_16.history['val_acc'])\n",
        "\n",
        "plt.plot(hist_16_32.history['acc'])\n",
        "plt.plot(hist_16_32.history['val_acc'])\n",
        "plt.plot(hist_32_32.history['acc'])\n",
        "plt.plot(hist_32_32.history['val_acc'])\n",
        "plt.plot(hist_64_32.history['acc'])\n",
        "plt.plot(hist_64_32.history['val_acc'])\n",
        "plt.plot(hist_128_32.history['acc'])\n",
        "plt.plot(hist_128_32.history['val_acc'])\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "#plt.legend(['Val', 'Val2', 'Val3', 'Val4'], loc = 'lower right')\n",
        "plt.legend(['Train_16_32', 'Val_16_32', 'Train_32_32', 'Val_32_32', 'Train_64_32', 'Val_64_32', 'Train_128_32', 'Val_128_32'], loc = 'lower right')\n",
        "plt.show()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrsFOU--Um5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "87fdc5f2-5fe1-4f42-a97d-9ddddd81a614"
      },
      "source": [
        "  #plot the accuracy\n",
        "\n",
        "#plt.plot(hist_16_32.history['acc'])\n",
        "plt.plot(hist_16_64.history['val_acc'])\n",
        "#plt.plot(hist_32_32.history['acc'])\n",
        "plt.plot(hist_32_64.history['val_acc'])\n",
        "#plt.plot(hist_64_32.history['acc'])\n",
        "plt.plot(hist_64_64.history['val_acc'])\n",
        "#plt.plot(hist_128_32.history['acc'])\n",
        "plt.plot(hist_128_64.history['val_acc'])\n",
        "\n",
        "plt.plot(hist_16_128.history['val_acc'])\n",
        "#plt.plot(hist_32_32.history['acc'])\n",
        "plt.plot(hist_32_128.history['val_acc'])\n",
        "#plt.plot(hist_64_32.history['acc'])\n",
        "plt.plot(hist_64_128.history['val_acc'])\n",
        "#plt.plot(hist_128_32.history['acc'])\n",
        "plt.plot(hist_128_128.history['val_acc'])\n",
        "\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Val_16_64', 'Val_32_64', 'Val_64_64', 'Val_128_64', 'Val_16_128', 'Val_32_128', 'Val_64_128', 'Val_128_128'], loc = 'lower right')\n",
        "#plt.legend(['Train', 'Val', 'Train2', 'Val2', 'Train3', 'Val3', 'Train4', 'Val4'], loc = 'lower right')\n",
        "plt.show()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzDrOHYGZWce5IzAz6zh3BGZmHXce4wShSbUzVyIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG-T68SGsN9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "65a572d7-8a32-425f-f2ce-63e68b31e7a7"
      },
      "source": [
        "print(Acc_16_16)\n",
        "print(Acc_16_32)\n",
        "print(Acc_16_64)\n",
        "print(Acc_16_128)\n",
        "\n",
        "print(Acc_32_16)\n",
        "print(Acc_32_32)\n",
        "print(Acc_32_64)\n",
        "print(Acc_32_128)\n",
        "\n",
        "print(Acc_64_16)\n",
        "print(Acc_64_32)\n",
        "print(Acc_64_64)\n",
        "print(Acc_64_128)\n",
        "\n",
        "print(Acc_128_16)\n",
        "print(Acc_128_32)\n",
        "print(Acc_128_64)\n",
        "print(Acc_128_128)\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6159420289855072\n",
            "0.6702898559362992\n",
            "0.6557971031769462\n",
            "0.637681157692619\n",
            "0.6521739139073137\n",
            "0.648550726365352\n",
            "0.6376811602841252\n",
            "0.6413043495537578\n",
            "0.6376811594202898\n",
            "0.6557971031769462\n",
            "0.6340579718783281\n",
            "0.6413043478260869\n",
            "0.6557971023131108\n",
            "0.6485507255015166\n",
            "0.6376811602841252\n",
            "0.6413043486899224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOstQ-uv0EZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b7b136b6-ecd3-4c26-b748-010444a9c53e"
      },
      "source": [
        "print(Loss_16_16)\n",
        "print(Loss_16_32)\n",
        "print(Loss_16_64)\n",
        "print(Loss_16_128)\n",
        "\n",
        "print(Loss_32_16)\n",
        "print(Loss_32_32)\n",
        "print(Loss_32_64)\n",
        "print(Loss_32_128)\n",
        "\n",
        "print(Loss_64_16)\n",
        "print(Loss_64_32)\n",
        "print(Loss_64_64)\n",
        "print(Loss_64_128)\n",
        "\n",
        "print(Loss_128_16)\n",
        "print(Loss_128_32)\n",
        "print(Loss_128_64)\n",
        "print(Loss_128_128)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9542327056760374\n",
            "0.8986314828845038\n",
            "0.8655295631159907\n",
            "0.9453376134236654\n",
            "0.8825117410093114\n",
            "0.9168307141981263\n",
            "0.9907052750172822\n",
            "0.9776625857836958\n",
            "0.9006468100824218\n",
            "0.9037364479424297\n",
            "0.9615308115447777\n",
            "1.0188227770985037\n",
            "0.9247718185618303\n",
            "0.9914429330307505\n",
            "0.9565808116525844\n",
            "1.1100956063339675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eSyvoyrc_kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}