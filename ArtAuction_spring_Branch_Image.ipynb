{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArtAuction_spring_Branch_Image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gohenry/DataScienceCoursera/blob/master/ArtAuction_spring_Branch_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt4UBerZ8ya7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install --user --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ZLzTDiKU3i",
        "colab_type": "code",
        "outputId": "6574fb1c-770a-4a4b-d9df-0b4124a136e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "help(tf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on package tensorflow:\n",
            "\n",
            "NAME\n",
            "    tensorflow\n",
            "\n",
            "DESCRIPTION\n",
            "    Top-level module of TensorFlow. By convention, we refer to this module as \n",
            "    `tf` instead of `tensorflow`, following the common practice of importing \n",
            "    TensorFlow via the command `import tensorflow as tf`.\n",
            "    \n",
            "    The primary function of this module is to import all of the public TensorFlow \n",
            "    interfaces into a single place. The interfaces themselves are located in \n",
            "    sub-modules, as described below.\n",
            "    \n",
            "    Note that the file `__init__.py` in the TensorFlow source code tree is actually \n",
            "    only a placeholder to enable test cases to run. The TensorFlow build replaces \n",
            "    this file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    _api (package)\n",
            "    audio (package)\n",
            "    autodiff (package)\n",
            "    autograph (package)\n",
            "    bitwise (package)\n",
            "    compat (package)\n",
            "    compiler (package)\n",
            "    config (package)\n",
            "    core (package)\n",
            "    data (package)\n",
            "    debugging (package)\n",
            "    distribute (package)\n",
            "    dtypes (package)\n",
            "    errors (package)\n",
            "    estimator (package)\n",
            "    examples (package)\n",
            "    experimental (package)\n",
            "    feature_column (package)\n",
            "    graph_util (package)\n",
            "    image (package)\n",
            "    io (package)\n",
            "    keras (package)\n",
            "    linalg (package)\n",
            "    lite (package)\n",
            "    lookup (package)\n",
            "    math (package)\n",
            "    mixed_precision (package)\n",
            "    mlir (package)\n",
            "    nest (package)\n",
            "    nn (package)\n",
            "    profiler (package)\n",
            "    python (package)\n",
            "    quantization (package)\n",
            "    queue (package)\n",
            "    ragged (package)\n",
            "    random (package)\n",
            "    raw_ops (package)\n",
            "    saved_model (package)\n",
            "    sets (package)\n",
            "    signal (package)\n",
            "    sparse (package)\n",
            "    strings (package)\n",
            "    summary (package)\n",
            "    sysconfig (package)\n",
            "    test (package)\n",
            "    tools (package)\n",
            "    tpu (package)\n",
            "    train (package)\n",
            "    v2\n",
            "    version (package)\n",
            "    xla (package)\n",
            "\n",
            "SUBMODULES\n",
            "    _API_MODULE\n",
            "    _compat\n",
            "    _fi\n",
            "    _ll\n",
            "    _module_util\n",
            "    initializers\n",
            "    losses\n",
            "    metrics\n",
            "    optimizers\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        tensorflow.python.eager.backprop.GradientTape\n",
            "        tensorflow.python.framework.device_spec.DeviceSpecV2\n",
            "        tensorflow.python.framework.ops.Graph\n",
            "        tensorflow.python.framework.ops.Operation\n",
            "        tensorflow.python.framework.ops.RegisterGradient\n",
            "        tensorflow.python.framework.ops.name_scope_v2\n",
            "        tensorflow.python.framework.tensor_shape.TensorShape\n",
            "        tensorflow.python.framework.type_spec.TypeSpec\n",
            "            tensorflow.python.data.ops.optional_ops.OptionalSpec\n",
            "            tensorflow.python.framework.indexed_slices.IndexedSlicesSpec\n",
            "            tensorflow.python.ops.tensor_array_ops.TensorArraySpec\n",
            "        tensorflow.python.ops.critical_section_ops.CriticalSection\n",
            "        tensorflow.python.ops.gradients_util.AggregationMethod\n",
            "        tensorflow.python.ops.tensor_array_ops.TensorArray\n",
            "    enum.Enum(builtins.object)\n",
            "        tensorflow.python.ops.unconnected_gradients.UnconnectedGradients\n",
            "        tensorflow.python.ops.variables.VariableAggregationV2\n",
            "        tensorflow.python.ops.variables.VariableSynchronization\n",
            "    tensorflow.python._dtypes.DType(pybind11_builtins.pybind11_object)\n",
            "        tensorflow.python.framework.dtypes.DType\n",
            "    tensorflow.python.framework.composite_tensor.CompositeTensor(builtins.object)\n",
            "        tensorflow.python.framework.indexed_slices.IndexedSlices(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "        tensorflow.python.framework.sparse_tensor.SparseTensor(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "        tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor\n",
            "    tensorflow.python.framework.tensor_like._TensorLike(builtins.object)\n",
            "        tensorflow.python.framework.indexed_slices.IndexedSlices(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "        tensorflow.python.framework.ops.Tensor\n",
            "        tensorflow.python.framework.sparse_tensor.SparseTensor(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "    tensorflow.python.framework.tensor_spec.DenseSpec(tensorflow.python.framework.type_spec.TypeSpec)\n",
            "        tensorflow.python.framework.tensor_spec.TensorSpec(tensorflow.python.framework.tensor_spec.DenseSpec, tensorflow.python.framework.type_spec.BatchableTypeSpec)\n",
            "    tensorflow.python.framework.type_spec.BatchableTypeSpec(tensorflow.python.framework.type_spec.TypeSpec)\n",
            "        tensorflow.python.framework.sparse_tensor.SparseTensorSpec\n",
            "        tensorflow.python.framework.tensor_spec.TensorSpec(tensorflow.python.framework.tensor_spec.DenseSpec, tensorflow.python.framework.type_spec.BatchableTypeSpec)\n",
            "        tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec\n",
            "    tensorflow.python.ops.init_ops_v2.Initializer(builtins.object)\n",
            "        tensorflow.python.ops.init_ops_v2.Constant\n",
            "        tensorflow.python.ops.init_ops_v2.Ones\n",
            "        tensorflow.python.ops.init_ops_v2.RandomNormal\n",
            "        tensorflow.python.ops.init_ops_v2.RandomUniform\n",
            "        tensorflow.python.ops.init_ops_v2.Zeros\n",
            "    tensorflow.python.training.tracking.base.Trackable(builtins.object)\n",
            "        tensorflow.python.ops.variables.Variable\n",
            "    tensorflow.python.training.tracking.tracking.AutoTrackable(tensorflow.python.training.tracking.base.Trackable)\n",
            "        tensorflow.python.module.module.Module\n",
            "    \n",
            "    class AggregationMethod(builtins.object)\n",
            "     |  A class listing aggregation methods used to combine gradients.\n",
            "     |  \n",
            "     |  Computing partial derivatives can require aggregating gradient\n",
            "     |  contributions. This class lists the various methods that can\n",
            "     |  be used to combine gradients in the graph.\n",
            "     |  \n",
            "     |  The following aggregation methods are part of the stable API for\n",
            "     |  aggregating gradients:\n",
            "     |  \n",
            "     |  *  `ADD_N`: All of the gradient terms are summed as part of one\n",
            "     |     operation using the \"AddN\" op (see `tf.add_n`). This\n",
            "     |     method has the property that all gradients must be ready and\n",
            "     |     buffered separately in memory before any aggregation is performed.\n",
            "     |  *  `DEFAULT`: The system-chosen default aggregation method.\n",
            "     |  \n",
            "     |  The following aggregation methods are experimental and may not\n",
            "     |  be supported in future releases:\n",
            "     |  \n",
            "     |  * `EXPERIMENTAL_TREE`: Gradient terms are summed in pairs using\n",
            "     |    using the \"AddN\" op. This method of summing gradients may reduce\n",
            "     |    performance, but it can improve memory utilization because the\n",
            "     |    gradients can be released earlier.\n",
            "     |  \n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  ADD_N = 0\n",
            "     |  \n",
            "     |  DEFAULT = 0\n",
            "     |  \n",
            "     |  EXPERIMENTAL_ACCUMULATE_N = 2\n",
            "     |  \n",
            "     |  EXPERIMENTAL_TREE = 1\n",
            "    \n",
            "    class CriticalSection(builtins.object)\n",
            "     |  Critical section.\n",
            "     |  \n",
            "     |  A `CriticalSection` object is a resource in the graph which executes subgraphs\n",
            "     |  in **serial** order.  A common example of a subgraph one may wish to run\n",
            "     |  exclusively is the one given by the following function:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  v = resource_variable_ops.ResourceVariable(0.0, name=\"v\")\n",
            "     |  \n",
            "     |  def count():\n",
            "     |    value = v.read_value()\n",
            "     |    with tf.control_dependencies([value]):\n",
            "     |      with tf.control_dependencies([v.assign_add(1)]):\n",
            "     |        return tf.identity(value)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Here, a snapshot of `v` is captured in `value`; and then `v` is updated.\n",
            "     |  The snapshot value is returned.\n",
            "     |  \n",
            "     |  If multiple workers or threads all execute `count` in parallel, there is no\n",
            "     |  guarantee that access to the variable `v` is atomic at any point within\n",
            "     |  any thread's calculation of `count`.  In fact, even implementing an atomic\n",
            "     |  counter that guarantees that the user will see each value `0, 1, ...,` is\n",
            "     |  currently impossible.\n",
            "     |  \n",
            "     |  The solution is to ensure any access to the underlying resource `v` is\n",
            "     |  only processed through a critical section:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  cs = CriticalSection()\n",
            "     |  f1 = cs.execute(count)\n",
            "     |  f2 = cs.execute(count)\n",
            "     |  output = f1 + f2\n",
            "     |  session.run(output)\n",
            "     |  ```\n",
            "     |  The functions `f1` and `f2` will be executed serially, and updates to `v`\n",
            "     |  will be atomic.\n",
            "     |  \n",
            "     |  **NOTES**\n",
            "     |  \n",
            "     |  All resource objects, including the critical section and any captured\n",
            "     |  variables of functions executed on that critical section, will be\n",
            "     |  colocated to the same device (host and cpu/gpu).\n",
            "     |  \n",
            "     |  When using multiple critical sections on the same resources, there is no\n",
            "     |  guarantee of exclusive access to those resources.  This behavior is disallowed\n",
            "     |  by default (but see the kwarg `exclusive_resource_access`).\n",
            "     |  \n",
            "     |  For example, running the same function in two separate critical sections\n",
            "     |  will not ensure serial execution:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  v = tf.compat.v1.get_variable(\"v\", initializer=0.0, use_resource=True)\n",
            "     |  def accumulate(up):\n",
            "     |    x = v.read_value()\n",
            "     |    with tf.control_dependencies([x]):\n",
            "     |      with tf.control_dependencies([v.assign_add(up)]):\n",
            "     |        return tf.identity(x)\n",
            "     |  ex1 = CriticalSection().execute(\n",
            "     |    accumulate, 1.0, exclusive_resource_access=False)\n",
            "     |  ex2 = CriticalSection().execute(\n",
            "     |    accumulate, 1.0, exclusive_resource_access=False)\n",
            "     |  bad_sum = ex1 + ex2\n",
            "     |  sess.run(v.initializer)\n",
            "     |  sess.run(bad_sum)  # May return 0.0\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, name=None, shared_name=None, critical_section_def=None, import_scope=None)\n",
            "     |      Creates a critical section.\n",
            "     |  \n",
            "     |  execute(self, fn, exclusive_resource_access=True, name=None)\n",
            "     |      Execute function `fn()` inside the critical section.\n",
            "     |      \n",
            "     |      `fn` should not accept any arguments.  To add extra arguments to when\n",
            "     |      calling `fn` in the critical section, create a lambda:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      critical_section.execute(lambda: fn(*my_args, **my_kwargs))\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        fn: The function to execute.  Must return at least one tensor.\n",
            "     |        exclusive_resource_access: Whether the resources required by\n",
            "     |          `fn` should be exclusive to this `CriticalSection`.  Default: `True`.\n",
            "     |          You may want to set this to `False` if you will be accessing a\n",
            "     |          resource in read-only mode in two different CriticalSections.\n",
            "     |        name: The name to use when creating the execute operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The tensors returned from `fn()`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `fn` attempts to lock this `CriticalSection` in any nested\n",
            "     |          or lazy way that may cause a deadlock.\n",
            "     |        ValueError: If `exclusive_resource_access == True` and\n",
            "     |          another `CriticalSection` has an execution requesting the same\n",
            "     |          resources as `fn``.  Note, even if `exclusive_resource_access` is\n",
            "     |          `True`, if another execution in another `CriticalSection` was created\n",
            "     |          without `exclusive_resource_access=True`, a `ValueError` will be raised.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  name\n",
            "    \n",
            "    class DType(tensorflow.python._dtypes.DType)\n",
            "     |  Represents the type of the elements in a `Tensor`.\n",
            "     |  \n",
            "     |  The following `DType` objects are defined:\n",
            "     |  \n",
            "     |  * `tf.float16`: 16-bit half-precision floating-point.\n",
            "     |  * `tf.float32`: 32-bit single-precision floating-point.\n",
            "     |  * `tf.float64`: 64-bit double-precision floating-point.\n",
            "     |  * `tf.bfloat16`: 16-bit truncated floating-point.\n",
            "     |  * `tf.complex64`: 64-bit single-precision complex.\n",
            "     |  * `tf.complex128`: 128-bit double-precision complex.\n",
            "     |  * `tf.int8`: 8-bit signed integer.\n",
            "     |  * `tf.uint8`: 8-bit unsigned integer.\n",
            "     |  * `tf.uint16`: 16-bit unsigned integer.\n",
            "     |  * `tf.uint32`: 32-bit unsigned integer.\n",
            "     |  * `tf.uint64`: 64-bit unsigned integer.\n",
            "     |  * `tf.int16`: 16-bit signed integer.\n",
            "     |  * `tf.int32`: 32-bit signed integer.\n",
            "     |  * `tf.int64`: 64-bit signed integer.\n",
            "     |  * `tf.bool`: Boolean.\n",
            "     |  * `tf.string`: String.\n",
            "     |  * `tf.qint8`: Quantized 8-bit signed integer.\n",
            "     |  * `tf.quint8`: Quantized 8-bit unsigned integer.\n",
            "     |  * `tf.qint16`: Quantized 16-bit signed integer.\n",
            "     |  * `tf.quint16`: Quantized 16-bit unsigned integer.\n",
            "     |  * `tf.qint32`: Quantized 32-bit signed integer.\n",
            "     |  * `tf.resource`: Handle to a mutable resource.\n",
            "     |  * `tf.variant`: Values of arbitrary types.\n",
            "     |  \n",
            "     |  The `tf.as_dtype()` function converts numpy types and string type\n",
            "     |  names to a `DType` object.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      DType\n",
            "     |      tensorflow.python._dtypes.DType\n",
            "     |      pybind11_builtins.pybind11_object\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Returns True iff this DType refers to the same type as `other`.\n",
            "     |  \n",
            "     |  __hash__(...)\n",
            "     |      __hash__(self: tensorflow.python._dtypes.DType) -> int\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Returns True iff self != other.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  is_compatible_with(self, other)\n",
            "     |      Returns True if the `other` DType will be converted to this DType.\n",
            "     |      \n",
            "     |      The conversion rules are as follows:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      DType(T)       .is_compatible_with(DType(T))        == True\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `DType` (or object that may be converted to a `DType`).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        True if a Tensor of the `other` `DType` will be implicitly converted to\n",
            "     |        this `DType`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  as_numpy_dtype\n",
            "     |      Returns a Python `type` object based on this `DType`.\n",
            "     |  \n",
            "     |  base_dtype\n",
            "     |      Returns a non-reference `DType` based on this `DType`.\n",
            "     |  \n",
            "     |  limits\n",
            "     |      Return intensity limits, i.e.\n",
            "     |      \n",
            "     |      (min, max) tuple, of the dtype.\n",
            "     |      Args:\n",
            "     |        clip_negative : bool, optional If True, clip the negative range (i.e.\n",
            "     |          return 0 for min intensity) even if the image dtype allows negative\n",
            "     |          values. Returns\n",
            "     |        min, max : tuple Lower and upper intensity limits.\n",
            "     |  \n",
            "     |  max\n",
            "     |      Returns the maximum representable value in this data type.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if this is a non-numeric, unordered, or quantized type.\n",
            "     |  \n",
            "     |  min\n",
            "     |      Returns the minimum representable value in this data type.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if this is a non-numeric, unordered, or quantized type.\n",
            "     |  \n",
            "     |  real_dtype\n",
            "     |      Returns the `DType` corresponding to this `DType`'s real part.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python._dtypes.DType:\n",
            "     |  \n",
            "     |  __init__(...)\n",
            "     |      __init__(self: tensorflow.python._dtypes.DType, arg0: object) -> None\n",
            "     |  \n",
            "     |  __repr__(...)\n",
            "     |      __repr__(self: tensorflow.python._dtypes.DType) -> str\n",
            "     |  \n",
            "     |  __str__(...)\n",
            "     |      __str__(self: tensorflow.python._dtypes.DType) -> str\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python._dtypes.DType:\n",
            "     |  \n",
            "     |  as_datatype_enum\n",
            "     |      Returns a `types_pb2.DataType` enum value based on this data type.\n",
            "     |  \n",
            "     |  is_bool\n",
            "     |      Returns whether this is a boolean data type.\n",
            "     |  \n",
            "     |  is_complex\n",
            "     |      Returns whether this is a complex floating point type.\n",
            "     |  \n",
            "     |  is_floating\n",
            "     |      Returns whether this is a (non-quantized, real) floating point type.\n",
            "     |  \n",
            "     |  is_integer\n",
            "     |      Returns whether this is a (non-quantized) integer type.\n",
            "     |  \n",
            "     |  is_numpy_compatible\n",
            "     |      Returns whether this data type has a compatible NumPy data type.\n",
            "     |  \n",
            "     |  is_quantized\n",
            "     |      Returns whether this is a quantized data type.\n",
            "     |  \n",
            "     |  is_unsigned\n",
            "     |      Returns whether this type is unsigned.\n",
            "     |      \n",
            "     |      Non-numeric, unordered, and quantized types are not considered unsigned, and\n",
            "     |      this function returns `False`.\n",
            "     |  \n",
            "     |  name\n",
            "     |  \n",
            "     |  size\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from pybind11_builtins.pybind11_object:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) from pybind11_builtins.pybind11_type\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "    \n",
            "    DeviceSpec = class DeviceSpecV2(builtins.object)\n",
            "     |  Represents a (possibly partial) specification for a TensorFlow device.\n",
            "     |  \n",
            "     |  `DeviceSpec`s are used throughout TensorFlow to describe where state is stored\n",
            "     |  and computations occur. Using `DeviceSpec` allows you to parse device spec\n",
            "     |  strings to verify their validity, merge them or compose them programmatically.\n",
            "     |  \n",
            "     |  Example:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  # Place the operations on device \"GPU:0\" in the \"ps\" job.\n",
            "     |  device_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n",
            "     |  with tf.device(device_spec.to_string()):\n",
            "     |    # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n",
            "     |    my_var = tf.Variable(..., name=\"my_variable\")\n",
            "     |    squared_var = tf.square(my_var)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  With eager execution disabled (by default in TensorFlow 1.x and by calling\n",
            "     |  disable_eager_execution() in TensorFlow 2.x), the following syntax\n",
            "     |  can be used:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  tf.compat.v1.disable_eager_execution()\n",
            "     |  \n",
            "     |  # Same as previous\n",
            "     |  device_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n",
            "     |  # No need of .to_string() method.\n",
            "     |  with tf.device(device_spec):\n",
            "     |    my_var = tf.Variable(..., name=\"my_variable\")\n",
            "     |    squared_var = tf.square(my_var)\n",
            "     |   ```\n",
            "     |  \n",
            "     |  If a `DeviceSpec` is partially specified, it will be merged with other\n",
            "     |  `DeviceSpec`s according to the scope in which it is defined. `DeviceSpec`\n",
            "     |  components defined in inner scopes take precedence over those defined in\n",
            "     |  outer scopes.\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  gpu0_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n",
            "     |  with tf.device(DeviceSpec(job=\"train\").to_string()):\n",
            "     |    with tf.device(gpu0_spec.to_string()):\n",
            "     |      # Nodes created here will be assigned to /job:ps/device:GPU:0.\n",
            "     |    with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1).to_string()):\n",
            "     |      # Nodes created here will be assigned to /job:train/device:GPU:1.\n",
            "     |  ```\n",
            "     |  \n",
            "     |  A `DeviceSpec` consists of 5 components -- each of\n",
            "     |  which is optionally specified:\n",
            "     |  \n",
            "     |  * Job: The job name.\n",
            "     |  * Replica: The replica index.\n",
            "     |  * Task: The task index.\n",
            "     |  * Device type: The device type string (e.g. \"CPU\" or \"GPU\").\n",
            "     |  * Device index: The device index.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Checks if the `other` DeviceSpec is same as the current instance, eg have\n",
            "     |      \n",
            "     |         same value for all the internal fields.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another DeviceSpec\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        Return `True` if `other` is also a DeviceSpec instance and has same value\n",
            "     |        as the current instance.\n",
            "     |        Return `False` otherwise.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __init__(self, job=None, replica=None, task=None, device_type=None, device_index=None)\n",
            "     |      Create a new `DeviceSpec` object.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        job: string.  Optional job name.\n",
            "     |        replica: int.  Optional replica index.\n",
            "     |        task: int.  Optional task index.\n",
            "     |        device_type: Optional device type string (e.g. \"CPU\" or \"GPU\")\n",
            "     |        device_index: int.  Optional device index.  If left\n",
            "     |          unspecified, device represents 'any' device_index.\n",
            "     |  \n",
            "     |  make_merged_spec(self, dev)\n",
            "     |      Returns a new DeviceSpec which incorporates `dev`.\n",
            "     |      \n",
            "     |      When combining specs, `dev` will take precedence over the current spec.\n",
            "     |      So for instance:\n",
            "     |      ```\n",
            "     |      first_spec = tf.DeviceSpec(job=0, device_type=\"CPU\")\n",
            "     |      second_spec = tf.DeviceSpec(device_type=\"GPU\")\n",
            "     |      combined_spec = first_spec.make_merged_spec(second_spec)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      is equivalent to:\n",
            "     |      ```\n",
            "     |      combined_spec = tf.DeviceSpec(job=0, device_type=\"GPU\")\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        dev: a `DeviceSpec`\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A new `DeviceSpec` which combines `self` and `dev`\n",
            "     |  \n",
            "     |  parse_from_string(self, spec)\n",
            "     |      Parse a `DeviceSpec` name into its components.\n",
            "     |      \n",
            "     |      2.x behavior change:\n",
            "     |        In TensorFlow 1.x, this function mutates its own state and returns itself.\n",
            "     |        In 2.x, DeviceSpecs are immutable, and this function will return a\n",
            "     |          DeviceSpec which contains the spec.\n",
            "     |      \n",
            "     |        Recommended:\n",
            "     |          ```\n",
            "     |          # my_spec and my_updated_spec are unrelated.\n",
            "     |          my_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\n",
            "     |          my_updated_spec = tf.DeviceSpec.from_string(\"/GPU:0\")\n",
            "     |          with tf.device(my_updated_spec):\n",
            "     |            ...\n",
            "     |          ```\n",
            "     |      \n",
            "     |        Will work in 1.x and 2.x (though deprecated in 2.x):\n",
            "     |          ```\n",
            "     |          my_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\n",
            "     |          my_updated_spec = my_spec.parse_from_string(\"/GPU:0\")\n",
            "     |          with tf.device(my_updated_spec):\n",
            "     |            ...\n",
            "     |          ```\n",
            "     |      \n",
            "     |        Will NOT work in 2.x:\n",
            "     |          ```\n",
            "     |          my_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\n",
            "     |          my_spec.parse_from_string(\"/GPU:0\")  # <== Will not update my_spec\n",
            "     |          with tf.device(my_spec):\n",
            "     |            ...\n",
            "     |          ```\n",
            "     |      \n",
            "     |        In general, `DeviceSpec.from_string` should completely replace\n",
            "     |        `DeviceSpec.parse_from_string`, and `DeviceSpec.replace` should\n",
            "     |        completely replace setting attributes directly.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        spec: an optional string of the form\n",
            "     |         /job:<name>/replica:<id>/task:<id>/device:CPU:<id>\n",
            "     |        or\n",
            "     |         /job:<name>/replica:<id>/task:<id>/device:GPU:<id>\n",
            "     |        as cpu and gpu are mutually exclusive.\n",
            "     |        All entries are optional.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The `DeviceSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: if the spec was not valid.\n",
            "     |  \n",
            "     |  replace(self, **kwargs)\n",
            "     |      Convenience method for making a new DeviceSpec by overriding fields.\n",
            "     |      \n",
            "     |      For instance:\n",
            "     |      ```\n",
            "     |      my_spec = DeviceSpec=(job=\"my_job\", device=\"CPU\")\n",
            "     |      my_updated_spec = my_spec.replace(device=\"GPU\")\n",
            "     |      my_other_spec = my_spec.replace(device=None)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        **kwargs: This method takes the same args as the DeviceSpec constructor\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A DeviceSpec with the fields specified in kwargs overridden.\n",
            "     |  \n",
            "     |  to_string(self)\n",
            "     |      Return a string representation of this `DeviceSpec`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        a string of the form\n",
            "     |        /job:<name>/replica:<id>/task:<id>/device:<device_type>:<id>.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_string(spec) from builtins.type\n",
            "     |      Construct a `DeviceSpec` from a string.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        spec: a string of the form\n",
            "     |         /job:<name>/replica:<id>/task:<id>/device:CPU:<id>\n",
            "     |        or\n",
            "     |         /job:<name>/replica:<id>/task:<id>/device:GPU:<id>\n",
            "     |        as cpu and gpu are mutually exclusive.\n",
            "     |        All entries are optional.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A DeviceSpec.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  device_index\n",
            "     |  \n",
            "     |  device_type\n",
            "     |  \n",
            "     |  job\n",
            "     |  \n",
            "     |  replica\n",
            "     |  \n",
            "     |  task\n",
            "    \n",
            "    class GradientTape(builtins.object)\n",
            "     |  Record operations for automatic differentiation.\n",
            "     |  \n",
            "     |  Operations are recorded if they are executed within this context manager and\n",
            "     |  at least one of their inputs is being \"watched\".\n",
            "     |  \n",
            "     |  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
            "     |  where `trainable=True` is default in both cases) are automatically watched.\n",
            "     |  Tensors can be manually watched by invoking the `watch` method on this context\n",
            "     |  manager.\n",
            "     |  \n",
            "     |  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n",
            "     |  be computed as:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  x = tf.constant(3.0)\n",
            "     |  with tf.GradientTape() as g:\n",
            "     |    g.watch(x)\n",
            "     |    y = x * x\n",
            "     |  dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
            "     |  ```\n",
            "     |  \n",
            "     |  GradientTapes can be nested to compute higher-order derivatives. For example,\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  x = tf.constant(3.0)\n",
            "     |  with tf.GradientTape() as g:\n",
            "     |    g.watch(x)\n",
            "     |    with tf.GradientTape() as gg:\n",
            "     |      gg.watch(x)\n",
            "     |      y = x * x\n",
            "     |    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
            "     |  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n",
            "     |  ```\n",
            "     |  \n",
            "     |  By default, the resources held by a GradientTape are released as soon as\n",
            "     |  GradientTape.gradient() method is called. To compute multiple gradients over\n",
            "     |  the same computation, create a persistent gradient tape. This allows multiple\n",
            "     |  calls to the gradient() method as resources are released when the tape object\n",
            "     |  is garbage collected. For example:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  x = tf.constant(3.0)\n",
            "     |  with tf.GradientTape(persistent=True) as g:\n",
            "     |    g.watch(x)\n",
            "     |    y = x * x\n",
            "     |    z = y * y\n",
            "     |  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
            "     |  dy_dx = g.gradient(y, x)  # 6.0\n",
            "     |  del g  # Drop the reference to the tape\n",
            "     |  ```\n",
            "     |  \n",
            "     |  By default GradientTape will automatically watch any trainable variables that\n",
            "     |  are accessed inside the context. If you want fine grained control over which\n",
            "     |  variables are watched you can disable automatic tracking by passing\n",
            "     |  `watch_accessed_variables=False` to the tape constructor:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
            "     |    tape.watch(variable_a)\n",
            "     |    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n",
            "     |    z = variable_b ** 3  # No gradients will be available since `variable_b` is\n",
            "     |                         # not being watched.\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Note that when using models you should ensure that your variables exist when\n",
            "     |  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n",
            "     |  first iteration not have any gradients:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  a = tf.keras.layers.Dense(32)\n",
            "     |  b = tf.keras.layers.Dense(32)\n",
            "     |  \n",
            "     |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
            "     |    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
            "     |                             # `a.variables` will return an empty list and the\n",
            "     |                             # tape will not be watching anything.\n",
            "     |    result = b(a(inputs))\n",
            "     |    tape.gradient(result, a.variables)  # The result of this computation will be\n",
            "     |                                        # a list of `None`s since a's variables\n",
            "     |                                        # are not being watched.\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Note that only tensors with real or complex dtypes are differentiable.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __del__(self)\n",
            "     |  \n",
            "     |  __enter__(self)\n",
            "     |      Enters a context inside which operations are recorded on this tape.\n",
            "     |  \n",
            "     |  __exit__(self, typ, value, traceback)\n",
            "     |      Exits the recording context, no further operations are traced.\n",
            "     |  \n",
            "     |  __init__(self, persistent=False, watch_accessed_variables=True)\n",
            "     |      Creates a new GradientTape.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        persistent: Boolean controlling whether a persistent gradient tape\n",
            "     |          is created. False by default, which means at most one call can\n",
            "     |          be made to the gradient() method on this object.\n",
            "     |        watch_accessed_variables: Boolean controlling whether the tape will\n",
            "     |          automatically `watch` any (trainable) variables accessed while the tape\n",
            "     |          is active. Defaults to True meaning gradients can be requested from any\n",
            "     |          result computed in the tape derived from reading a trainable `Variable`.\n",
            "     |          If False users must explicitly `watch` any `Variable`s they want to\n",
            "     |          request gradients from.\n",
            "     |  \n",
            "     |  batch_jacobian(self, target, source, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n",
            "     |      Computes and stacks per-example jacobians.\n",
            "     |      \n",
            "     |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n",
            "     |      definition of a Jacobian. This function is essentially an efficient\n",
            "     |      implementation of the following:\n",
            "     |      \n",
            "     |      `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\n",
            "     |      \n",
            "     |      Note that compared to `GradientTape.jacobian` which computes gradient of\n",
            "     |      each output value w.r.t each input value, this function is useful when\n",
            "     |      `target[i,...]` is independent of `source[j,...]` for `j != i`. This\n",
            "     |      assumption allows more efficient computation as compared to\n",
            "     |      `GradientTape.jacobian`. The output, as well as intermediate activations,\n",
            "     |      are lower dimensional and avoid a bunch of redundant zeros which would\n",
            "     |      result in the jacobian computation given the independence assumption.\n",
            "     |      \n",
            "     |      Example usage:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with tf.GradientTape() as g:\n",
            "     |        x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n",
            "     |        g.watch(x)\n",
            "     |        y = x * x\n",
            "     |      batch_jacobian = g.batch_jacobian(y, x)\n",
            "     |      # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].\n",
            "     |          `target[i,...]` should only depend on `source[i,...]`.\n",
            "     |        source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\n",
            "     |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
            "     |          alters the value which will be returned if the target and sources are\n",
            "     |          unconnected. The possible values and effects are detailed in\n",
            "     |          'UnconnectedGradients' and it defaults to 'none'.\n",
            "     |        parallel_iterations: A knob to control how many iterations are dispatched\n",
            "     |          in parallel. This knob can be used to control the total memory usage.\n",
            "     |        experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else\n",
            "     |          uses a tf.while_loop.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`\n",
            "     |        is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked\n",
            "     |        per-example jacobians.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        RuntimeError: If called on a non-persistent tape with eager execution\n",
            "     |          enabled and without enabling experimental_use_pfor.\n",
            "     |        ValueError: If vectorization of jacobian computation fails or if first\n",
            "     |          dimension of `target` and `source` do not match.\n",
            "     |  \n",
            "     |  gradient(self, target, sources, output_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n",
            "     |      Computes the gradient using operations recorded in context of this tape.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        target: a list or nested structure of Tensors or Variables to be\n",
            "     |          differentiated.\n",
            "     |        sources: a list or nested structure of Tensors or Variables. `target`\n",
            "     |          will be differentiated against elements in `sources`.\n",
            "     |        output_gradients: a list of gradients, one for each element of\n",
            "     |          target. Defaults to None.\n",
            "     |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
            "     |          alters the value which will be returned if the target and sources are\n",
            "     |          unconnected. The possible values and effects are detailed in\n",
            "     |          'UnconnectedGradients' and it defaults to 'none'.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        a list or nested structure of Tensors (or IndexedSlices, or None),\n",
            "     |        one for each element in `sources`. Returned structure is the same as\n",
            "     |        the structure of `sources`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        RuntimeError: if called inside the context of the tape, or if called more\n",
            "     |         than once on a non-persistent tape.\n",
            "     |        ValueError: if the target is a variable or if unconnected gradients is\n",
            "     |         called with an unknown value.\n",
            "     |  \n",
            "     |  jacobian(self, target, sources, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n",
            "     |      Computes the jacobian using operations recorded in context of this tape.\n",
            "     |      \n",
            "     |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n",
            "     |      definition of a Jacobian.\n",
            "     |      \n",
            "     |      Example usage:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with tf.GradientTape() as g:\n",
            "     |        x  = tf.constant([1.0, 2.0])\n",
            "     |        g.watch(x)\n",
            "     |        y = x * x\n",
            "     |      jacobian = g.jacobian(y, x)\n",
            "     |      # jacobian value is [[2., 0.], [0., 4.]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        target: Tensor to be differentiated.\n",
            "     |        sources: a list or nested structure of Tensors or Variables. `target`\n",
            "     |          will be differentiated against elements in `sources`.\n",
            "     |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
            "     |          alters the value which will be returned if the target and sources are\n",
            "     |          unconnected. The possible values and effects are detailed in\n",
            "     |          'UnconnectedGradients' and it defaults to 'none'.\n",
            "     |        parallel_iterations: A knob to control how many iterations are dispatched\n",
            "     |          in parallel. This knob can be used to control the total memory usage.\n",
            "     |        experimental_use_pfor: If true, vectorizes the jacobian computation. Else\n",
            "     |          falls back to a sequential while_loop. Vectorization can sometimes fail\n",
            "     |          or lead to excessive memory usage. This option can be used to disable\n",
            "     |          vectorization in such cases.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list or nested structure of Tensors (or None), one for each element in\n",
            "     |        `sources`. Returned structure is the same as the structure of `sources`.\n",
            "     |        Note if any gradient is sparse (IndexedSlices), jacobian function\n",
            "     |        currently makes it dense and returns a Tensor instead. This may change in\n",
            "     |        the future.\n",
            "     |      \n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        RuntimeError: If called on a non-persistent tape with eager execution\n",
            "     |          enabled and without enabling experimental_use_pfor.\n",
            "     |        ValueError: If vectorization of jacobian computation fails.\n",
            "     |  \n",
            "     |  reset(self)\n",
            "     |      Clears all information stored in this tape.\n",
            "     |      \n",
            "     |      Equivalent to exiting and reentering the tape context manager with a new\n",
            "     |      tape. For example, the two following code blocks are equivalent:\n",
            "     |      \n",
            "     |      ```\n",
            "     |      with tf.GradientTape() as t:\n",
            "     |        loss = loss_fn()\n",
            "     |      with tf.GradientTape() as t:\n",
            "     |        loss += other_loss_fn()\n",
            "     |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
            "     |      \n",
            "     |      \n",
            "     |      # The following is equivalent to the above\n",
            "     |      with tf.GradientTape() as t:\n",
            "     |        loss = loss_fn()\n",
            "     |        t.reset()\n",
            "     |        loss += other_loss_fn()\n",
            "     |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n",
            "     |      ```\n",
            "     |      \n",
            "     |      This is useful if you don't want to exit the context manager for the tape,\n",
            "     |      or can't because the desired reset point is inside a control flow construct:\n",
            "     |      \n",
            "     |      ```\n",
            "     |      with tf.GradientTape() as t:\n",
            "     |        loss = ...\n",
            "     |        if loss > k:\n",
            "     |          t.reset()\n",
            "     |      ```\n",
            "     |  \n",
            "     |  stop_recording(self)\n",
            "     |      Temporarily stops recording operations on this tape.\n",
            "     |      \n",
            "     |      Operations executed while this context manager is active will not be\n",
            "     |      recorded on the tape. This is useful for reducing the memory used by tracing\n",
            "     |      all computations.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```\n",
            "     |        with tf.GradientTape(persistent=True) as t:\n",
            "     |          loss = compute_loss(model)\n",
            "     |          with t.stop_recording():\n",
            "     |            # The gradient computation below is not traced, saving memory.\n",
            "     |            grads = t.gradient(loss, model.variables)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Yields:\n",
            "     |        None\n",
            "     |      Raises:\n",
            "     |        RuntimeError: if the tape is not currently recording.\n",
            "     |  \n",
            "     |  watch(self, tensor)\n",
            "     |      Ensures that `tensor` is being traced by this tape.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        tensor: a Tensor or list of Tensors.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: if it encounters something that is not a tensor.\n",
            "     |  \n",
            "     |  watched_variables(self)\n",
            "     |      Returns variables watched by this tape in order of construction.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class Graph(builtins.object)\n",
            "     |  A TensorFlow computation, represented as a dataflow graph.\n",
            "     |  \n",
            "     |  Graphs are used by `tf.function`s to represent the function's computations.\n",
            "     |  Each graph contains a set of `tf.Operation` objects, which represent units of\n",
            "     |  computation; and `tf.Tensor` objects, which represent the units of data that\n",
            "     |  flow between operations.\n",
            "     |  \n",
            "     |  ### Using graphs directly (deprecated)\n",
            "     |  \n",
            "     |  A `tf.Graph` can be constructed and used directly without a `tf.function`, as\n",
            "     |  was required in TensorFlow 1, but this is deprecated and it is recommended to\n",
            "     |  use a `tf.function` instead. If a graph is directly used, other deprecated\n",
            "     |  TensorFlow 1 classes are also required to execute the graph, such as a\n",
            "     |  `tf.compat.v1.Session`.\n",
            "     |  \n",
            "     |  A default graph can be registered with the `tf.Graph.as_default` context\n",
            "     |  manager. Then, operations will be added to the graph instead of being executed\n",
            "     |  eagerly. For example:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  g = tf.Graph()\n",
            "     |  with g.as_default():\n",
            "     |    # Define operations and tensors in `g`.\n",
            "     |    c = tf.constant(30.0)\n",
            "     |    assert c.graph is g\n",
            "     |  ```\n",
            "     |  \n",
            "     |  `tf.compat.v1.get_default_graph()` can be used to obtain the default graph.\n",
            "     |  \n",
            "     |  Important note: This class *is not* thread-safe for graph construction. All\n",
            "     |  operations should be created from a single thread, or external\n",
            "     |  synchronization must be provided. Unless otherwise specified, all methods\n",
            "     |  are not thread-safe.\n",
            "     |  \n",
            "     |  A `Graph` instance supports an arbitrary number of \"collections\"\n",
            "     |  that are identified by name. For convenience when building a large\n",
            "     |  graph, collections can store groups of related objects: for\n",
            "     |  example, the `tf.Variable` uses a collection (named\n",
            "     |  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n",
            "     |  all variables that are created during the construction of a graph. The caller\n",
            "     |  may define additional collections by specifying a new name.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self)\n",
            "     |      Creates a new, empty Graph.\n",
            "     |  \n",
            "     |  add_to_collection(self, name, value)\n",
            "     |      Stores `value` in the collection with the given `name`.\n",
            "     |      \n",
            "     |      Note that collections are not sets, so it is possible to add a value to\n",
            "     |      a collection several times.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The key for the collection. The `GraphKeys` class contains many\n",
            "     |          standard names for collections.\n",
            "     |        value: The value to add to the collection.\n",
            "     |  \n",
            "     |  add_to_collections(self, names, value)\n",
            "     |      Stores `value` in the collections given by `names`.\n",
            "     |      \n",
            "     |      Note that collections are not sets, so it is possible to add a value to\n",
            "     |      a collection several times. This function makes sure that duplicates in\n",
            "     |      `names` are ignored, but it will not check for pre-existing membership of\n",
            "     |      `value` in any of the collections in `names`.\n",
            "     |      \n",
            "     |      `names` can be any iterable, but if `names` is a string, it is treated as a\n",
            "     |      single collection name.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        names: The keys for the collections to add to. The `GraphKeys` class\n",
            "     |          contains many standard names for collections.\n",
            "     |        value: The value to add to the collections.\n",
            "     |  \n",
            "     |  as_default(self)\n",
            "     |      Returns a context manager that makes this `Graph` the default graph.\n",
            "     |      \n",
            "     |      This method should be used if you want to create multiple graphs\n",
            "     |      in the same process. For convenience, a global default graph is\n",
            "     |      provided, and all ops will be added to this graph if you do not\n",
            "     |      create a new graph explicitly.\n",
            "     |      \n",
            "     |      Use this method with the `with` keyword to specify that ops created within\n",
            "     |      the scope of a block should be added to this graph. In this case, once\n",
            "     |      the scope of the `with` is exited, the previous default graph is set again\n",
            "     |      as default. There is a stack, so it's ok to have multiple nested levels\n",
            "     |      of `as_default` calls.\n",
            "     |      \n",
            "     |      The default graph is a property of the current thread. If you\n",
            "     |      create a new thread, and wish to use the default graph in that\n",
            "     |      thread, you must explicitly add a `with g.as_default():` in that\n",
            "     |      thread's function.\n",
            "     |      \n",
            "     |      The following code examples are equivalent:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      # 1. Using Graph.as_default():\n",
            "     |      g = tf.Graph()\n",
            "     |      with g.as_default():\n",
            "     |        c = tf.constant(5.0)\n",
            "     |        assert c.graph is g\n",
            "     |      \n",
            "     |      # 2. Constructing and making default:\n",
            "     |      with tf.Graph().as_default() as g:\n",
            "     |        c = tf.constant(5.0)\n",
            "     |        assert c.graph is g\n",
            "     |      ```\n",
            "     |      \n",
            "     |      If eager execution is enabled ops created under this context manager will be\n",
            "     |      added to the graph instead of executed eagerly.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A context manager for using this graph as the default graph.\n",
            "     |  \n",
            "     |  as_graph_def(self, from_version=None, add_shapes=False)\n",
            "     |      Returns a serialized `GraphDef` representation of this graph.\n",
            "     |      \n",
            "     |      The serialized `GraphDef` can be imported into another `Graph`\n",
            "     |      (using `tf.import_graph_def`) or used with the\n",
            "     |      [C++ Session API](../../api_docs/cc/index.md).\n",
            "     |      \n",
            "     |      This method is thread-safe.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        from_version: Optional.  If this is set, returns a `GraphDef` containing\n",
            "     |          only the nodes that were added to this graph since its `version`\n",
            "     |          property had the given value.\n",
            "     |        add_shapes: If true, adds an \"_output_shapes\" list attr to each node with\n",
            "     |          the inferred shapes of each of its outputs.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A\n",
            "     |        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
            "     |        protocol buffer.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If the `graph_def` would be too large.\n",
            "     |  \n",
            "     |  as_graph_element(self, obj, allow_tensor=True, allow_operation=True)\n",
            "     |      Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n",
            "     |      \n",
            "     |      This function validates that `obj` represents an element of this\n",
            "     |      graph, and gives an informative error message if it is not.\n",
            "     |      \n",
            "     |      This function is the canonical way to get/validate an object of\n",
            "     |      one of the allowed types from an external argument reference in the\n",
            "     |      Session API.\n",
            "     |      \n",
            "     |      This method may be called concurrently from multiple threads.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        obj: A `Tensor`, an `Operation`, or the name of a tensor or operation. Can\n",
            "     |          also be any object with an `_as_graph_element()` method that returns a\n",
            "     |          value of one of these types. Note: `_as_graph_element` will be called\n",
            "     |          inside the graph's lock and so may not modify the graph.\n",
            "     |        allow_tensor: If true, `obj` may refer to a `Tensor`.\n",
            "     |        allow_operation: If true, `obj` may refer to an `Operation`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `obj` is not a type we support attempting to convert\n",
            "     |          to types.\n",
            "     |        ValueError: If `obj` is of an appropriate type but invalid. For\n",
            "     |          example, an invalid string.\n",
            "     |        KeyError: If `obj` is not an object in the graph.\n",
            "     |  \n",
            "     |  clear_collection(self, name)\n",
            "     |      Clears all values in a collection.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The key for the collection. The `GraphKeys` class contains many\n",
            "     |          standard names for collections.\n",
            "     |  \n",
            "     |  colocate_with(self, op, ignore_existing=False)\n",
            "     |      Returns a context manager that specifies an op to colocate with.\n",
            "     |      \n",
            "     |      Note: this function is not for public use, only for internal libraries.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      a = tf.Variable([1.0])\n",
            "     |      with g.colocate_with(a):\n",
            "     |        b = tf.constant(1.0)\n",
            "     |        c = tf.add(a, b)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      `b` and `c` will always be colocated with `a`, no matter where `a`\n",
            "     |      is eventually placed.\n",
            "     |      \n",
            "     |      **NOTE** Using a colocation scope resets any existing device constraints.\n",
            "     |      \n",
            "     |      If `op` is `None` then `ignore_existing` must be `True` and the new\n",
            "     |      scope resets all colocation and device constraints.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        op: The op to colocate all created ops with, or `None`.\n",
            "     |        ignore_existing: If true, only applies colocation of this op within the\n",
            "     |          context, rather than applying all colocation properties on the stack.\n",
            "     |          If `op` is `None`, this value must be `True`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: if op is None but ignore_existing is False.\n",
            "     |      \n",
            "     |      Yields:\n",
            "     |        A context manager that specifies the op with which to colocate\n",
            "     |        newly created ops.\n",
            "     |  \n",
            "     |  container(self, container_name)\n",
            "     |      Returns a context manager that specifies the resource container to use.\n",
            "     |      \n",
            "     |      Stateful operations, such as variables and queues, can maintain their\n",
            "     |      states on devices so that they can be shared by multiple processes.\n",
            "     |      A resource container is a string name under which these stateful\n",
            "     |      operations are tracked. These resources can be released or cleared\n",
            "     |      with `tf.Session.reset()`.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with g.container('experiment0'):\n",
            "     |        # All stateful Operations constructed in this context will be placed\n",
            "     |        # in resource container \"experiment0\".\n",
            "     |        v1 = tf.Variable([1.0])\n",
            "     |        v2 = tf.Variable([2.0])\n",
            "     |        with g.container(\"experiment1\"):\n",
            "     |          # All stateful Operations constructed in this context will be\n",
            "     |          # placed in resource container \"experiment1\".\n",
            "     |          v3 = tf.Variable([3.0])\n",
            "     |          q1 = tf.queue.FIFOQueue(10, tf.float32)\n",
            "     |        # All stateful Operations constructed in this context will be\n",
            "     |        # be created in the \"experiment0\".\n",
            "     |        v4 = tf.Variable([4.0])\n",
            "     |        q1 = tf.queue.FIFOQueue(20, tf.float32)\n",
            "     |        with g.container(\"\"):\n",
            "     |          # All stateful Operations constructed in this context will be\n",
            "     |          # be placed in the default resource container.\n",
            "     |          v5 = tf.Variable([5.0])\n",
            "     |          q3 = tf.queue.FIFOQueue(30, tf.float32)\n",
            "     |      \n",
            "     |      # Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n",
            "     |      # will become undefined (such as uninitialized).\n",
            "     |      tf.Session.reset(target, [\"experiment0\"])\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        container_name: container name string.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A context manager for defining resource containers for stateful ops,\n",
            "     |          yields the container name.\n",
            "     |  \n",
            "     |  control_dependencies(self, control_inputs)\n",
            "     |      Returns a context manager that specifies control dependencies.\n",
            "     |      \n",
            "     |      Use with the `with` keyword to specify that all operations constructed\n",
            "     |      within the context should have control dependencies on\n",
            "     |      `control_inputs`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with g.control_dependencies([a, b, c]):\n",
            "     |        # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
            "     |        d = ...\n",
            "     |        e = ...\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Multiple calls to `control_dependencies()` can be nested, and in\n",
            "     |      that case a new `Operation` will have control dependencies on the union\n",
            "     |      of `control_inputs` from all active contexts.\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with g.control_dependencies([a, b]):\n",
            "     |        # Ops constructed here run after `a` and `b`.\n",
            "     |        with g.control_dependencies([c, d]):\n",
            "     |          # Ops constructed here run after `a`, `b`, `c`, and `d`.\n",
            "     |      ```\n",
            "     |      \n",
            "     |      You can pass None to clear the control dependencies:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with g.control_dependencies([a, b]):\n",
            "     |        # Ops constructed here run after `a` and `b`.\n",
            "     |        with g.control_dependencies(None):\n",
            "     |          # Ops constructed here run normally, not waiting for either `a` or `b`.\n",
            "     |          with g.control_dependencies([c, d]):\n",
            "     |            # Ops constructed here run after `c` and `d`, also not waiting\n",
            "     |            # for either `a` or `b`.\n",
            "     |      ```\n",
            "     |      \n",
            "     |      *N.B.* The control dependencies context applies *only* to ops that\n",
            "     |      are constructed within the context. Merely using an op or tensor\n",
            "     |      in the context does not add a control dependency. The following\n",
            "     |      example illustrates this point:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      # WRONG\n",
            "     |      def my_func(pred, tensor):\n",
            "     |        t = tf.matmul(tensor, tensor)\n",
            "     |        with tf.control_dependencies([pred]):\n",
            "     |          # The matmul op is created outside the context, so no control\n",
            "     |          # dependency will be added.\n",
            "     |          return t\n",
            "     |      \n",
            "     |      # RIGHT\n",
            "     |      def my_func(pred, tensor):\n",
            "     |        with tf.control_dependencies([pred]):\n",
            "     |          # The matmul op is created in the context, so a control dependency\n",
            "     |          # will be added.\n",
            "     |          return tf.matmul(tensor, tensor)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Also note that though execution of ops created under this scope will trigger\n",
            "     |      execution of the dependencies, the ops created under this scope might still\n",
            "     |      be pruned from a normal tensorflow graph. For example, in the following\n",
            "     |      snippet of code the dependencies are never executed:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |        loss = model.loss()\n",
            "     |        with tf.control_dependencies(dependencies):\n",
            "     |          loss = loss + tf.constant(1)  # note: dependencies ignored in the\n",
            "     |                                        # backward pass\n",
            "     |        return tf.gradients(loss, model.variables)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      This is because evaluating the gradient graph does not require evaluating\n",
            "     |      the constant(1) op created in the forward pass.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        control_inputs: A list of `Operation` or `Tensor` objects which must be\n",
            "     |          executed or computed before running the operations defined in the\n",
            "     |          context.  Can also be `None` to clear the control dependencies.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |       A context manager that specifies control dependencies for all\n",
            "     |       operations constructed within the context.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `control_inputs` is not a list of `Operation` or\n",
            "     |          `Tensor` objects.\n",
            "     |  \n",
            "     |  create_op(self, op_type, inputs, dtypes=None, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)\n",
            "     |      Creates an `Operation` in this graph. (deprecated arguments)\n",
            "     |      \n",
            "     |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(compute_shapes)`. They will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
            "     |      \n",
            "     |      This is a low-level interface for creating an `Operation`. Most\n",
            "     |      programs will not call this method directly, and instead use the\n",
            "     |      Python op constructors, such as `tf.constant()`, which add ops to\n",
            "     |      the default graph.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        op_type: The `Operation` type to create. This corresponds to the\n",
            "     |          `OpDef.name` field for the proto that defines the operation.\n",
            "     |        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n",
            "     |        dtypes: (Optional) A list of `DType` objects that will be the types of the\n",
            "     |          tensors that the operation produces.\n",
            "     |        input_types: (Optional.) A list of `DType`s that will be the types of the\n",
            "     |          tensors that the operation consumes. By default, uses the base `DType`\n",
            "     |          of each input in `inputs`. Operations that expect reference-typed inputs\n",
            "     |          must specify `input_types` explicitly.\n",
            "     |        name: (Optional.) A string name for the operation. If not specified, a\n",
            "     |          name is generated based on `op_type`.\n",
            "     |        attrs: (Optional.) A dictionary where the key is the attribute name (a\n",
            "     |          string) and the value is the respective `attr` attribute of the\n",
            "     |          `NodeDef` proto that will represent the operation (an `AttrValue`\n",
            "     |          proto).\n",
            "     |        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n",
            "     |          the operation will have.\n",
            "     |        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n",
            "     |          computed).\n",
            "     |        compute_device: (Optional.) If True, device functions will be executed to\n",
            "     |          compute the device property of the Operation.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if any of the inputs is not a `Tensor`.\n",
            "     |        ValueError: if colocation conflicts with existing device assignment.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An `Operation` object.\n",
            "     |  \n",
            "     |  device(self, device_name_or_function)\n",
            "     |      Returns a context manager that specifies the default device to use.\n",
            "     |      \n",
            "     |      The `device_name_or_function` argument may either be a device name\n",
            "     |      string, a device function, or None:\n",
            "     |      \n",
            "     |      * If it is a device name string, all operations constructed in\n",
            "     |        this context will be assigned to the device with that name, unless\n",
            "     |        overridden by a nested `device()` context.\n",
            "     |      * If it is a function, it will be treated as a function from\n",
            "     |        Operation objects to device name strings, and invoked each time\n",
            "     |        a new Operation is created. The Operation will be assigned to\n",
            "     |        the device with the returned name.\n",
            "     |      * If it is None, all `device()` invocations from the enclosing context\n",
            "     |        will be ignored.\n",
            "     |      \n",
            "     |      For information about the valid syntax of device name strings, see\n",
            "     |      the documentation in\n",
            "     |      [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with g.device('/device:GPU:0'):\n",
            "     |        # All operations constructed in this context will be placed\n",
            "     |        # on GPU 0.\n",
            "     |        with g.device(None):\n",
            "     |          # All operations constructed in this context will have no\n",
            "     |          # assigned device.\n",
            "     |      \n",
            "     |      # Defines a function from `Operation` to device string.\n",
            "     |      def matmul_on_gpu(n):\n",
            "     |        if n.type == \"MatMul\":\n",
            "     |          return \"/device:GPU:0\"\n",
            "     |        else:\n",
            "     |          return \"/cpu:0\"\n",
            "     |      \n",
            "     |      with g.device(matmul_on_gpu):\n",
            "     |        # All operations of type \"MatMul\" constructed in this context\n",
            "     |        # will be placed on GPU 0; all other operations will be placed\n",
            "     |        # on CPU 0.\n",
            "     |      ```\n",
            "     |      \n",
            "     |      **N.B.** The device scope may be overridden by op wrappers or\n",
            "     |      other library code. For example, a variable assignment op\n",
            "     |      `v.assign()` must be colocated with the `tf.Variable` `v`, and\n",
            "     |      incompatible device scopes will be ignored.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        device_name_or_function: The device name or function to use in the\n",
            "     |          context.\n",
            "     |      \n",
            "     |      Yields:\n",
            "     |        A context manager that specifies the default device to use for newly\n",
            "     |        created ops.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        RuntimeError: If device scopes are not properly nested.\n",
            "     |  \n",
            "     |  finalize(self)\n",
            "     |      Finalizes this graph, making it read-only.\n",
            "     |      \n",
            "     |      After calling `g.finalize()`, no new operations can be added to\n",
            "     |      `g`.  This method is used to ensure that no operations are added\n",
            "     |      to a graph when it is shared between multiple threads, for example\n",
            "     |      when using a `tf.compat.v1.train.QueueRunner`.\n",
            "     |  \n",
            "     |  get_all_collection_keys(self)\n",
            "     |      Returns a list of collections used in this graph.\n",
            "     |  \n",
            "     |  get_collection(self, name, scope=None)\n",
            "     |      Returns a list of values in the collection with the given `name`.\n",
            "     |      \n",
            "     |      This is different from `get_collection_ref()` which always returns the\n",
            "     |      actual collection list if it exists in that it returns a new list each time\n",
            "     |      it is called.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The key for the collection. For example, the `GraphKeys` class\n",
            "     |          contains many standard names for collections.\n",
            "     |        scope: (Optional.) A string. If supplied, the resulting list is filtered\n",
            "     |          to include only items whose `name` attribute matches `scope` using\n",
            "     |          `re.match`. Items without a `name` attribute are never returned if a\n",
            "     |          scope is supplied. The choice of `re.match` means that a `scope` without\n",
            "     |          special tokens filters by prefix.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The list of values in the collection with the given `name`, or\n",
            "     |        an empty list if no value has been added to that collection. The\n",
            "     |        list contains the values in the order under which they were\n",
            "     |        collected.\n",
            "     |  \n",
            "     |  get_collection_ref(self, name)\n",
            "     |      Returns a list of values in the collection with the given `name`.\n",
            "     |      \n",
            "     |      If the collection exists, this returns the list itself, which can\n",
            "     |      be modified in place to change the collection.  If the collection does\n",
            "     |      not exist, it is created as an empty list and the list is returned.\n",
            "     |      \n",
            "     |      This is different from `get_collection()` which always returns a copy of\n",
            "     |      the collection list if it exists and never creates an empty collection.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The key for the collection. For example, the `GraphKeys` class\n",
            "     |          contains many standard names for collections.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The list of values in the collection with the given `name`, or an empty\n",
            "     |        list if no value has been added to that collection.\n",
            "     |  \n",
            "     |  get_name_scope(self)\n",
            "     |      Returns the current name scope.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with tf.name_scope('scope1'):\n",
            "     |        with tf.name_scope('scope2'):\n",
            "     |          print(tf.compat.v1.get_default_graph().get_name_scope())\n",
            "     |      ```\n",
            "     |      would print the string `scope1/scope2`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A string representing the current name scope.\n",
            "     |  \n",
            "     |  get_operation_by_name(self, name)\n",
            "     |      Returns the `Operation` with the given `name`.\n",
            "     |      \n",
            "     |      This method may be called concurrently from multiple threads.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The name of the `Operation` to return.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The `Operation` with the given `name`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `name` is not a string.\n",
            "     |        KeyError: If `name` does not correspond to an operation in this graph.\n",
            "     |  \n",
            "     |  get_operations(self)\n",
            "     |      Return the list of operations in the graph.\n",
            "     |      \n",
            "     |      You can modify the operations in place, but modifications\n",
            "     |      to the list such as inserts/delete have no effect on the\n",
            "     |      list of operations known to the graph.\n",
            "     |      \n",
            "     |      This method may be called concurrently from multiple threads.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list of Operations.\n",
            "     |  \n",
            "     |  get_tensor_by_name(self, name)\n",
            "     |      Returns the `Tensor` with the given `name`.\n",
            "     |      \n",
            "     |      This method may be called concurrently from multiple threads.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The name of the `Tensor` to return.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The `Tensor` with the given `name`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `name` is not a string.\n",
            "     |        KeyError: If `name` does not correspond to a tensor in this graph.\n",
            "     |  \n",
            "     |  gradient_override_map(self, op_type_map)\n",
            "     |      EXPERIMENTAL: A context manager for overriding gradient functions.\n",
            "     |      \n",
            "     |      This context manager can be used to override the gradient function\n",
            "     |      that will be used for ops within the scope of the context.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      @tf.RegisterGradient(\"CustomSquare\")\n",
            "     |      def _custom_square_grad(op, grad):\n",
            "     |        # ...\n",
            "     |      \n",
            "     |      with tf.Graph().as_default() as g:\n",
            "     |        c = tf.constant(5.0)\n",
            "     |        s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n",
            "     |        with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n",
            "     |          s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n",
            "     |                                # gradient of s_2.\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        op_type_map: A dictionary mapping op type strings to alternative op type\n",
            "     |          strings.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A context manager that sets the alternative op type to be used for one\n",
            "     |        or more ops created in that context.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `op_type_map` is not a dictionary mapping strings to\n",
            "     |          strings.\n",
            "     |  \n",
            "     |  is_feedable(self, tensor)\n",
            "     |      Returns `True` if and only if `tensor` is feedable.\n",
            "     |  \n",
            "     |  is_fetchable(self, tensor_or_op)\n",
            "     |      Returns `True` if and only if `tensor_or_op` is fetchable.\n",
            "     |  \n",
            "     |  name_scope(self, name)\n",
            "     |      Returns a context manager that creates hierarchical names for operations.\n",
            "     |      \n",
            "     |      A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
            "     |      statement pushes a new name onto the stack for the lifetime of the context.\n",
            "     |      \n",
            "     |      The `name` argument will be interpreted as follows:\n",
            "     |      \n",
            "     |      * A string (not ending with '/') will create a new name scope, in which\n",
            "     |        `name` is appended to the prefix of all operations created in the\n",
            "     |        context. If `name` has been used before, it will be made unique by\n",
            "     |        calling `self.unique_name(name)`.\n",
            "     |      * A scope previously captured from a `with g.name_scope(...) as\n",
            "     |        scope:` statement will be treated as an \"absolute\" name scope, which\n",
            "     |        makes it possible to re-enter existing scopes.\n",
            "     |      * A value of `None` or the empty string will reset the current name scope\n",
            "     |        to the top-level (empty) name scope.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      with tf.Graph().as_default() as g:\n",
            "     |        c = tf.constant(5.0, name=\"c\")\n",
            "     |        assert c.op.name == \"c\"\n",
            "     |        c_1 = tf.constant(6.0, name=\"c\")\n",
            "     |        assert c_1.op.name == \"c_1\"\n",
            "     |      \n",
            "     |        # Creates a scope called \"nested\"\n",
            "     |        with g.name_scope(\"nested\") as scope:\n",
            "     |          nested_c = tf.constant(10.0, name=\"c\")\n",
            "     |          assert nested_c.op.name == \"nested/c\"\n",
            "     |      \n",
            "     |          # Creates a nested scope called \"inner\".\n",
            "     |          with g.name_scope(\"inner\"):\n",
            "     |            nested_inner_c = tf.constant(20.0, name=\"c\")\n",
            "     |            assert nested_inner_c.op.name == \"nested/inner/c\"\n",
            "     |      \n",
            "     |          # Create a nested scope called \"inner_1\".\n",
            "     |          with g.name_scope(\"inner\"):\n",
            "     |            nested_inner_1_c = tf.constant(30.0, name=\"c\")\n",
            "     |            assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n",
            "     |      \n",
            "     |            # Treats `scope` as an absolute name scope, and\n",
            "     |            # switches to the \"nested/\" scope.\n",
            "     |            with g.name_scope(scope):\n",
            "     |              nested_d = tf.constant(40.0, name=\"d\")\n",
            "     |              assert nested_d.op.name == \"nested/d\"\n",
            "     |      \n",
            "     |              with g.name_scope(\"\"):\n",
            "     |                e = tf.constant(50.0, name=\"e\")\n",
            "     |                assert e.op.name == \"e\"\n",
            "     |      ```\n",
            "     |      \n",
            "     |      The name of the scope itself can be captured by `with\n",
            "     |      g.name_scope(...) as scope:`, which stores the name of the scope\n",
            "     |      in the variable `scope`. This value can be used to name an\n",
            "     |      operation that represents the overall result of executing the ops\n",
            "     |      in a scope. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      inputs = tf.constant(...)\n",
            "     |      with g.name_scope('my_layer') as scope:\n",
            "     |        weights = tf.Variable(..., name=\"weights\")\n",
            "     |        biases = tf.Variable(..., name=\"biases\")\n",
            "     |        affine = tf.matmul(inputs, weights) + biases\n",
            "     |        output = tf.nn.relu(affine, name=scope)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      NOTE: This constructor validates the given `name`. Valid scope\n",
            "     |      names match one of the following regular expressions:\n",
            "     |      \n",
            "     |          [A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)\n",
            "     |          [A-Za-z0-9_.\\-/]* (for other scopes)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name for the scope.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A context manager that installs `name` as a new name scope.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `name` is not a valid scope name, according to the rules\n",
            "     |          above.\n",
            "     |  \n",
            "     |  prevent_feeding(self, tensor)\n",
            "     |      Marks the given `tensor` as unfeedable in this graph.\n",
            "     |  \n",
            "     |  prevent_fetching(self, op)\n",
            "     |      Marks the given `op` as unfetchable in this graph.\n",
            "     |  \n",
            "     |  switch_to_thread_local(self)\n",
            "     |      Make device, colocation and dependencies stacks thread-local.\n",
            "     |      \n",
            "     |      Device, colocation and dependencies stacks are not thread-local be default.\n",
            "     |      If multiple threads access them, then the state is shared.  This means that\n",
            "     |      one thread may affect the behavior of another thread.\n",
            "     |      \n",
            "     |      After this method is called, the stacks become thread-local.  If multiple\n",
            "     |      threads access them, then the state is not shared.  Each thread uses its own\n",
            "     |      value; a thread doesn't affect other threads by mutating such a stack.\n",
            "     |      \n",
            "     |      The initial value for every thread's stack is set to the current value\n",
            "     |      of the stack when `switch_to_thread_local()` was first called.\n",
            "     |  \n",
            "     |  unique_name(self, name, mark_as_used=True)\n",
            "     |      Return a unique operation name for `name`.\n",
            "     |      \n",
            "     |      Note: You rarely need to call `unique_name()` directly.  Most of\n",
            "     |      the time you just need to create `with g.name_scope()` blocks to\n",
            "     |      generate structured names.\n",
            "     |      \n",
            "     |      `unique_name` is used to generate structured names, separated by\n",
            "     |      `\"/\"`, to help identify operations when debugging a graph.\n",
            "     |      Operation names are displayed in error messages reported by the\n",
            "     |      TensorFlow runtime, and in various visualization tools such as\n",
            "     |      TensorBoard.\n",
            "     |      \n",
            "     |      If `mark_as_used` is set to `True`, which is the default, a new\n",
            "     |      unique name is created and marked as in use. If it's set to `False`,\n",
            "     |      the unique name is returned without actually being marked as used.\n",
            "     |      This is useful when the caller simply wants to know what the name\n",
            "     |      to be created will be.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The name for an operation.\n",
            "     |        mark_as_used: Whether to mark this name as being used.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A string to be passed to `create_op()` that will be used\n",
            "     |        to name the operation being created.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  building_function\n",
            "     |      Returns True iff this graph represents a function.\n",
            "     |  \n",
            "     |  collections\n",
            "     |      Returns the names of the collections known to this graph.\n",
            "     |  \n",
            "     |  finalized\n",
            "     |      True if this graph has been finalized.\n",
            "     |  \n",
            "     |  graph_def_versions\n",
            "     |      The GraphDef version information of this graph.\n",
            "     |      \n",
            "     |      For details on the meaning of each version, see\n",
            "     |      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `VersionDef`.\n",
            "     |  \n",
            "     |  seed\n",
            "     |      The graph-level random seed of this graph.\n",
            "     |  \n",
            "     |  version\n",
            "     |      Returns a version number that increases as ops are added to the graph.\n",
            "     |      \n",
            "     |      Note that this is unrelated to the\n",
            "     |      `tf.Graph.graph_def_versions`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |         An integer version that increases as ops are added to the graph.\n",
            "    \n",
            "    class IndexedSlices(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "     |  A sparse representation of a set of tensor slices at given indices.\n",
            "     |  \n",
            "     |  This class is a simple wrapper for a pair of `Tensor` objects:\n",
            "     |  \n",
            "     |  * `values`: A `Tensor` of any dtype with shape `[D0, D1, ..., Dn]`.\n",
            "     |  * `indices`: A 1-D integer `Tensor` with shape `[D0]`.\n",
            "     |  \n",
            "     |  An `IndexedSlices` is typically used to represent a subset of a larger\n",
            "     |  tensor `dense` of shape `[LARGE0, D1, .. , DN]` where `LARGE0 >> D0`.\n",
            "     |  The values in `indices` are the indices in the first dimension of\n",
            "     |  the slices that have been extracted from the larger tensor.\n",
            "     |  \n",
            "     |  The dense tensor `dense` represented by an `IndexedSlices` `slices` has\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n",
            "     |  ```\n",
            "     |  \n",
            "     |  The `IndexedSlices` class is used principally in the definition of\n",
            "     |  gradients for operations that have sparse gradients\n",
            "     |  (e.g. `tf.gather`).\n",
            "     |  \n",
            "     |  Contrast this representation with\n",
            "     |  `tf.SparseTensor`,\n",
            "     |  which uses multi-dimensional indices and scalar values.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      IndexedSlices\n",
            "     |      tensorflow.python.framework.tensor_like._TensorLike\n",
            "     |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, values, indices, dense_shape=None)\n",
            "     |      Creates an `IndexedSlices`.\n",
            "     |  \n",
            "     |  __neg__(self)\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  consumers(self)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  dense_shape\n",
            "     |      A 1-D `Tensor` containing the shape of the corresponding dense tensor.\n",
            "     |  \n",
            "     |  device\n",
            "     |      The name of the device on which `values` will be produced, or `None`.\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `DType` of elements in this tensor.\n",
            "     |  \n",
            "     |  graph\n",
            "     |      The `Graph` that contains the values, indices, and shape tensors.\n",
            "     |  \n",
            "     |  indices\n",
            "     |      A 1-D `Tensor` containing the indices of the slices.\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of this `IndexedSlices`.\n",
            "     |  \n",
            "     |  op\n",
            "     |      The `Operation` that produces `values` as an output.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      Gets the `tf.TensorShape` representing the shape of the dense tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.TensorShape` object.\n",
            "     |  \n",
            "     |  values\n",
            "     |      A `Tensor` containing the values of the slices.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.framework.tensor_like._TensorLike:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class IndexedSlicesSpec(tensorflow.python.framework.type_spec.TypeSpec)\n",
            "     |  Type specification for a `tf.IndexedSlices`.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      IndexedSlicesSpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, shape=None, dtype=tf.float32, indices_dtype=tf.int64, dense_shape_dtype=None, indices_shape=None)\n",
            "     |      Constructs a type specification for a `tf.IndexedSlices`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: The dense shape of the `IndexedSlices`, or `None` to allow any\n",
            "     |          dense shape.\n",
            "     |        dtype: `tf.DType` of values in the `IndexedSlices`.\n",
            "     |        indices_dtype: `tf.DType` of the `indices` in the `IndexedSlices`.  One\n",
            "     |          of `tf.int32` or `tf.int64`.\n",
            "     |        dense_shape_dtype: `tf.DType` of the `dense_shape` in the `IndexedSlices`.\n",
            "     |          One of `tf.int32`, `tf.int64`, or `None` (if the `IndexedSlices` has\n",
            "     |          no `dense_shape` tensor).\n",
            "     |        indices_shape: The shape of the `indices` component, which indicates\n",
            "     |          how many slices are in the `IndexedSlices`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_value)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "    \n",
            "    class Module(tensorflow.python.training.tracking.tracking.AutoTrackable)\n",
            "     |  Base neural network module class.\n",
            "     |  \n",
            "     |  A module is a named container for `tf.Variable`s, other `tf.Module`s and\n",
            "     |  functions which apply to user input. For example a dense layer in a neural\n",
            "     |  network might be implemented as a `tf.Module`:\n",
            "     |  \n",
            "     |   >>> class Dense(tf.Module):\n",
            "     |   ...   def __init__(self, in_features, out_features, name=None):\n",
            "     |   ...     super(Dense, self).__init__(name=name)\n",
            "     |   ...     self.w = tf.Variable(\n",
            "     |   ...       tf.random.normal([in_features, out_features]), name='w')\n",
            "     |   ...     self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
            "     |   ...   def __call__(self, x):\n",
            "     |   ...     y = tf.matmul(x, self.w) + self.b\n",
            "     |   ...     return tf.nn.relu(y)\n",
            "     |  \n",
            "     |  You can use the Dense layer as you would expect:\n",
            "     |  \n",
            "     |  >>> d = Dense(in_features=3, out_features=2)\n",
            "     |  >>> d(tf.ones([1, 3]))\n",
            "     |  <tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)>\n",
            "     |  \n",
            "     |  \n",
            "     |  By subclassing `tf.Module` instead of `object` any `tf.Variable` or\n",
            "     |  `tf.Module` instances assigned to object properties can be collected using\n",
            "     |  the `variables`, `trainable_variables` or `submodules` property:\n",
            "     |  \n",
            "     |  >>> d.variables\n",
            "     |      (<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=...,\n",
            "     |      dtype=float32)>,\n",
            "     |      <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=..., dtype=float32)>)\n",
            "     |  \n",
            "     |  \n",
            "     |  Subclasses of `tf.Module` can also take advantage of the `_flatten` method\n",
            "     |  which can be used to implement tracking of any other types.\n",
            "     |  \n",
            "     |  All `tf.Module` classes have an associated `tf.name_scope` which can be used\n",
            "     |  to group operations in TensorBoard and create hierarchies for variable names\n",
            "     |  which can help with debugging. We suggest using the name scope when creating\n",
            "     |  nested submodules/parameters or for forward methods whose graph you might want\n",
            "     |  to inspect in TensorBoard. You can enter the name scope explicitly using\n",
            "     |  `with self.name_scope:` or you can annotate methods (apart from `__init__`)\n",
            "     |  with `@tf.Module.with_name_scope`.\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  class MLP(tf.Module):\n",
            "     |    def __init__(self, input_size, sizes, name=None):\n",
            "     |      super(MLP, self).__init__(name=name)\n",
            "     |      self.layers = []\n",
            "     |      with self.name_scope:\n",
            "     |        for size in sizes:\n",
            "     |          self.layers.append(Dense(input_size=input_size, output_size=size))\n",
            "     |          input_size = size\n",
            "     |  \n",
            "     |    @tf.Module.with_name_scope\n",
            "     |    def __call__(self, x):\n",
            "     |      for layer in self.layers:\n",
            "     |        x = layer(x)\n",
            "     |      return x\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Module\n",
            "     |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            "     |      tensorflow.python.training.tracking.base.Trackable\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, name=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  with_name_scope(method) from builtins.type\n",
            "     |      Decorator to automatically enter the module name scope.\n",
            "     |      \n",
            "     |      >>> class MyModule(tf.Module):\n",
            "     |      ...   @tf.Module.with_name_scope\n",
            "     |      ...   def __call__(self, x):\n",
            "     |      ...     if not hasattr(self, 'w'):\n",
            "     |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            "     |      ...     return tf.matmul(x, self.w)\n",
            "     |      \n",
            "     |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            "     |      names included the module name:\n",
            "     |      \n",
            "     |      >>> mod = MyModule()\n",
            "     |      >>> mod(tf.ones([1, 2]))\n",
            "     |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            "     |      >>> mod.w\n",
            "     |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            "     |      numpy=..., dtype=float32)>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        method: The method to wrap.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The original method wrapped such that it enters the module's name scope.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  name\n",
            "     |      Returns the name of this module as passed or determined in the ctor.\n",
            "     |      \n",
            "     |      NOTE: This is not the same as the `self.name_scope.name` which includes\n",
            "     |      parent module names.\n",
            "     |  \n",
            "     |  name_scope\n",
            "     |      Returns a `tf.name_scope` instance for this class.\n",
            "     |  \n",
            "     |  submodules\n",
            "     |      Sequence of all sub-modules.\n",
            "     |      \n",
            "     |      Submodules are modules which are properties of this module, or found as\n",
            "     |      properties of modules which are properties of this module (and so on).\n",
            "     |      \n",
            "     |      >>> a = tf.Module()\n",
            "     |      >>> b = tf.Module()\n",
            "     |      >>> c = tf.Module()\n",
            "     |      >>> a.b = b\n",
            "     |      >>> b.c = c\n",
            "     |      >>> list(a.submodules) == [b, c]\n",
            "     |      True\n",
            "     |      >>> list(b.submodules) == [c]\n",
            "     |      True\n",
            "     |      >>> list(c.submodules) == []\n",
            "     |      True\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A sequence of all submodules.\n",
            "     |  \n",
            "     |  trainable_variables\n",
            "     |      Sequence of trainable variables owned by this module and its submodules.\n",
            "     |      \n",
            "     |      Note: this method uses reflection to find variables on the current instance\n",
            "     |      and submodules. For performance reasons you may wish to cache the result\n",
            "     |      of calling this method if you don't expect the return value to change.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A sequence of variables for the current module (sorted by attribute\n",
            "     |        name) followed by variables from all submodules recursively (breadth\n",
            "     |        first).\n",
            "     |  \n",
            "     |  variables\n",
            "     |      Sequence of variables owned by this module and its submodules.\n",
            "     |      \n",
            "     |      Note: this method uses reflection to find variables on the current instance\n",
            "     |      and submodules. For performance reasons you may wish to cache the result\n",
            "     |      of calling this method if you don't expect the return value to change.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A sequence of variables for the current module (sorted by attribute\n",
            "     |        name) followed by variables from all submodules recursively (breadth\n",
            "     |        first).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.training.tracking.tracking.AutoTrackable:\n",
            "     |  \n",
            "     |  __delattr__(self, name)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value)\n",
            "     |      Support self.foo = trackable syntax.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class Operation(builtins.object)\n",
            "     |  Represents a graph node that performs computation on tensors.\n",
            "     |  \n",
            "     |  An `Operation` is a node in a `tf.Graph` that takes zero or more `Tensor`\n",
            "     |  objects as input, and produces zero or more `Tensor` objects as output.\n",
            "     |  Objects of type `Operation` are created by calling a Python op constructor\n",
            "     |  (such as `tf.matmul`) within a `tf.function` or under a `tf.Graph.as_default`\n",
            "     |  context manager.\n",
            "     |  \n",
            "     |  For example, within a `tf.function`, `c = tf.matmul(a, b)` creates an\n",
            "     |  `Operation` of type \"MatMul\" that takes tensors `a` and `b` as input, and\n",
            "     |  produces `c` as output.\n",
            "     |  \n",
            "     |  If a `tf.compat.v1.Session` is used, an `Operation` of a `tf.Graph` can be\n",
            "     |  executed by passing it to `tf.Session.run`. `op.run()` is a shortcut for\n",
            "     |  calling `tf.compat.v1.get_default_session().run(op)`.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, node_def, g, inputs=None, output_types=None, control_inputs=None, input_types=None, original_op=None, op_def=None)\n",
            "     |      Creates an `Operation`.\n",
            "     |      \n",
            "     |      NOTE: This constructor validates the name of the `Operation` (passed\n",
            "     |      as `node_def.name`). Valid `Operation` names match the following\n",
            "     |      regular expression:\n",
            "     |      \n",
            "     |          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        node_def: `node_def_pb2.NodeDef`.  `NodeDef` for the `Operation`. Used for\n",
            "     |          attributes of `node_def_pb2.NodeDef`, typically `name`, `op`, and\n",
            "     |          `device`.  The `input` attribute is irrelevant here as it will be\n",
            "     |          computed when generating the model.\n",
            "     |        g: `Graph`. The parent graph.\n",
            "     |        inputs: list of `Tensor` objects. The inputs to this `Operation`.\n",
            "     |        output_types: list of `DType` objects.  List of the types of the `Tensors`\n",
            "     |          computed by this operation.  The length of this list indicates the\n",
            "     |          number of output endpoints of the `Operation`.\n",
            "     |        control_inputs: list of operations or tensors from which to have a control\n",
            "     |          dependency.\n",
            "     |        input_types: List of `DType` objects representing the types of the tensors\n",
            "     |          accepted by the `Operation`.  By default uses `[x.dtype.base_dtype for x\n",
            "     |          in inputs]`.  Operations that expect reference-typed inputs must specify\n",
            "     |          these explicitly.\n",
            "     |        original_op: Optional. Used to associate the new `Operation` with an\n",
            "     |          existing `Operation` (for example, a replica with the op that was\n",
            "     |          replicated).\n",
            "     |        op_def: Optional. The `op_def_pb2.OpDef` proto that describes the op type\n",
            "     |          that this `Operation` represents.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if control inputs are not Operations or Tensors,\n",
            "     |          or if `node_def` is not a `NodeDef`,\n",
            "     |          or if `g` is not a `Graph`,\n",
            "     |          or if `inputs` are not tensors,\n",
            "     |          or if `inputs` and `input_types` are incompatible.\n",
            "     |        ValueError: if the `node_def` name is not valid.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  colocation_groups(self)\n",
            "     |      Returns the list of colocation groups of the op.\n",
            "     |  \n",
            "     |  get_attr(self, name)\n",
            "     |      Returns the value of the attr of this op with the given `name`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The name of the attr to fetch.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The value of the attr, as a Python object.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If this op does not have an attr with the given `name`.\n",
            "     |  \n",
            "     |  run(self, feed_dict=None, session=None)\n",
            "     |      Runs this operation in a `Session`.\n",
            "     |      \n",
            "     |      Calling this method will execute all preceding operations that\n",
            "     |      produce the inputs needed for this operation.\n",
            "     |      \n",
            "     |      *N.B.* Before invoking `Operation.run()`, its graph must have been\n",
            "     |      launched in a session, and either a default session must be\n",
            "     |      available, or `session` must be specified explicitly.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        feed_dict: A dictionary that maps `Tensor` objects to feed values. See\n",
            "     |          `tf.Session.run` for a description of the valid feed values.\n",
            "     |        session: (Optional.) The `Session` to be used to run to this operation. If\n",
            "     |          none, the default session will be used.\n",
            "     |  \n",
            "     |  values(self)\n",
            "     |      DEPRECATED: Use outputs.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  control_inputs\n",
            "     |      The `Operation` objects on which this op has a control dependency.\n",
            "     |      \n",
            "     |      Before this op is executed, TensorFlow will ensure that the\n",
            "     |      operations in `self.control_inputs` have finished executing. This\n",
            "     |      mechanism can be used to run ops sequentially for performance\n",
            "     |      reasons, or to ensure that the side effects of an op are observed\n",
            "     |      in the correct order.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list of `Operation` objects.\n",
            "     |  \n",
            "     |  device\n",
            "     |      The name of the device to which this op has been assigned, if any.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The string name of the device to which this op has been\n",
            "     |        assigned, or an empty string if it has not been assigned to a\n",
            "     |        device.\n",
            "     |  \n",
            "     |  graph\n",
            "     |      The `Graph` that contains this operation.\n",
            "     |  \n",
            "     |  inputs\n",
            "     |      The sequence of `Tensor` objects representing the data inputs of this op.\n",
            "     |  \n",
            "     |  name\n",
            "     |      The full name of this operation.\n",
            "     |  \n",
            "     |  node_def\n",
            "     |      Returns the `NodeDef` representation of this operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A\n",
            "     |        [`NodeDef`](https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto)\n",
            "     |        protocol buffer.\n",
            "     |  \n",
            "     |  op_def\n",
            "     |      Returns the `OpDef` proto that represents the type of this op.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An\n",
            "     |        [`OpDef`](https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto)\n",
            "     |        protocol buffer.\n",
            "     |  \n",
            "     |  outputs\n",
            "     |      The list of `Tensor` objects representing the outputs of this op.\n",
            "     |  \n",
            "     |  traceback\n",
            "     |      Returns the call stack from when this operation was constructed.\n",
            "     |  \n",
            "     |  type\n",
            "     |      The type of the op (e.g. `\"MatMul\"`).\n",
            "    \n",
            "    class OptionalSpec(tensorflow.python.framework.type_spec.TypeSpec)\n",
            "     |  Represents an optional potentially containing a structured value.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      OptionalSpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, value_structure)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods defined here:\n",
            "     |  \n",
            "     |  from_value(value)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |      The Python type for values that are compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_value)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "    \n",
            "    class RaggedTensor(tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "     |  Represents a ragged tensor.\n",
            "     |  \n",
            "     |  A `RaggedTensor` is a tensor with one or more *ragged dimensions*, which are\n",
            "     |  dimensions whose slices may have different lengths.  For example, the inner\n",
            "     |  (column) dimension of `rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]` is ragged,\n",
            "     |  since the column slices (`rt[0, :]`, ..., `rt[4, :]`) have different lengths.\n",
            "     |  Dimensions whose slices all have the same length are called *uniform\n",
            "     |  dimensions*.  The outermost dimension of a `RaggedTensor` is always uniform,\n",
            "     |  since it consists of a single slice (and so there is no possibility for\n",
            "     |  differing slice lengths).\n",
            "     |  \n",
            "     |  The total number of dimensions in a `RaggedTensor` is called its *rank*,\n",
            "     |  and the number of ragged dimensions in a `RaggedTensor` is called its\n",
            "     |  *ragged-rank*.  A `RaggedTensor`'s ragged-rank is fixed at graph creation\n",
            "     |  time: it can't depend on the runtime values of `Tensor`s, and can't vary\n",
            "     |  dynamically for different session runs.\n",
            "     |  \n",
            "     |  ### Potentially Ragged Tensors\n",
            "     |  \n",
            "     |  Many ops support both `Tensor`s and `RaggedTensor`s.  The term \"potentially\n",
            "     |  ragged tensor\" may be used to refer to a tensor that might be either a\n",
            "     |  `Tensor` or a `RaggedTensor`.  The ragged-rank of a `Tensor` is zero.\n",
            "     |  \n",
            "     |  ### Documenting RaggedTensor Shapes\n",
            "     |  \n",
            "     |  When documenting the shape of a RaggedTensor, ragged dimensions can be\n",
            "     |  indicated by enclosing them in parentheses.  For example, the shape of\n",
            "     |  a 3-D `RaggedTensor` that stores the fixed-size word embedding for each\n",
            "     |  word in a sentence, for each sentence in a batch, could be written as\n",
            "     |  `[num_sentences, (num_words), embedding_size]`.  The parentheses around\n",
            "     |  `(num_words)` indicate that dimension is ragged, and that the length\n",
            "     |  of each element list in that dimension may vary for each item.\n",
            "     |  \n",
            "     |  ### Component Tensors\n",
            "     |  \n",
            "     |  Internally, a `RaggedTensor` consists of a concatenated list of values that\n",
            "     |  are partitioned into variable-length rows.  In particular, each `RaggedTensor`\n",
            "     |  consists of:\n",
            "     |  \n",
            "     |    * A `values` tensor, which concatenates the variable-length rows into a\n",
            "     |      flattened list.  For example, the `values` tensor for\n",
            "     |      `[[3, 1, 4, 1], [], [5, 9, 2], [6], []]` is `[3, 1, 4, 1, 5, 9, 2, 6]`.\n",
            "     |  \n",
            "     |    * A `row_splits` vector, which indicates how those flattened values are\n",
            "     |      divided into rows.  In particular, the values for row `rt[i]` are stored\n",
            "     |      in the slice `rt.values[rt.row_splits[i]:rt.row_splits[i+1]]`.\n",
            "     |  \n",
            "     |  Example:\n",
            "     |  \n",
            "     |  >>> print(tf.RaggedTensor.from_row_splits(\n",
            "     |  ...       values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |  ...       row_splits=[0, 4, 4, 7, 8, 8]))\n",
            "     |  <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  ### Alternative Row-Partitioning Schemes\n",
            "     |  \n",
            "     |  In addition to `row_splits`, ragged tensors provide support for four other\n",
            "     |  row-partitioning schemes:\n",
            "     |  \n",
            "     |    * `row_lengths`: a vector with shape `[nrows]`, which specifies the length\n",
            "     |      of each row.\n",
            "     |  \n",
            "     |    * `value_rowids` and `nrows`: `value_rowids` is a vector with shape\n",
            "     |      `[nvals]`, corresponding one-to-one with `values`, which specifies\n",
            "     |      each value's row index.  In particular, the row `rt[row]` consists of the\n",
            "     |      values `rt.values[j]` where `value_rowids[j]==row`.  `nrows` is an\n",
            "     |      integer scalar that specifies the number of rows in the\n",
            "     |      `RaggedTensor`. (`nrows` is used to indicate trailing empty rows.)\n",
            "     |  \n",
            "     |    * `row_starts`: a vector with shape `[nrows]`, which specifies the start\n",
            "     |      offset of each row.  Equivalent to `row_splits[:-1]`.\n",
            "     |  \n",
            "     |    * `row_limits`: a vector with shape `[nrows]`, which specifies the stop\n",
            "     |      offset of each row.  Equivalent to `row_splits[1:]`.\n",
            "     |  \n",
            "     |    * `uniform_row_length`: A scalar tensor, specifying the length of every\n",
            "     |      row.  This row-partitioning scheme may only be used if all rows have\n",
            "     |      the same length.\n",
            "     |  \n",
            "     |  Example: The following ragged tensors are equivalent, and all represent the\n",
            "     |  nested list `[[3, 1, 4, 1], [], [5, 9, 2], [6], []]`.\n",
            "     |  \n",
            "     |  >>> values = [3, 1, 4, 1, 5, 9, 2, 6]\n",
            "     |  >>> rt1 = RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n",
            "     |  >>> rt2 = RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n",
            "     |  >>> rt3 = RaggedTensor.from_value_rowids(\n",
            "     |  ...     values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n",
            "     |  >>> rt4 = RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n",
            "     |  >>> rt5 = RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n",
            "     |  \n",
            "     |  ### Multiple Ragged Dimensions\n",
            "     |  \n",
            "     |  `RaggedTensor`s with multiple ragged dimensions can be defined by using\n",
            "     |  a nested `RaggedTensor` for the `values` tensor.  Each nested `RaggedTensor`\n",
            "     |  adds a single ragged dimension.\n",
            "     |  \n",
            "     |  >>> inner_rt = RaggedTensor.from_row_splits(  # =rt1 from above\n",
            "     |  ...     values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n",
            "     |  >>> outer_rt = RaggedTensor.from_row_splits(\n",
            "     |  ...     values=inner_rt, row_splits=[0, 3, 3, 5])\n",
            "     |  >>> print(outer_rt.to_list())\n",
            "     |  [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\n",
            "     |  >>> print(outer_rt.ragged_rank)\n",
            "     |  2\n",
            "     |  \n",
            "     |  The factory function `RaggedTensor.from_nested_row_splits` may be used to\n",
            "     |  construct a `RaggedTensor` with multiple ragged dimensions directly, by\n",
            "     |  providing a list of `row_splits` tensors:\n",
            "     |  \n",
            "     |  >>> RaggedTensor.from_nested_row_splits(\n",
            "     |  ...     flat_values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |  ...     nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8])).to_list()\n",
            "     |  [[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\n",
            "     |  \n",
            "     |  ### Uniform Inner Dimensions\n",
            "     |  \n",
            "     |  `RaggedTensor`s with uniform inner dimensions can be defined\n",
            "     |  by using a multidimensional `Tensor` for `values`.\n",
            "     |  \n",
            "     |  >>> rt = RaggedTensor.from_row_splits(values=tf.ones([5, 3], tf.int32),\n",
            "     |  ...                                   row_splits=[0, 2, 5])\n",
            "     |  >>> print(rt.to_list())\n",
            "     |  [[[1, 1, 1], [1, 1, 1]],\n",
            "     |   [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]\n",
            "     |  >>> print(rt.shape)\n",
            "     |  (2, None, 3)\n",
            "     |  \n",
            "     |  ### Uniform Outer Dimensions\n",
            "     |  \n",
            "     |  `RaggedTensor`s with uniform outer dimensions can be defined by using\n",
            "     |  one or more `RaggedTensor` with a `uniform_row_length` row-partitioning\n",
            "     |  tensor.  For example, a `RaggedTensor` with shape `[2, 2, None]` can be\n",
            "     |  constructed with this method from a `RaggedTensor` values with shape\n",
            "     |  `[4, None]`:\n",
            "     |  \n",
            "     |  >>> values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\n",
            "     |  >>> print(values.shape)\n",
            "     |  (4, None)\n",
            "     |  >>> rt6 = tf.RaggedTensor.from_uniform_row_length(values, 2)\n",
            "     |  >>> print(rt6)\n",
            "     |  <tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\n",
            "     |  >>> print(rt6.shape)\n",
            "     |  (2, 2, None)\n",
            "     |  \n",
            "     |  Note that `rt6` only contains one ragged dimension (the innermost\n",
            "     |  dimension). In contrast, if `from_row_splits` is used to construct a similar\n",
            "     |  `RaggedTensor`, then that `RaggedTensor` will have two ragged dimensions:\n",
            "     |  \n",
            "     |  >>> rt7 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\n",
            "     |  >>> print(rt7.shape)\n",
            "     |  (2, None, None)\n",
            "     |  \n",
            "     |  Uniform and ragged outer dimensions may be interleaved, meaning that a\n",
            "     |  tensor with any combination of ragged and uniform dimensions may be created.\n",
            "     |  For example, a RaggedTensor `t4` with shape `[3, None, 4, 8, None, 2]` could\n",
            "     |  be constructed as follows:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  t0 = tf.zeros([1000, 2])                           # Shape:         [1000, 2]\n",
            "     |  t1 = RaggedTensor.from_row_lengths(t0, [...])      #           [160, None, 2]\n",
            "     |  t2 = RaggedTensor.from_uniform_row_length(t1, 8)   #         [20, 8, None, 2]\n",
            "     |  t3 = RaggedTensor.from_uniform_row_length(t2, 4)   #       [5, 4, 8, None, 2]\n",
            "     |  t4 = RaggedTensor.from_row_lengths(t3, [...])      # [3, None, 4, 8, None, 2]\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      RaggedTensor\n",
            "     |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __abs__ = abs(x, name=None)\n",
            "     |      Computes the absolute value of a tensor.\n",
            "     |      \n",
            "     |      Given a tensor of integer or floating-point values, this operation returns a\n",
            "     |      tensor of the same type, where each element contains the absolute value of the\n",
            "     |      corresponding element in the input.\n",
            "     |      \n",
            "     |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
            "     |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
            "     |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
            "     |      + b^2}\\\\).  For example:\n",
            "     |      \n",
            "     |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
            "     |      >>> tf.abs(x)\n",
            "     |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
            "     |      array([[5.25594901],\n",
            "     |             [6.60492241]])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
            "     |          `int32`, `int64`, `complex64` or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
            "     |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
            "     |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
            "     |      \n",
            "     |        If `x` is a `SparseTensor`, returns\n",
            "     |        `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
            "     |  \n",
            "     |  __add__ = add(x, y, name=None)\n",
            "     |      Returns x + y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __and__ = logical_and(x, y, name=None)\n",
            "     |      Logical AND function.\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical AND with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical AND of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_and(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_and(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_and(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  __bool__ = _dummy_bool(_)\n",
            "     |      Dummy method to prevent a RaggedTensor from being used as a Python bool.\n",
            "     |  \n",
            "     |  __div__ = div(x, y, name=None)\n",
            "     |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Deprecated in favor of operator or tf.math.divide.\n",
            "     |      \n",
            "     |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
            "     |      3 division operator semantics.\n",
            "     |      \n",
            "     |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
            "     |      and `y` are both integers then the result will be an integer. This is in\n",
            "     |      contrast to Python 3, where division with `/` is always a float while division\n",
            "     |      with `//` is always an integer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __floordiv__ = floordiv(x, y, name=None)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __ge__ = greater_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x >= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5, 2, 5, 10])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __getitem__ = ragged_tensor_getitem(self, key)\n",
            "     |      Returns the specified piece of this RaggedTensor.\n",
            "     |      \n",
            "     |      Supports multidimensional indexing and slicing, with one restriction:\n",
            "     |      indexing into a ragged inner dimension is not allowed.  This case is\n",
            "     |      problematic because the indicated value may exist in some rows but not\n",
            "     |      others.  In such cases, it's not obvious whether we should (1) report an\n",
            "     |      IndexError; (2) use a default value; or (3) skip that value and return a\n",
            "     |      tensor with fewer rows than we started with.  Following the guiding\n",
            "     |      principles of Python (\"In the face of ambiguity, refuse the temptation to\n",
            "     |      guess\"), we simply disallow this operation.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        self: The RaggedTensor to slice.\n",
            "     |        key: Indicates which piece of the RaggedTensor to return, using standard\n",
            "     |          Python semantics (e.g., negative values index from the end).  `key`\n",
            "     |          may have any of the following types:\n",
            "     |      \n",
            "     |          * `int` constant\n",
            "     |          * Scalar integer `Tensor`\n",
            "     |          * `slice` containing integer constants and/or scalar integer\n",
            "     |            `Tensor`s\n",
            "     |          * `Ellipsis`\n",
            "     |          * `tf.newaxis`\n",
            "     |          * `tuple` containing any of the above (for multidimensional indexing)\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` or `RaggedTensor` object.  Values that include at least one\n",
            "     |        ragged dimension are returned as `RaggedTensor`.  Values that include no\n",
            "     |        ragged dimensions are returned as `Tensor`.  See above for examples of\n",
            "     |        expressions that return `Tensor`s vs `RaggedTensor`s.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `key` is out of bounds.\n",
            "     |        ValueError: If `key` is not supported.\n",
            "     |        TypeError: If the indices in `key` have an unsupported type.\n",
            "     |      \n",
            "     |      Examples:\n",
            "     |      \n",
            "     |      >>> # A 2-D ragged tensor with 1 ragged dimension.\n",
            "     |      >>> rt = tf.ragged.constant([['a', 'b', 'c'], ['d', 'e'], ['f'], ['g']])\n",
            "     |      >>> rt[0].numpy()                 # First row (1-D `Tensor`)\n",
            "     |      array([b'a', b'b', b'c'], dtype=object)\n",
            "     |      >>> rt[:3].to_list()              # First three rows (2-D RaggedTensor)\n",
            "     |      [[b'a', b'b', b'c'], [b'd', b'e'], [b'f']]\n",
            "     |      >>> rt[3, 0].numpy()              # 1st element of 4th row (scalar)\n",
            "     |      b'g'\n",
            "     |      \n",
            "     |      >>> # A 3-D ragged tensor with 2 ragged dimensions.\n",
            "     |      >>> rt = tf.ragged.constant([[[1, 2, 3], [4]],\n",
            "     |      ...                          [[5], [], [6]],\n",
            "     |      ...                          [[7]],\n",
            "     |      ...                          [[8, 9], [10]]])\n",
            "     |      >>> rt[1].to_list()               # Second row (2-D RaggedTensor)\n",
            "     |      [[5], [], [6]]\n",
            "     |      >>> rt[3, 0].numpy()              # First element of fourth row (1-D Tensor)\n",
            "     |      array([8, 9], dtype=int32)\n",
            "     |      >>> rt[:, 1:3].to_list()          # Items 1-3 of each row (3-D RaggedTensor)\n",
            "     |      [[[4]], [[], [6]], [], [[10]]]\n",
            "     |      >>> rt[:, -1:].to_list()          # Last item of each row (3-D RaggedTensor)\n",
            "     |      [[[4]], [[6]], [[7]], [[10]]]\n",
            "     |  \n",
            "     |  __gt__ = greater(x, y, name=None)\n",
            "     |      Returns the truth value of (x > y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 2, 5])\n",
            "     |      tf.math.greater(x, y) ==> [False, True, True]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater(x, y) ==> [False, False, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __init__(self, values, row_splits, cached_row_lengths=None, cached_value_rowids=None, cached_nrows=None, internal=False, uniform_row_length=None)\n",
            "     |      Creates a `RaggedTensor` with a specified partitioning for `values`.\n",
            "     |      \n",
            "     |      This constructor is private -- please use one of the following ops to\n",
            "     |      build `RaggedTensor`s:\n",
            "     |      \n",
            "     |        * `tf.RaggedTensor.from_row_lengths`\n",
            "     |        * `tf.RaggedTensor.from_value_rowids`\n",
            "     |        * `tf.RaggedTensor.from_row_splits`\n",
            "     |        * `tf.RaggedTensor.from_row_starts`\n",
            "     |        * `tf.RaggedTensor.from_row_limits`\n",
            "     |        * `tf.RaggedTensor.from_nested_row_splits`\n",
            "     |        * `tf.RaggedTensor.from_nested_row_lengths`\n",
            "     |        * `tf.RaggedTensor.from_nested_value_rowids`\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor of any dtype and shape `[nvals, ...]`.\n",
            "     |        row_splits: A 1-D integer tensor with shape `[nrows+1]`.\n",
            "     |        cached_row_lengths: A 1-D integer tensor with shape `[nrows]`\n",
            "     |        cached_value_rowids: A 1-D integer tensor with shape `[nvals]`.\n",
            "     |        cached_nrows: A 1-D integer scalar tensor.\n",
            "     |        internal: True if the constructor is being called by one of the factory\n",
            "     |          methods.  If false, an exception will be raised.\n",
            "     |        uniform_row_length: A scalar tensor.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If a row partitioning tensor has an inappropriate dtype.\n",
            "     |        TypeError: If exactly one row partitioning argument was not specified.\n",
            "     |        ValueError: If a row partitioning tensor has an inappropriate shape.\n",
            "     |        ValueError: If multiple partitioning arguments are specified.\n",
            "     |        ValueError: If nrows is specified but value_rowids is not None.\n",
            "     |  \n",
            "     |  __invert__ = logical_not(x, name=None)\n",
            "     |      Returns the truth value of `NOT x` element-wise.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
            "     |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __le__ = less_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x <= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 6])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __lt__ = less(x, y, name=None)\n",
            "     |      Returns the truth value of (x < y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less(x, y) ==> [False, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 7])\n",
            "     |      tf.math.less(x, y) ==> [False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __mod__ = floor_mod(x, y, name=None)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __mul__ = multiply(x, y, name=None)\n",
            "     |      Returns an element-wise x * y.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      >>> x = tf.constant(([1, 2, 3, 4]))\n",
            "     |      >>> tf.math.multiply(x, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>\n",
            "     |      \n",
            "     |      Since `tf.math.multiply` will convert its arguments to `Tensor`s, you can also\n",
            "     |      pass in non-`Tensor` arguments:\n",
            "     |      \n",
            "     |      >>> tf.math.multiply(7,6)\n",
            "     |      <tf.Tensor: shape=(), dtype=int32, numpy=42>\n",
            "     |      \n",
            "     |      If `x.shape` is not thes same as `y.shape`, they will be broadcast to a\n",
            "     |      compatible shape. (More about broadcasting\n",
            "     |      [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).)\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      >>> x = tf.ones([1, 2]);\n",
            "     |      >>> y = tf.ones([2, 1]);\n",
            "     |      >>> x * y  # Taking advantage of operator overriding\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
            "     |      array([[1., 1.],\n",
            "     |           [1., 1.]], dtype=float32)>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A Tensor. Must be one of the following types: `bfloat16`,\n",
            "     |          `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`,\n",
            "     |          `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |      \n",
            "     |      A `Tensor`.  Has the same type as `x`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |      \n",
            "     |       * InvalidArgumentError: When `x` and `y` have incomptatible shapes or types.\n",
            "     |  \n",
            "     |  __neg__ = neg(x, name=None)\n",
            "     |      Computes numerical negative value element-wise.\n",
            "     |      \n",
            "     |      I.e., \\\\(y = -x\\\\).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |      \n",
            "     |        If `x` is a `SparseTensor`, returns\n",
            "     |        `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
            "     |  \n",
            "     |  __nonzero__ = _dummy_bool(_)\n",
            "     |      Dummy method to prevent a RaggedTensor from being used as a Python bool.\n",
            "     |  \n",
            "     |  __or__ = logical_or(x, y, name=None)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __pow__ = pow(x, y, name=None)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __radd__ = add(x, y, name=None)\n",
            "     |      Returns x + y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rand__ = logical_and(x, y, name=None)\n",
            "     |      Logical AND function.\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical AND with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical AND of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_and(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_and(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_and(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  __rdiv__ = div(x, y, name=None)\n",
            "     |      Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Deprecated in favor of operator or tf.math.divide.\n",
            "     |      \n",
            "     |      NOTE: Prefer using the Tensor division operator or tf.divide which obey Python\n",
            "     |      3 division operator semantics.\n",
            "     |      \n",
            "     |      This function divides `x` and `y`, forcing Python 2 semantics. That is, if `x`\n",
            "     |      and `y` are both integers then the result will be an integer. This is in\n",
            "     |      contrast to Python 3, where division with `/` is always a float while division\n",
            "     |      with `//` is always an integer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __rfloordiv__ = floordiv(x, y, name=None)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __rmod__ = floor_mod(x, y, name=None)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rmul__ = multiply(x, y, name=None)\n",
            "     |      Returns an element-wise x * y.\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      >>> x = tf.constant(([1, 2, 3, 4]))\n",
            "     |      >>> tf.math.multiply(x, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>\n",
            "     |      \n",
            "     |      Since `tf.math.multiply` will convert its arguments to `Tensor`s, you can also\n",
            "     |      pass in non-`Tensor` arguments:\n",
            "     |      \n",
            "     |      >>> tf.math.multiply(7,6)\n",
            "     |      <tf.Tensor: shape=(), dtype=int32, numpy=42>\n",
            "     |      \n",
            "     |      If `x.shape` is not thes same as `y.shape`, they will be broadcast to a\n",
            "     |      compatible shape. (More about broadcasting\n",
            "     |      [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).)\n",
            "     |      \n",
            "     |      For example:\n",
            "     |      \n",
            "     |      >>> x = tf.ones([1, 2]);\n",
            "     |      >>> y = tf.ones([2, 1]);\n",
            "     |      >>> x * y  # Taking advantage of operator overriding\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
            "     |      array([[1., 1.],\n",
            "     |           [1., 1.]], dtype=float32)>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A Tensor. Must be one of the following types: `bfloat16`,\n",
            "     |          `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`,\n",
            "     |          `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |      \n",
            "     |      A `Tensor`.  Has the same type as `x`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |      \n",
            "     |       * InvalidArgumentError: When `x` and `y` have incomptatible shapes or types.\n",
            "     |  \n",
            "     |  __ror__ = logical_or(x, y, name=None)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __rpow__ = pow(x, y, name=None)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __rsub__ = subtract(x, y, name=None)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rtruediv__ = truediv(x, y, name=None)\n",
            "     |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
            "     |      \n",
            "     |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
            "     |      division operator semantics.\n",
            "     |      \n",
            "     |      This function forces Python 3 division operator semantics where all integer\n",
            "     |      arguments are cast to floating types first.   This op is generated by normal\n",
            "     |      `x / y` division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.  If you want integer division that rounds\n",
            "     |      down, use `x // y` or `tf.math.floordiv`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
            "     |      point, the output will have the same type.  If the inputs are integral, the\n",
            "     |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
            "     |      and `int64` (matching the behavior of Numpy).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of numeric type.\n",
            "     |        y: `Tensor` denominator of numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` evaluated in floating point.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `x` and `y` have different dtypes.\n",
            "     |  \n",
            "     |  __rxor__ = logical_xor(x, y, name='LogicalXor')\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  __sub__ = subtract(x, y, name=None)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __truediv__ = truediv(x, y, name=None)\n",
            "     |      Divides x / y elementwise (using Python 3 division operator semantics).\n",
            "     |      \n",
            "     |      NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
            "     |      division operator semantics.\n",
            "     |      \n",
            "     |      This function forces Python 3 division operator semantics where all integer\n",
            "     |      arguments are cast to floating types first.   This op is generated by normal\n",
            "     |      `x / y` division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.  If you want integer division that rounds\n",
            "     |      down, use `x // y` or `tf.math.floordiv`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same numeric type.  If the inputs are floating\n",
            "     |      point, the output will have the same type.  If the inputs are integral, the\n",
            "     |      inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
            "     |      and `int64` (matching the behavior of Numpy).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of numeric type.\n",
            "     |        y: `Tensor` denominator of numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` evaluated in floating point.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `x` and `y` have different dtypes.\n",
            "     |  \n",
            "     |  __xor__ = logical_xor(x, y, name='LogicalXor')\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  bounding_shape(self, axis=None, name=None, out_type=None)\n",
            "     |      Returns the tight bounding box shape for this `RaggedTensor`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        axis: An integer scalar or vector indicating which axes to return the\n",
            "     |          bounding box for.  If not specified, then the full bounding box is\n",
            "     |          returned.\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |        out_type: `dtype` for the returned tensor.  Defaults to\n",
            "     |          `self.row_splits.dtype`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An integer `Tensor` (`dtype=self.row_splits.dtype`).  If `axis` is not\n",
            "     |        specified, then `output` is a vector with\n",
            "     |        `output.shape=[self.shape.ndims]`.  If `axis` is a scalar, then the\n",
            "     |        `output` is a scalar.  If `axis` is a vector, then `output` is a vector,\n",
            "     |        where `output[i]` is the bounding size for dimension `axis[i]`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[1, 2, 3, 4], [5], [], [6, 7, 8, 9], [10]])\n",
            "     |      >>> rt.bounding_shape().numpy()\n",
            "     |      array([5, 4])\n",
            "     |  \n",
            "     |  consumers(self)\n",
            "     |  \n",
            "     |  merge_dims(self, outer_axis, inner_axis)\n",
            "     |      Merges outer_axis...inner_axis into a single dimension.\n",
            "     |      \n",
            "     |      Returns a copy of this RaggedTensor with the specified range of dimensions\n",
            "     |      flattened into a single dimension, with elements in row-major order.\n",
            "     |      \n",
            "     |      #### Examples:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[[1, 2], [3]], [[4, 5, 6]]])\n",
            "     |      >>> print(rt.merge_dims(0, 1))\n",
            "     |      <tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>\n",
            "     |      >>> print(rt.merge_dims(1, 2))\n",
            "     |      <tf.RaggedTensor [[1, 2, 3], [4, 5, 6]]>\n",
            "     |      >>> print(rt.merge_dims(0, 2))\n",
            "     |      tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)\n",
            "     |      \n",
            "     |      To mimic the behavior of `np.flatten` (which flattens all dimensions), use\n",
            "     |      `rt.merge_dims(0, -1).  To mimic the behavior of `tf.layers.Flatten` (which\n",
            "     |      flattens all dimensions except the outermost batch dimension), use\n",
            "     |      `rt.merge_dims(1, -1)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        outer_axis: `int`: The first dimension in the range of dimensions to\n",
            "     |          merge. May be negative if `self.shape.rank` is statically known.\n",
            "     |        inner_axis: `int`: The last dimension in the range of dimensions to\n",
            "     |          merge. May be negative if `self.shape.rank` is statically known.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A copy of this tensor, with the specified dimensions merged into a\n",
            "     |        single dimension.  The shape of the returned tensor will be\n",
            "     |        `self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:]`, where `N`\n",
            "     |        is the total number of slices in the merged dimensions.\n",
            "     |  \n",
            "     |  nested_row_lengths(self, name=None)\n",
            "     |      Returns a tuple containing the row_lengths for all ragged dimensions.\n",
            "     |      \n",
            "     |      `rt.nested_row_lengths()` is a tuple containing the `row_lengths` tensors\n",
            "     |      for all ragged dimensions in `rt`, ordered from outermost to innermost.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tuple` of 1-D integer `Tensors`.  The length of the tuple is equal to\n",
            "     |        `self.ragged_rank`.\n",
            "     |  \n",
            "     |  nested_value_rowids(self, name=None)\n",
            "     |      Returns a tuple containing the value_rowids for all ragged dimensions.\n",
            "     |      \n",
            "     |      `rt.nested_value_rowids` is a tuple containing the `value_rowids` tensors\n",
            "     |      for\n",
            "     |      all ragged dimensions in `rt`, ordered from outermost to innermost.  In\n",
            "     |      particular, `rt.nested_value_rowids = (rt.value_rowids(),) + value_ids`\n",
            "     |      where:\n",
            "     |      \n",
            "     |          * `value_ids = ()` if `rt.values` is a `Tensor`.\n",
            "     |          * `value_ids = rt.values.nested_value_rowids` otherwise.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tuple` of 1-D integer `Tensor`s.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant(\n",
            "     |      ...     [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\n",
            "     |      >>> for i, ids in enumerate(rt.nested_value_rowids()):\n",
            "     |      ...   print('row ids for dimension %d: %s' % (i+1, ids.numpy()))\n",
            "     |      row ids for dimension 1: [0 0 0]\n",
            "     |      row ids for dimension 2: [0 0 0 2 2]\n",
            "     |      row ids for dimension 3: [0 0 0 0 2 2 2 3]\n",
            "     |  \n",
            "     |  nrows(self, out_type=None, name=None)\n",
            "     |      Returns the number of rows in this ragged tensor.\n",
            "     |      \n",
            "     |      I.e., the size of the outermost dimension of the tensor.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        out_type: `dtype` for the returned tensor.  Defaults to\n",
            "     |          `self.row_splits.dtype`.\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A scalar `Tensor` with dtype `out_type`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.nrows())  # rt has 5 rows.\n",
            "     |      tf.Tensor(5, shape=(), dtype=int64)\n",
            "     |  \n",
            "     |  numpy(self)\n",
            "     |      Returns a numpy `array` with the values for this `RaggedTensor`.\n",
            "     |      \n",
            "     |      Requires that this `RaggedTensor` was constructed in eager execution mode.\n",
            "     |      \n",
            "     |      Ragged dimensions are encoded using numpy `arrays` with `dtype=object` and\n",
            "     |      `rank=1`, where each element is a single row.\n",
            "     |      \n",
            "     |      #### Examples\n",
            "     |      \n",
            "     |      In the following example, the value returned by `RaggedTensor.numpy()`\n",
            "     |      contains three numpy `array` objects: one for each row (with `rank=1` and\n",
            "     |      `dtype=int64`), and one to combine them (with `rank=1` and `dtype=object`):\n",
            "     |      \n",
            "     |      >>> tf.ragged.constant([[1, 2, 3], [4, 5]], dtype=tf.int64).numpy()\n",
            "     |      array([array([1, 2, 3]), array([4, 5])], dtype=object)\n",
            "     |      \n",
            "     |      Uniform dimensions are encoded using multidimensional numpy `array`s.  In\n",
            "     |      the following example, the value returned by `RaggedTensor.numpy()` contains\n",
            "     |      a single numpy `array` object, with `rank=2` and `dtype=int64`:\n",
            "     |      \n",
            "     |      >>> tf.ragged.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int64).numpy()\n",
            "     |      array([[1, 2, 3], [4, 5, 6]])\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A numpy `array`.\n",
            "     |  \n",
            "     |  row_lengths(self, axis=1, name=None)\n",
            "     |      Returns the lengths of the rows in this ragged tensor.\n",
            "     |      \n",
            "     |      `rt.row_lengths()[i]` indicates the number of values in the\n",
            "     |      `i`th row of `rt`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        axis: An integer constant indicating the axis whose row lengths should be\n",
            "     |          returned.\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A potentially ragged integer Tensor with shape `self.shape[:axis]`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `axis` is out of bounds.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant(\n",
            "     |      ...     [[[3, 1, 4], [1]], [], [[5, 9], [2]], [[6]], []])\n",
            "     |      >>> print(rt.row_lengths())  # lengths of rows in rt\n",
            "     |      tf.Tensor([2 0 2 1 0], shape=(5,), dtype=int64)\n",
            "     |      >>> print(rt.row_lengths(axis=2))  # lengths of axis=2 rows.\n",
            "     |      <tf.RaggedTensor [[3, 1], [], [2, 1], [1], []]>\n",
            "     |  \n",
            "     |  row_limits(self, name=None)\n",
            "     |      Returns the limit indices for rows in this ragged tensor.\n",
            "     |      \n",
            "     |      These indices specify where the values for each row end in\n",
            "     |      `self.values`.  `rt.row_limits(self)` is equal to `rt.row_splits[:-1]`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 1-D integer Tensor with shape `[nrows]`.\n",
            "     |        The returned tensor is nonnegative, and is sorted in ascending order.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.values)\n",
            "     |      tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n",
            "     |      >>> print(rt.row_limits())  # indices of row limits in rt.values\n",
            "     |      tf.Tensor([4 4 7 8 8], shape=(5,), dtype=int64)\n",
            "     |  \n",
            "     |  row_starts(self, name=None)\n",
            "     |      Returns the start indices for rows in this ragged tensor.\n",
            "     |      \n",
            "     |      These indices specify where the values for each row begin in\n",
            "     |      `self.values`.  `rt.row_starts()` is equal to `rt.row_splits[:-1]`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 1-D integer Tensor with shape `[nrows]`.\n",
            "     |        The returned tensor is nonnegative, and is sorted in ascending order.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.values)\n",
            "     |      tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n",
            "     |      >>> print(rt.row_starts())  # indices of row starts in rt.values\n",
            "     |      tf.Tensor([0 4 4 7 8], shape=(5,), dtype=int64)\n",
            "     |  \n",
            "     |  to_list(self)\n",
            "     |      Returns a nested Python `list` with the values for this `RaggedTensor`.\n",
            "     |      \n",
            "     |      Requires that `rt` was constructed in eager execution mode.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A nested Python `list`.\n",
            "     |  \n",
            "     |  to_sparse(self, name=None)\n",
            "     |      Converts this `RaggedTensor` into a `tf.SparseTensor`.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[1, 2, 3], [4], [], [5, 6]])\n",
            "     |      >>> print(rt.to_sparse())\n",
            "     |      SparseTensor(indices=tf.Tensor(\n",
            "     |                       [[0 0] [0 1] [0 2] [1 0] [3 0] [3 1]],\n",
            "     |                       shape=(6, 2), dtype=int64),\n",
            "     |                   values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32),\n",
            "     |                   dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A SparseTensor with the same values as `self`.\n",
            "     |  \n",
            "     |  to_tensor(self, default_value=None, name=None, shape=None)\n",
            "     |      Converts this `RaggedTensor` into a `tf.Tensor`.\n",
            "     |      \n",
            "     |      If `shape` is specified, then the result is padded and/or truncated to\n",
            "     |      the specified shape.\n",
            "     |      \n",
            "     |      Examples:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\n",
            "     |      >>> print(rt.to_tensor())\n",
            "     |      tf.Tensor(\n",
            "     |          [[9 8 7] [0 0 0] [6 5 0] [4 0 0]], shape=(4, 3), dtype=int32)\n",
            "     |      >>> print(rt.to_tensor(shape=[5, 2]))\n",
            "     |      tf.Tensor(\n",
            "     |          [[9 8] [0 0] [6 5] [4 0] [0 0]], shape=(5, 2), dtype=int32)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        default_value: Value to set for indices not specified in `self`. Defaults\n",
            "     |          to zero.  `default_value` must be broadcastable to\n",
            "     |          `self.shape[self.ragged_rank + 1:]`.\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |        shape: The shape of the resulting dense tensor.  In particular,\n",
            "     |          `result.shape[i]` is `shape[i]` (if `shape[i]` is not None), or\n",
            "     |          `self.bounding_shape(i)` (otherwise).`shape.rank` must be `None` or\n",
            "     |          equal to `self.rank`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` with shape `ragged.bounding_shape(self)` and the\n",
            "     |        values specified by the non-empty values in `self`.  Empty values are\n",
            "     |        assigned `default_value`.\n",
            "     |  \n",
            "     |  value_rowids(self, name=None)\n",
            "     |      Returns the row indices for the `values` in this ragged tensor.\n",
            "     |      \n",
            "     |      `rt.value_rowids()` corresponds one-to-one with the outermost dimension of\n",
            "     |      `rt.values`, and specifies the row containing each value.  In particular,\n",
            "     |      the row `rt[row]` consists of the values `rt.values[j]` where\n",
            "     |      `rt.value_rowids()[j] == row`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name prefix for the returned tensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 1-D integer `Tensor` with shape `self.values.shape[:1]`.\n",
            "     |        The returned tensor is nonnegative, and is sorted in ascending order.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.values)\n",
            "     |      tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n",
            "     |      >>> print(rt.value_rowids())  # corresponds 1:1 with rt.values\n",
            "     |      tf.Tensor([0 0 0 0 2 2 2 3], shape=(8,), dtype=int64)\n",
            "     |  \n",
            "     |  with_flat_values(self, new_values)\n",
            "     |      Returns a copy of `self` with `flat_values` replaced by `new_value`.\n",
            "     |      \n",
            "     |      Preserves cached row-partitioning tensors such as `self.cached_nrows` and\n",
            "     |      `self.cached_value_rowids` if they have values.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        new_values: Potentially ragged tensor that should replace\n",
            "     |        `self.flat_values`.  Must have `rank > 0`, and must have the same\n",
            "     |        number of rows as `self.flat_values`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.\n",
            "     |        `result.rank = self.ragged_rank + new_values.rank`.\n",
            "     |        `result.ragged_rank = self.ragged_rank + new_values.ragged_rank`.\n",
            "     |  \n",
            "     |  with_row_splits_dtype(self, dtype)\n",
            "     |      Returns a copy of this RaggedTensor with the given `row_splits` dtype.\n",
            "     |      \n",
            "     |      For RaggedTensors with multiple ragged dimensions, the `row_splits` for all\n",
            "     |      nested `RaggedTensor` objects are cast to the given dtype.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        dtype: The dtype for `row_splits`.  One of `tf.int32` or `tf.int64`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A copy of this RaggedTensor, with the `row_splits` cast to the given\n",
            "     |        type.\n",
            "     |  \n",
            "     |  with_values(self, new_values)\n",
            "     |      Returns a copy of `self` with `values` replaced by `new_value`.\n",
            "     |      \n",
            "     |      Preserves cached row-partitioning tensors such as `self.cached_nrows` and\n",
            "     |      `self.cached_value_rowids` if they have values.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        new_values: Potentially ragged tensor to use as the `values` for the\n",
            "     |          returned `RaggedTensor`.  Must have `rank > 0`, and must have the same\n",
            "     |          number of rows as `self.values`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = 1 + new_values.rank`.\n",
            "     |        `result.ragged_rank = 1 + new_values.ragged_rank`\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_nested_row_lengths(flat_values, nested_row_lengths, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` from a nested list of `row_lengths` tensors.\n",
            "     |      \n",
            "     |      Equivalent to:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = flat_values\n",
            "     |      for row_lengths in reversed(nested_row_lengths):\n",
            "     |        result = from_row_lengths(result, row_lengths)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        flat_values: A potentially ragged tensor.\n",
            "     |        nested_row_lengths: A list of 1-D integer tensors.  The `i`th tensor is\n",
            "     |          used as the `row_lengths` for the `i`th ragged dimension.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` (or `flat_values` if `nested_row_lengths` is empty).\n",
            "     |  \n",
            "     |  from_nested_row_splits(flat_values, nested_row_splits, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` from a nested list of `row_splits` tensors.\n",
            "     |      \n",
            "     |      Equivalent to:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = flat_values\n",
            "     |      for row_splits in reversed(nested_row_splits):\n",
            "     |        result = from_row_splits(result, row_splits)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        flat_values: A potentially ragged tensor.\n",
            "     |        nested_row_splits: A list of 1-D integer tensors.  The `i`th tensor is\n",
            "     |          used as the `row_splits` for the `i`th ragged dimension.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` (or `flat_values` if `nested_row_splits` is empty).\n",
            "     |  \n",
            "     |  from_nested_value_rowids(flat_values, nested_value_rowids, nested_nrows=None, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` from a nested list of `value_rowids` tensors.\n",
            "     |      \n",
            "     |      Equivalent to:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = flat_values\n",
            "     |      for (rowids, nrows) in reversed(zip(nested_value_rowids, nested_nrows)):\n",
            "     |        result = from_value_rowids(result, rowids, nrows)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        flat_values: A potentially ragged tensor.\n",
            "     |        nested_value_rowids: A list of 1-D integer tensors.  The `i`th tensor is\n",
            "     |          used as the `value_rowids` for the `i`th ragged dimension.\n",
            "     |        nested_nrows: A list of integer scalars.  The `i`th scalar is used as the\n",
            "     |          `nrows` for the `i`th ragged dimension.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |      \n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` (or `flat_values` if `nested_value_rowids` is empty).\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `len(nested_values_rowids) != len(nested_nrows)`.\n",
            "     |  \n",
            "     |  from_row_lengths(values, row_lengths, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `row_lengths`.\n",
            "     |      \n",
            "     |      The returned `RaggedTensor` corresponds with the python list defined by:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = [[values.pop(0) for i in range(length)]\n",
            "     |                for length in row_lengths]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        row_lengths: A 1-D integer tensor with shape `[nrows]`.  Must be\n",
            "     |          nonnegative.  `sum(row_lengths)` must be `nvals`.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> print(tf.RaggedTensor.from_row_lengths(\n",
            "     |      ...     values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |      ...     row_lengths=[4, 0, 3, 1, 0]))\n",
            "     |      <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  from_row_limits(values, row_limits, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `row_limits`.\n",
            "     |      \n",
            "     |      Equivalent to: `from_row_splits(values, concat([0, row_limits]))`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        row_limits: A 1-D integer tensor with shape `[nrows]`.  Must be sorted in\n",
            "     |          ascending order.  If `nrows>0`, then `row_limits[-1]` must be `nvals`.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> print(tf.RaggedTensor.from_row_limits(\n",
            "     |      ...     values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |      ...     row_limits=[4, 4, 7, 8, 8]))\n",
            "     |      <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  from_row_splits(values, row_splits, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `row_splits`.\n",
            "     |      \n",
            "     |      The returned `RaggedTensor` corresponds with the python list defined by:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = [values[row_splits[i]:row_splits[i + 1]]\n",
            "     |                for i in range(len(row_splits) - 1)]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        row_splits: A 1-D integer tensor with shape `[nrows+1]`.  Must not be\n",
            "     |          empty, and must be sorted in ascending order.  `row_splits[0]` must be\n",
            "     |          zero and `row_splits[-1]` must be `nvals`.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `row_splits` is an empty list.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> print(tf.RaggedTensor.from_row_splits(\n",
            "     |      ...     values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |      ...     row_splits=[0, 4, 4, 7, 8, 8]))\n",
            "     |      <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  from_row_starts(values, row_starts, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `row_starts`.\n",
            "     |      \n",
            "     |      Equivalent to: `from_row_splits(values, concat([row_starts, nvals]))`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        row_starts: A 1-D integer tensor with shape `[nrows]`.  Must be\n",
            "     |          nonnegative and sorted in ascending order.  If `nrows>0`, then\n",
            "     |          `row_starts[0]` must be zero.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> print(tf.RaggedTensor.from_row_starts(\n",
            "     |      ...     values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |      ...     row_starts=[0, 4, 4, 7, 8]))\n",
            "     |      <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  from_sparse(st_input, name=None, row_splits_dtype=tf.int64) from abc.ABCMeta\n",
            "     |      Converts a 2D `tf.SparseTensor` to a `RaggedTensor`.\n",
            "     |      \n",
            "     |      Each row of the `output` `RaggedTensor` will contain the explicit values\n",
            "     |      from the same row in `st_input`.  `st_input` must be ragged-right.  If not\n",
            "     |      it is not ragged-right, then an error will be generated.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      >>> st = tf.SparseTensor(indices=[[0, 0], [0, 1], [0, 2], [1, 0], [3, 0]],\n",
            "     |      ...                      values=[1, 2, 3, 4, 5],\n",
            "     |      ...                      dense_shape=[4, 3])\n",
            "     |      >>> tf.RaggedTensor.from_sparse(st).to_list()\n",
            "     |      [[1, 2, 3], [4], [], [5]]\n",
            "     |      \n",
            "     |      Currently, only two-dimensional `SparseTensors` are supported.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        st_input: The sparse tensor to convert.  Must have rank 2.\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |        row_splits_dtype: `dtype` for the returned `RaggedTensor`'s `row_splits`\n",
            "     |          tensor.  One of `tf.int32` or `tf.int64`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` with the same values as `st_input`.\n",
            "     |        `output.ragged_rank = rank(st_input) - 1`.\n",
            "     |        `output.shape = [st_input.dense_shape[0], None]`.\n",
            "     |      Raises:\n",
            "     |        ValueError: If the number of dimensions in `st_input` is not known\n",
            "     |          statically, or is not two.\n",
            "     |  \n",
            "     |  from_tensor(tensor, lengths=None, padding=None, ragged_rank=1, name=None, row_splits_dtype=tf.int64) from abc.ABCMeta\n",
            "     |      Converts a `tf.Tensor` into a `RaggedTensor`.\n",
            "     |      \n",
            "     |      The set of absent/default values may be specified using a vector of lengths\n",
            "     |      or a padding value (but not both).  If `lengths` is specified, then the\n",
            "     |      output tensor will satisfy `output[row] = tensor[row][:lengths[row]]`. If\n",
            "     |      'lengths' is a list of lists or tuple of lists, those lists will be used\n",
            "     |      as nested row lengths. If `padding` is specified, then any row *suffix*\n",
            "     |      consisting entirely of `padding` will be excluded from the returned\n",
            "     |      `RaggedTensor`.  If neither `lengths` nor `padding` is specified, then the\n",
            "     |      returned `RaggedTensor` will have no absent/default values.\n",
            "     |      \n",
            "     |      Examples:\n",
            "     |      \n",
            "     |      >>> dt = tf.constant([[5, 7, 0], [0, 3, 0], [6, 0, 0]])\n",
            "     |      >>> tf.RaggedTensor.from_tensor(dt)\n",
            "     |      <tf.RaggedTensor [[5, 7, 0], [0, 3, 0], [6, 0, 0]]>\n",
            "     |      >>> tf.RaggedTensor.from_tensor(dt, lengths=[1, 0, 3])\n",
            "     |      <tf.RaggedTensor [[5], [], [6, 0, 0]]>\n",
            "     |      \n",
            "     |      >>> tf.RaggedTensor.from_tensor(dt, padding=0)\n",
            "     |      <tf.RaggedTensor [[5, 7], [0, 3], [6]]>\n",
            "     |      \n",
            "     |      >>> dt = tf.constant([[[5, 0], [7, 0], [0, 0]],\n",
            "     |      ...                   [[0, 0], [3, 0], [0, 0]],\n",
            "     |      ...                   [[6, 0], [0, 0], [0, 0]]])\n",
            "     |      >>> tf.RaggedTensor.from_tensor(dt, lengths=([2, 0, 3], [1, 1, 2, 0, 1]))\n",
            "     |      <tf.RaggedTensor [[[5], [7]], [], [[6, 0], [], [0]]]>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        tensor: The `Tensor` to convert.  Must have rank `ragged_rank + 1` or\n",
            "     |          higher.\n",
            "     |        lengths: An optional set of row lengths, specified using a 1-D integer\n",
            "     |          `Tensor` whose length is equal to `tensor.shape[0]` (the number of rows\n",
            "     |          in `tensor`).  If specified, then `output[row]` will contain\n",
            "     |          `tensor[row][:lengths[row]]`.  Negative lengths are treated as zero. You\n",
            "     |          may optionally pass a list or tuple of lengths to this argument, which\n",
            "     |          will be used as nested row lengths to construct a ragged tensor with\n",
            "     |          multiple ragged dimensions.\n",
            "     |        padding: An optional padding value.  If specified, then any row suffix\n",
            "     |          consisting entirely of `padding` will be excluded from the returned\n",
            "     |          RaggedTensor.  `padding` is a `Tensor` with the same dtype as `tensor`\n",
            "     |          and with `shape=tensor.shape[ragged_rank + 1:]`.\n",
            "     |        ragged_rank: Integer specifying the ragged rank for the returned\n",
            "     |          `RaggedTensor`.  Must be greater than zero.\n",
            "     |        name: A name prefix for the returned tensors (optional).\n",
            "     |        row_splits_dtype: `dtype` for the returned `RaggedTensor`'s `row_splits`\n",
            "     |          tensor.  One of `tf.int32` or `tf.int64`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` with the specified `ragged_rank`.  The shape of the\n",
            "     |        returned ragged tensor is compatible with the shape of `tensor`.\n",
            "     |      Raises:\n",
            "     |        ValueError: If both `lengths` and `padding` are specified.\n",
            "     |  \n",
            "     |  from_uniform_row_length(values, uniform_row_length, nrows=None, validate=True, name=None) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `uniform_row_length`.\n",
            "     |      \n",
            "     |      This method can be used to create `RaggedTensor`s with multiple uniform\n",
            "     |      outer dimensions.  For example, a `RaggedTensor` with shape `[2, 2, None]`\n",
            "     |      can be constructed with this method from a `RaggedTensor` values with shape\n",
            "     |      `[4, None]`:\n",
            "     |      \n",
            "     |      >>> values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\n",
            "     |      >>> print(values.shape)\n",
            "     |      (4, None)\n",
            "     |      >>> rt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\n",
            "     |      >>> print(rt1)\n",
            "     |      <tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\n",
            "     |      >>> print(rt1.shape)\n",
            "     |      (2, 2, None)\n",
            "     |      \n",
            "     |      Note that `rt1` only contains one ragged dimension (the innermost\n",
            "     |      dimension). In contrast, if `from_row_splits` is used to construct a similar\n",
            "     |      `RaggedTensor`, then that `RaggedTensor` will have two ragged dimensions:\n",
            "     |      \n",
            "     |      >>> rt2 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\n",
            "     |      >>> print(rt2.shape)\n",
            "     |      (2, None, None)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        uniform_row_length: A scalar integer tensor.  Must be nonnegative.\n",
            "     |          The size of the outer axis of `values` must be evenly divisible by\n",
            "     |          `uniform_row_length`.\n",
            "     |        nrows: The number of rows in the constructed RaggedTensor.  If not\n",
            "     |          specified, then it defaults to `nvals/uniform_row_length` (or `0` if\n",
            "     |          `uniform_row_length==0`).  `nrows` only needs to be specified if\n",
            "     |          `uniform_row_length` might be zero.  `uniform_row_length*nrows` must\n",
            "     |          be `nvals`.\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor` that corresponds with the python list defined by:\n",
            "     |      \n",
            "     |        ```python\n",
            "     |        result = [[values.pop(0) for i in range(uniform_row_length)]\n",
            "     |                  for _ in range(nrows)]\n",
            "     |        ```\n",
            "     |      \n",
            "     |        `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |  \n",
            "     |  from_value_rowids(values, value_rowids, nrows=None, name=None, validate=True) from abc.ABCMeta\n",
            "     |      Creates a `RaggedTensor` with rows partitioned by `value_rowids`.\n",
            "     |      \n",
            "     |      The returned `RaggedTensor` corresponds with the python list defined by:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      result = [[values[i] for i in range(len(values)) if value_rowids[i] == row]\n",
            "     |                for row in range(nrows)]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        values: A potentially ragged tensor with shape `[nvals, ...]`.\n",
            "     |        value_rowids: A 1-D integer tensor with shape `[nvals]`, which corresponds\n",
            "     |          one-to-one with `values`, and specifies each value's row index.  Must be\n",
            "     |          nonnegative, and must be sorted in ascending order.\n",
            "     |        nrows: An integer scalar specifying the number of rows.  This should be\n",
            "     |          specified if the `RaggedTensor` may containing empty training rows. Must\n",
            "     |          be greater than `value_rowids[-1]` (or zero if `value_rowids` is empty).\n",
            "     |          Defaults to `value_rowids[-1]` (or zero if `value_rowids` is empty).\n",
            "     |        name: A name prefix for the RaggedTensor (optional).\n",
            "     |        validate: If true, then use assertions to check that the arguments form\n",
            "     |          a valid `RaggedTensor`.  Note: these assertions incur a runtime cost,\n",
            "     |          since they must be checked for each tensor value.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `RaggedTensor`.  `result.rank = values.rank + 1`.\n",
            "     |        `result.ragged_rank = values.ragged_rank + 1`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `nrows` is incompatible with `value_rowids`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> print(tf.RaggedTensor.from_value_rowids(\n",
            "     |      ...     values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
            "     |      ...     value_rowids=[0, 0, 0, 0, 2, 2, 2, 3],\n",
            "     |      ...     nrows=5))\n",
            "     |      <tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `DType` of values in this tensor.\n",
            "     |  \n",
            "     |  flat_values\n",
            "     |      The innermost `values` tensor for this ragged tensor.\n",
            "     |      \n",
            "     |      Concretely, if `rt.values` is a `Tensor`, then `rt.flat_values` is\n",
            "     |      `rt.values`; otherwise, `rt.flat_values` is `rt.values.flat_values`.\n",
            "     |      \n",
            "     |      Conceptually, `flat_values` is the tensor formed by flattening the\n",
            "     |      outermost dimension and all of the ragged dimensions into a single\n",
            "     |      dimension.\n",
            "     |      \n",
            "     |      `rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]`\n",
            "     |      (where `nvals` is the number of items in the flattened dimensions).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\n",
            "     |      >>> print(rt.flat_values)\n",
            "     |      tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n",
            "     |  \n",
            "     |  nested_row_splits\n",
            "     |      A tuple containing the row_splits for all ragged dimensions.\n",
            "     |      \n",
            "     |      `rt.nested_row_splits` is a tuple containing the `row_splits` tensors for\n",
            "     |      all ragged dimensions in `rt`, ordered from outermost to innermost.  In\n",
            "     |      particular, `rt.nested_row_splits = (rt.row_splits,) + value_splits` where:\n",
            "     |      \n",
            "     |          * `value_splits = ()` if `rt.values` is a `Tensor`.\n",
            "     |          * `value_splits = rt.values.nested_row_splits` otherwise.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tuple` of 1-D integer `Tensor`s.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant(\n",
            "     |      ...     [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\n",
            "     |      >>> for i, splits in enumerate(rt.nested_row_splits):\n",
            "     |      ...   print('Splits for dimension %d: %s' % (i+1, splits.numpy()))\n",
            "     |      Splits for dimension 1: [0 3]\n",
            "     |      Splits for dimension 2: [0 3 3 5]\n",
            "     |      Splits for dimension 3: [0 4 4 7 8 8]\n",
            "     |  \n",
            "     |  ragged_rank\n",
            "     |      The number of ragged dimensions in this ragged tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A Python `int` indicating the number of ragged dimensions in this ragged\n",
            "     |        tensor.  The outermost dimension is not considered ragged.\n",
            "     |  \n",
            "     |  row_splits\n",
            "     |      The row-split indices for this ragged tensor's `values`.\n",
            "     |      \n",
            "     |      `rt.row_splits` specifies where the values for each row begin and end in\n",
            "     |      `rt.values`.  In particular, the values for row `rt[i]` are stored in\n",
            "     |      the slice `rt.values[rt.row_splits[i]:rt.row_splits[i+1]]`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 1-D integer `Tensor` with shape `[self.nrows+1]`.\n",
            "     |        The returned tensor is non-empty, and is sorted in ascending order.\n",
            "     |        `self.row_splits[0]` is zero, and `self.row_splits[-1]` is equal to\n",
            "     |        `self.values.shape[0]`.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.row_splits)  # indices of row splits in rt.values\n",
            "     |      tf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)\n",
            "     |  \n",
            "     |  shape\n",
            "     |      The statically known shape of this ragged tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` containing the statically known shape of this ragged\n",
            "     |        tensor.  Ragged dimensions have a size of `None`.\n",
            "     |      \n",
            "     |      Examples:\n",
            "     |      \n",
            "     |      >>> tf.ragged.constant([[0], [1, 2]]).shape\n",
            "     |      TensorShape([2, None])\n",
            "     |      \n",
            "     |      >>> tf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).shape\n",
            "     |      TensorShape([2, None, 2])\n",
            "     |  \n",
            "     |  uniform_row_length\n",
            "     |      The length of each row in this ragged tensor, or None if rows are ragged.\n",
            "     |      \n",
            "     |      >>> rt1 = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\n",
            "     |      >>> print(rt1.uniform_row_length)  # rows are ragged.\n",
            "     |      None\n",
            "     |      \n",
            "     |      >>> rt2 = tf.RaggedTensor.from_uniform_row_length(\n",
            "     |      ...     values=rt1, uniform_row_length=2)\n",
            "     |      >>> print(rt2)\n",
            "     |      <tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>\n",
            "     |      >>> print(rt2.uniform_row_length)  # rows are not ragged (all have size 2).\n",
            "     |      tf.Tensor(2, shape=(), dtype=int64)\n",
            "     |      \n",
            "     |      A RaggedTensor's rows are only considered to be uniform (i.e. non-ragged)\n",
            "     |      if it can be determined statically (at graph construction time) that the\n",
            "     |      rows all have the same length.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A scalar integer `Tensor`, specifying the length of every row in this\n",
            "     |        ragged tensor (for ragged tensors whose rows are uniform); or `None`\n",
            "     |        (for ragged tensors whose rows are ragged).\n",
            "     |  \n",
            "     |  values\n",
            "     |      The concatenated rows for this ragged tensor.\n",
            "     |      \n",
            "     |      `rt.values` is a potentially ragged tensor formed by flattening the two\n",
            "     |      outermost dimensions of `rt` into a single dimension.\n",
            "     |      \n",
            "     |      `rt.values.shape = [nvals] + rt.shape[2:]` (where `nvals` is the\n",
            "     |      number of items in the outer two dimensions of `rt`).\n",
            "     |      \n",
            "     |      `rt.ragged_rank = self.ragged_rank - 1`\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A potentially ragged tensor.\n",
            "     |      \n",
            "     |      #### Example:\n",
            "     |      \n",
            "     |      >>> rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
            "     |      >>> print(rt.values)\n",
            "     |      tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.framework.composite_tensor.CompositeTensor:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class RaggedTensorSpec(tensorflow.python.framework.type_spec.BatchableTypeSpec)\n",
            "     |  Type specification for a `tf.RaggedTensor`.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      RaggedTensorSpec\n",
            "     |      tensorflow.python.framework.type_spec.BatchableTypeSpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, shape=None, dtype=tf.float32, ragged_rank=None, row_splits_dtype=tf.int64)\n",
            "     |      Constructs a type specification for a `tf.RaggedTensor`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: The shape of the RaggedTensor, or `None` to allow any shape.  If\n",
            "     |          a shape is specified, then all ragged dimensions must have size `None`.\n",
            "     |        dtype: `tf.DType` of values in the RaggedTensor.\n",
            "     |        ragged_rank: Python integer, the ragged rank of the RaggedTensor\n",
            "     |          to be described.  Defaults to `shape.ndims - 1`.\n",
            "     |        row_splits_dtype: `dtype` for the RaggedTensor's `row_splits` tensor.\n",
            "     |          One of `tf.int32` or `tf.int64`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_value(value) from abc.ABCMeta\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |      The Python type for values that are compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_value)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "    \n",
            "    class RegisterGradient(builtins.object)\n",
            "     |  A decorator for registering the gradient function for an op type.\n",
            "     |  \n",
            "     |  This decorator is only used when defining a new op type. For an op\n",
            "     |  with `m` inputs and `n` outputs, the gradient function is a function\n",
            "     |  that takes the original `Operation` and `n` `Tensor` objects\n",
            "     |  (representing the gradients with respect to each output of the op),\n",
            "     |  and returns `m` `Tensor` objects (representing the partial gradients\n",
            "     |  with respect to each input of the op).\n",
            "     |  \n",
            "     |  For example, assuming that operations of type `\"Sub\"` take two\n",
            "     |  inputs `x` and `y`, and return a single output `x - y`, the\n",
            "     |  following gradient function would be registered:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  @tf.RegisterGradient(\"Sub\")\n",
            "     |  def _sub_grad(unused_op, grad):\n",
            "     |    return grad, tf.negative(grad)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  The decorator argument `op_type` is the string type of an\n",
            "     |  operation. This corresponds to the `OpDef.name` field for the proto\n",
            "     |  that defines the operation.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, f)\n",
            "     |      Registers the function `f` as gradient function for `op_type`.\n",
            "     |  \n",
            "     |  __init__(self, op_type)\n",
            "     |      Creates a new decorator with `op_type` as the Operation type.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        op_type: The string type of an operation. This corresponds to the\n",
            "     |          `OpDef.name` field for the proto that defines the operation.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If `op_type` is not string.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class SparseTensor(tensorflow.python.framework.tensor_like._TensorLike, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
            "     |  Represents a sparse tensor.\n",
            "     |  \n",
            "     |  TensorFlow represents a sparse tensor as three separate dense tensors:\n",
            "     |  `indices`, `values`, and `dense_shape`.  In Python, the three tensors are\n",
            "     |  collected into a `SparseTensor` class for ease of use.  If you have separate\n",
            "     |  `indices`, `values`, and `dense_shape` tensors, wrap them in a `SparseTensor`\n",
            "     |  object before passing to the ops below.\n",
            "     |  \n",
            "     |  Concretely, the sparse tensor `SparseTensor(indices, values, dense_shape)`\n",
            "     |  comprises the following components, where `N` and `ndims` are the number\n",
            "     |  of values and number of dimensions in the `SparseTensor`, respectively:\n",
            "     |  \n",
            "     |  * `indices`: A 2-D int64 tensor of shape `[N, ndims]`, which specifies the\n",
            "     |    indices of the elements in the sparse tensor that contain nonzero values\n",
            "     |    (elements are zero-indexed). For example, `indices=[[1,3], [2,4]]` specifies\n",
            "     |    that the elements with indexes of [1,3] and [2,4] have nonzero values.\n",
            "     |  \n",
            "     |  * `values`: A 1-D tensor of any type and shape `[N]`, which supplies the\n",
            "     |    values for each element in `indices`. For example, given `indices=[[1,3],\n",
            "     |    [2,4]]`, the parameter `values=[18, 3.6]` specifies that element [1,3] of\n",
            "     |    the sparse tensor has a value of 18, and element [2,4] of the tensor has a\n",
            "     |    value of 3.6.\n",
            "     |  \n",
            "     |  * `dense_shape`: A 1-D int64 tensor of shape `[ndims]`, which specifies the\n",
            "     |    dense_shape of the sparse tensor. Takes a list indicating the number of\n",
            "     |    elements in each dimension. For example, `dense_shape=[3,6]` specifies a\n",
            "     |    two-dimensional 3x6 tensor, `dense_shape=[2,3,4]` specifies a\n",
            "     |    three-dimensional 2x3x4 tensor, and `dense_shape=[9]` specifies a\n",
            "     |    one-dimensional tensor with 9 elements.\n",
            "     |  \n",
            "     |  The corresponding dense tensor satisfies:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  dense.shape = dense_shape\n",
            "     |  dense[tuple(indices[i])] = values[i]\n",
            "     |  ```\n",
            "     |  \n",
            "     |  By convention, `indices` should be sorted in row-major order (or equivalently\n",
            "     |  lexicographic order on the tuples `indices[i]`). This is not enforced when\n",
            "     |  `SparseTensor` objects are constructed, but most ops assume correct ordering.\n",
            "     |  If the ordering of sparse tensor `st` is wrong, a fixed version can be\n",
            "     |  obtained by calling `tf.sparse.reorder(st)`.\n",
            "     |  \n",
            "     |  Example: The sparse tensor\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
            "     |  ```\n",
            "     |  \n",
            "     |  represents the dense tensor\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  [[1, 0, 0, 0]\n",
            "     |   [0, 0, 2, 0]\n",
            "     |   [0, 0, 0, 0]]\n",
            "     |  ```\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      SparseTensor\n",
            "     |      tensorflow.python.framework.tensor_like._TensorLike\n",
            "     |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __div__ = binary_op_wrapper_sparse(sp_x, y)\n",
            "     |      Component-wise divides a SparseTensor by a dense Tensor.\n",
            "     |      \n",
            "     |      *Limitation*: this Op only broadcasts the dense side to the sparse side, but not\n",
            "     |      the other direction.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sp_indices: A `Tensor` of type `int64`.\n",
            "     |          2-D.  `N x R` matrix with the indices of non-empty values in a\n",
            "     |          SparseTensor, possibly not in canonical ordering.\n",
            "     |        sp_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
            "     |          1-D.  `N` non-empty values corresponding to `sp_indices`.\n",
            "     |        sp_shape: A `Tensor` of type `int64`.\n",
            "     |          1-D.  Shape of the input SparseTensor.\n",
            "     |        dense: A `Tensor`. Must have the same type as `sp_values`.\n",
            "     |          `R`-D.  The dense Tensor operand.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `sp_values`.\n",
            "     |  \n",
            "     |  __init__(self, indices, values, dense_shape)\n",
            "     |      Creates a `SparseTensor`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: A 2-D int64 tensor of shape `[N, ndims]`.\n",
            "     |        values: A 1-D tensor of any type and shape `[N]`.\n",
            "     |        dense_shape: A 1-D int64 tensor of shape `[ndims]`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: When building an eager SparseTensor if `dense_shape` is\n",
            "     |          unknown or contains unknown elements (None or -1).\n",
            "     |  \n",
            "     |  __mul__ = binary_op_wrapper_sparse(sp_x, y)\n",
            "     |      Component-wise multiplies a SparseTensor by a dense Tensor.\n",
            "     |      \n",
            "     |      The output locations corresponding to the implicitly zero elements in the sparse\n",
            "     |      tensor will be zero (i.e., will not take up storage space), regardless of the\n",
            "     |      contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).\n",
            "     |      \n",
            "     |      *Limitation*: this Op only broadcasts the dense side to the sparse side, but not\n",
            "     |      the other direction.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sp_indices: A `Tensor` of type `int64`.\n",
            "     |          2-D.  `N x R` matrix with the indices of non-empty values in a\n",
            "     |          SparseTensor, possibly not in canonical ordering.\n",
            "     |        sp_values: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
            "     |          1-D.  `N` non-empty values corresponding to `sp_indices`.\n",
            "     |        sp_shape: A `Tensor` of type `int64`.\n",
            "     |          1-D.  Shape of the input SparseTensor.\n",
            "     |        dense: A `Tensor`. Must have the same type as `sp_values`.\n",
            "     |          `R`-D.  The dense Tensor operand.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `sp_values`.\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  __truediv__ = binary_op_wrapper_sparse(sp_x, y)\n",
            "     |      Internal helper function for 'sp_t / dense_t'.\n",
            "     |  \n",
            "     |  consumers(self)\n",
            "     |  \n",
            "     |  eval(self, feed_dict=None, session=None)\n",
            "     |      Evaluates this sparse tensor in a `Session`.\n",
            "     |      \n",
            "     |      Calling this method will execute all preceding operations that\n",
            "     |      produce the inputs needed for the operation that produces this\n",
            "     |      tensor.\n",
            "     |      \n",
            "     |      *N.B.* Before invoking `SparseTensor.eval()`, its graph must have been\n",
            "     |      launched in a session, and either a default session must be\n",
            "     |      available, or `session` must be specified explicitly.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        feed_dict: A dictionary that maps `Tensor` objects to feed values. See\n",
            "     |          `tf.Session.run` for a description of the valid feed values.\n",
            "     |        session: (Optional.) The `Session` to be used to evaluate this sparse\n",
            "     |          tensor. If none, the default session will be used.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `SparseTensorValue` object.\n",
            "     |  \n",
            "     |  get_shape(self)\n",
            "     |      Get the `TensorShape` representing the shape of the dense tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` object.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_value(sparse_tensor_value) from abc.ABCMeta\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  dense_shape\n",
            "     |      A 1-D Tensor of int64 representing the shape of the dense tensor.\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `DType` of elements in this tensor.\n",
            "     |  \n",
            "     |  graph\n",
            "     |      The `Graph` that contains the index, value, and dense_shape tensors.\n",
            "     |  \n",
            "     |  indices\n",
            "     |      The indices of non-zero values in the represented dense tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 2-D Tensor of int64 with dense_shape `[N, ndims]`, where `N` is the\n",
            "     |          number of non-zero values in the tensor, and `ndims` is the rank.\n",
            "     |  \n",
            "     |  op\n",
            "     |      The `Operation` that produces `values` as an output.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      Get the `TensorShape` representing the shape of the dense tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` object.\n",
            "     |  \n",
            "     |  values\n",
            "     |      The non-zero values in the represented dense tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A 1-D Tensor of any data type.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.framework.tensor_like._TensorLike:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class SparseTensorSpec(tensorflow.python.framework.type_spec.BatchableTypeSpec)\n",
            "     |  Type specification for a `tf.SparseTensor`.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      SparseTensorSpec\n",
            "     |      tensorflow.python.framework.type_spec.BatchableTypeSpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, shape=None, dtype=tf.float32)\n",
            "     |      Constructs a type specification for a `tf.SparseTensor`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: The dense shape of the `SparseTensor`, or `None` to allow\n",
            "     |          any dense shape.\n",
            "     |        dtype: `tf.DType` of values in the `SparseTensor`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_value(value) from abc.ABCMeta\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `tf.dtypes.DType` specified by this type for the SparseTensor.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      The `tf.TensorShape` specified by this type for the SparseTensor.\n",
            "     |  \n",
            "     |  value_type\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_value)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "    \n",
            "    class Tensor(tensorflow.python.framework.tensor_like._TensorLike)\n",
            "     |  A tensor represents a rectangular array of data.\n",
            "     |  \n",
            "     |  When writing a TensorFlow program, the main object you manipulate and pass\n",
            "     |  around is the `tf.Tensor`. A `tf.Tensor` object represents a rectangular array\n",
            "     |  of arbitrary dimension, filled with data of a specific data type.\n",
            "     |  \n",
            "     |  A `tf.Tensor` has the following properties:\n",
            "     |  \n",
            "     |  * a data type (float32, int32, or string, for example)\n",
            "     |  * a shape\n",
            "     |  \n",
            "     |  Each element in the Tensor has the same data type, and the data type is always\n",
            "     |  known.\n",
            "     |  \n",
            "     |  In eager execution, which is the default mode in TensorFlow, results are\n",
            "     |  calculated immediately.\n",
            "     |  \n",
            "     |  >>> # Compute some values using a Tensor\n",
            "     |  >>> c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
            "     |  >>> d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
            "     |  >>> e = tf.matmul(c, d)\n",
            "     |  >>> print(e)\n",
            "     |  tf.Tensor(\n",
            "     |  [[1. 3.]\n",
            "     |   [3. 7.]], shape=(2, 2), dtype=float32)\n",
            "     |  \n",
            "     |  \n",
            "     |  Note that during eager execution, you may discover your `Tensors` are actually\n",
            "     |  of type `EagerTensor`.  This is an internal detail, but it does give you\n",
            "     |  access to a useful function, `numpy`:\n",
            "     |  \n",
            "     |  >>> type(e)\n",
            "     |  <class '...ops.EagerTensor'>\n",
            "     |  >>> print(e.numpy())\n",
            "     |    [[1. 3.]\n",
            "     |     [3. 7.]]\n",
            "     |  \n",
            "     |  TensorFlow can define computations without immediately executing them, most\n",
            "     |  commonly inside `tf.function`s, as well as in (legacy) Graph mode. In those\n",
            "     |  cases, the shape (that is, the rank of the Tensor and the size of\n",
            "     |  each dimension) might be only partially known.\n",
            "     |  \n",
            "     |  Most operations produce tensors of fully-known shapes if the shapes of their\n",
            "     |  inputs are also fully known, but in some cases it's only possible to find the\n",
            "     |  shape of a tensor at execution time.\n",
            "     |  \n",
            "     |  There are specialized tensors; for these, see `tf.Variable`, `tf.constant`,\n",
            "     |  `tf.placeholder`, `tf.SparseTensor`, and `tf.RaggedTensor`.\n",
            "     |  \n",
            "     |  For more on Tensors, see the [guide](https://tensorflow.org/guide/tensor`).\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Tensor\n",
            "     |      tensorflow.python.framework.tensor_like._TensorLike\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __abs__ = abs(x, name=None)\n",
            "     |      Computes the absolute value of a tensor.\n",
            "     |      \n",
            "     |      Given a tensor of integer or floating-point values, this operation returns a\n",
            "     |      tensor of the same type, where each element contains the absolute value of the\n",
            "     |      corresponding element in the input.\n",
            "     |      \n",
            "     |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
            "     |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
            "     |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
            "     |      + b^2}\\\\).  For example:\n",
            "     |      \n",
            "     |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
            "     |      >>> tf.abs(x)\n",
            "     |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
            "     |      array([[5.25594901],\n",
            "     |             [6.60492241]])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
            "     |          `int32`, `int64`, `complex64` or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
            "     |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
            "     |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
            "     |      \n",
            "     |        If `x` is a `SparseTensor`, returns\n",
            "     |        `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
            "     |  \n",
            "     |  __add__ = binary_op_wrapper(x, y)\n",
            "     |      Dispatches to add for strings and add_v2 for all other types.\n",
            "     |  \n",
            "     |  __and__ = binary_op_wrapper(x, y)\n",
            "     |      Returns the truth value of x AND y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __array__(self)\n",
            "     |  \n",
            "     |  __bool__(self)\n",
            "     |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
            "     |      \n",
            "     |      This overload raises a `TypeError` when the user inadvertently\n",
            "     |      treats a `Tensor` as a boolean (most commonly in an `if` or `while`\n",
            "     |      statement), in code that was not converted by AutoGraph. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      if tf.constant(True):  # Will raise.\n",
            "     |        # ...\n",
            "     |      \n",
            "     |      if tf.constant(5) < tf.constant(7):  # Will raise.\n",
            "     |        # ...\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        `TypeError`.\n",
            "     |  \n",
            "     |  __copy__(self)\n",
            "     |  \n",
            "     |  __div__ = binary_op_wrapper(x, y)\n",
            "     |      Divide two values using Python 2 semantics.\n",
            "     |      \n",
            "     |      Used for Tensor.__div__.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __eq__ = tensor_equals(self, other)\n",
            "     |      Compares two tensors element-wise for equality.\n",
            "     |  \n",
            "     |  __floordiv__ = binary_op_wrapper(x, y)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __ge__ = greater_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x >= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5, 2, 5, 10])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __getitem__ = _slice_helper(tensor, slice_spec, var=None)\n",
            "     |      Overload for Tensor.__getitem__.\n",
            "     |      \n",
            "     |      This operation extracts the specified region from the tensor.\n",
            "     |      The notation is similar to NumPy with the restriction that\n",
            "     |      currently only support basic indexing. That means that\n",
            "     |      using a non-scalar tensor as input is not currently allowed.\n",
            "     |      \n",
            "     |      Some useful examples:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      # Strip leading and trailing 2 elements\n",
            "     |      foo = tf.constant([1,2,3,4,5,6])\n",
            "     |      print(foo[2:-2].eval())  # => [3,4]\n",
            "     |      \n",
            "     |      # Skip every other row and reverse the order of the columns\n",
            "     |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
            "     |      print(foo[::2,::-1].eval())  # => [[3,2,1], [9,8,7]]\n",
            "     |      \n",
            "     |      # Use scalar tensors as indices on both dimensions\n",
            "     |      print(foo[tf.constant(0), tf.constant(2)].eval())  # => 3\n",
            "     |      \n",
            "     |      # Insert another dimension\n",
            "     |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
            "     |      print(foo[tf.newaxis, :, :].eval()) # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
            "     |      print(foo[:, tf.newaxis, :].eval()) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\n",
            "     |      print(foo[:, :, tf.newaxis].eval()) # => [[[1],[2],[3]], [[4],[5],[6]],\n",
            "     |      [[7],[8],[9]]]\n",
            "     |      \n",
            "     |      # Ellipses (3 equivalent operations)\n",
            "     |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
            "     |      print(foo[tf.newaxis, :, :].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
            "     |      print(foo[tf.newaxis, ...].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
            "     |      print(foo[tf.newaxis].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
            "     |      \n",
            "     |      # Masks\n",
            "     |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
            "     |      print(foo[foo > 2].eval())  # => [3, 4, 5, 6, 7, 8, 9]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Notes:\n",
            "     |        - `tf.newaxis` is `None` as in NumPy.\n",
            "     |        - An implicit ellipsis is placed at the end of the `slice_spec`\n",
            "     |        - NumPy advanced indexing is currently not supported.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        tensor: An ops.Tensor object.\n",
            "     |        slice_spec: The arguments to Tensor.__getitem__.\n",
            "     |        var: In the case of variable slice assignment, the Variable object to slice\n",
            "     |          (i.e. tensor is the read-only view of this variable).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If a slice range is negative size.\n",
            "     |        TypeError: If the slice indices aren't int, slice, ellipsis,\n",
            "     |          tf.newaxis or scalar int32/int64 tensors.\n",
            "     |  \n",
            "     |  __gt__ = greater(x, y, name=None)\n",
            "     |      Returns the truth value of (x > y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 2, 5])\n",
            "     |      tf.math.greater(x, y) ==> [False, True, True]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater(x, y) ==> [False, False, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __init__(self, op, value_index, dtype)\n",
            "     |      Creates a new `Tensor`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        op: An `Operation`. `Operation` that computes this tensor.\n",
            "     |        value_index: An `int`. Index of the operation's endpoint that produces\n",
            "     |          this tensor.\n",
            "     |        dtype: A `DType`. Type of elements stored in this tensor.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the op is not an `Operation`.\n",
            "     |  \n",
            "     |  __invert__ = logical_not(x, name=None)\n",
            "     |      Returns the truth value of `NOT x` element-wise.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
            "     |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __iter__(self)\n",
            "     |  \n",
            "     |  __le__ = less_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x <= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 6])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __len__(self)\n",
            "     |  \n",
            "     |  __lt__ = less(x, y, name=None)\n",
            "     |      Returns the truth value of (x < y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less(x, y) ==> [False, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 7])\n",
            "     |      tf.math.less(x, y) ==> [False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __matmul__ = binary_op_wrapper(x, y)\n",
            "     |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
            "     |      \n",
            "     |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
            "     |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
            "     |      and any further outer dimensions specify matching batch size.\n",
            "     |      \n",
            "     |      Both matrices must be of the same type. The supported types are:\n",
            "     |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
            "     |      \n",
            "     |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
            "     |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
            "     |      by default.\n",
            "     |      \n",
            "     |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
            "     |      multiplication algorithm can be used by setting the corresponding\n",
            "     |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
            "     |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
            "     |      datatypes `bfloat16` or `float32`.\n",
            "     |      \n",
            "     |      A simple 2-D tensor matrix multiplication:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "     |      >>> a  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "     |      array([[1, 2, 3],\n",
            "     |             [4, 5, 6]], dtype=int32)>\n",
            "     |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
            "     |      >>> b  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "     |      array([[ 7,  8],\n",
            "     |             [ 9, 10],\n",
            "     |             [11, 12]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "     |      array([[ 58,  64],\n",
            "     |             [139, 154]], dtype=int32)>\n",
            "     |      \n",
            "     |      A batch matrix multiplication with batch shape [2]:\n",
            "     |      \n",
            "     |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
            "     |      >>> a  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
            "     |      array([[[ 1,  2,  3],\n",
            "     |              [ 4,  5,  6]],\n",
            "     |             [[ 7,  8,  9],\n",
            "     |              [10, 11, 12]]], dtype=int32)>\n",
            "     |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
            "     |      >>> b  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "     |      array([[[13, 14],\n",
            "     |              [15, 16],\n",
            "     |              [17, 18]],\n",
            "     |             [[19, 20],\n",
            "     |              [21, 22],\n",
            "     |              [23, 24]]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
            "     |      array([[[ 94, 100],\n",
            "     |              [229, 244]],\n",
            "     |             [[508, 532],\n",
            "     |              [697, 730]]], dtype=int32)>\n",
            "     |      \n",
            "     |      Since python >= 3.5 the @ operator is supported\n",
            "     |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
            "     |      it simply calls the `tf.matmul()` function, so the following lines are\n",
            "     |      equivalent:\n",
            "     |      \n",
            "     |      >>> d = a @ b @ [[10], [11]]\n",
            "     |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
            "     |          `complex64`, `complex128` and rank > 1.\n",
            "     |        b: `tf.Tensor` with same type and rank as `a`.\n",
            "     |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
            "     |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
            "     |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        name: Name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
            "     |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
            "     |        transpose or adjoint attributes are `False`:\n",
            "     |      \n",
            "     |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
            "     |        for all indices `i`, `j`.\n",
            "     |      \n",
            "     |        Note: This is matrix product, not element-wise product.\n",
            "     |      \n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
            "     |          `adjoint_b` are both set to `True`.\n",
            "     |  \n",
            "     |  __mod__ = binary_op_wrapper(x, y)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __mul__ = binary_op_wrapper(x, y)\n",
            "     |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
            "     |  \n",
            "     |  __ne__ = tensor_not_equals(self, other)\n",
            "     |      Compares two tensors element-wise for equality.\n",
            "     |  \n",
            "     |  __neg__ = neg(x, name=None)\n",
            "     |      Computes numerical negative value element-wise.\n",
            "     |      \n",
            "     |      I.e., \\\\(y = -x\\\\).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |      \n",
            "     |        If `x` is a `SparseTensor`, returns\n",
            "     |        `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
            "     |  \n",
            "     |  __nonzero__(self)\n",
            "     |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
            "     |      \n",
            "     |      This is the Python 2.x counterpart to `__bool__()` above.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        `TypeError`.\n",
            "     |  \n",
            "     |  __or__ = binary_op_wrapper(x, y)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __pow__ = binary_op_wrapper(x, y)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __radd__ = r_binary_op_wrapper(y, x)\n",
            "     |      Dispatches to add for strings and add_v2 for all other types.\n",
            "     |  \n",
            "     |  __rand__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns the truth value of x AND y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
            "     |      Divide two values using Python 2 semantics.\n",
            "     |      \n",
            "     |      Used for Tensor.__div__.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
            "     |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
            "     |      \n",
            "     |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
            "     |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
            "     |      and any further outer dimensions specify matching batch size.\n",
            "     |      \n",
            "     |      Both matrices must be of the same type. The supported types are:\n",
            "     |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
            "     |      \n",
            "     |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
            "     |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
            "     |      by default.\n",
            "     |      \n",
            "     |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
            "     |      multiplication algorithm can be used by setting the corresponding\n",
            "     |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
            "     |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
            "     |      datatypes `bfloat16` or `float32`.\n",
            "     |      \n",
            "     |      A simple 2-D tensor matrix multiplication:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "     |      >>> a  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "     |      array([[1, 2, 3],\n",
            "     |             [4, 5, 6]], dtype=int32)>\n",
            "     |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
            "     |      >>> b  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "     |      array([[ 7,  8],\n",
            "     |             [ 9, 10],\n",
            "     |             [11, 12]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "     |      array([[ 58,  64],\n",
            "     |             [139, 154]], dtype=int32)>\n",
            "     |      \n",
            "     |      A batch matrix multiplication with batch shape [2]:\n",
            "     |      \n",
            "     |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
            "     |      >>> a  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
            "     |      array([[[ 1,  2,  3],\n",
            "     |              [ 4,  5,  6]],\n",
            "     |             [[ 7,  8,  9],\n",
            "     |              [10, 11, 12]]], dtype=int32)>\n",
            "     |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
            "     |      >>> b  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "     |      array([[[13, 14],\n",
            "     |              [15, 16],\n",
            "     |              [17, 18]],\n",
            "     |             [[19, 20],\n",
            "     |              [21, 22],\n",
            "     |              [23, 24]]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
            "     |      array([[[ 94, 100],\n",
            "     |              [229, 244]],\n",
            "     |             [[508, 532],\n",
            "     |              [697, 730]]], dtype=int32)>\n",
            "     |      \n",
            "     |      Since python >= 3.5 the @ operator is supported\n",
            "     |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
            "     |      it simply calls the `tf.matmul()` function, so the following lines are\n",
            "     |      equivalent:\n",
            "     |      \n",
            "     |      >>> d = a @ b @ [[10], [11]]\n",
            "     |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
            "     |          `complex64`, `complex128` and rank > 1.\n",
            "     |        b: `tf.Tensor` with same type and rank as `a`.\n",
            "     |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
            "     |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
            "     |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        name: Name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
            "     |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
            "     |        transpose or adjoint attributes are `False`:\n",
            "     |      \n",
            "     |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
            "     |        for all indices `i`, `j`.\n",
            "     |      \n",
            "     |        Note: This is matrix product, not element-wise product.\n",
            "     |      \n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
            "     |          `adjoint_b` are both set to `True`.\n",
            "     |  \n",
            "     |  __rmod__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rmul__ = r_binary_op_wrapper(y, x)\n",
            "     |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
            "     |  \n",
            "     |  __ror__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __rpow__ = r_binary_op_wrapper(y, x)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __rsub__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
            "     |  \n",
            "     |  __rxor__ = r_binary_op_wrapper(y, x)\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  __sub__ = binary_op_wrapper(x, y)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __truediv__ = binary_op_wrapper(x, y)\n",
            "     |  \n",
            "     |  __xor__ = binary_op_wrapper(x, y)\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  consumers(self)\n",
            "     |      Returns a list of `Operation`s that consume this tensor.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list of `Operation`s.\n",
            "     |  \n",
            "     |  eval(self, feed_dict=None, session=None)\n",
            "     |      Evaluates this tensor in a `Session`.\n",
            "     |      \n",
            "     |      Note: If you are not using `compat.v1` libraries, you should not need this,\n",
            "     |      (or `feed_dict` or `Session`).  In eager execution (or within `tf.function`)\n",
            "     |      you do not need to call `eval`.\n",
            "     |      \n",
            "     |      Calling this method will execute all preceding operations that\n",
            "     |      produce the inputs needed for the operation that produces this\n",
            "     |      tensor.\n",
            "     |      \n",
            "     |      *N.B.* Before invoking `Tensor.eval()`, its graph must have been\n",
            "     |      launched in a session, and either a default session must be\n",
            "     |      available, or `session` must be specified explicitly.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        feed_dict: A dictionary that maps `Tensor` objects to feed values. See\n",
            "     |          `tf.Session.run` for a description of the valid feed values.\n",
            "     |        session: (Optional.) The `Session` to be used to evaluate this tensor. If\n",
            "     |          none, the default session will be used.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A numpy array corresponding to the value of this tensor.\n",
            "     |  \n",
            "     |  experimental_ref(self)\n",
            "     |      DEPRECATED FUNCTION\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Use ref() instead.\n",
            "     |  \n",
            "     |  get_shape(self)\n",
            "     |      Alias of `tf.Tensor.shape`.\n",
            "     |  \n",
            "     |  ref(self)\n",
            "     |      Returns a hashable reference object to this Tensor.\n",
            "     |      \n",
            "     |      The primary use case for this API is to put tensors in a set/dictionary.\n",
            "     |      We can't put tensors in a set/dictionary as `tensor.__hash__()` is no longer\n",
            "     |      available starting Tensorflow 2.0.\n",
            "     |      \n",
            "     |      The following will raise an exception starting 2.0\n",
            "     |      \n",
            "     |      >>> x = tf.constant(5)\n",
            "     |      >>> y = tf.constant(10)\n",
            "     |      >>> z = tf.constant(10)\n",
            "     |      >>> tensor_set = {x, y, z}\n",
            "     |      Traceback (most recent call last):\n",
            "     |        ...\n",
            "     |      TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n",
            "     |      >>> tensor_dict = {x: 'five', y: 'ten'}\n",
            "     |      Traceback (most recent call last):\n",
            "     |        ...\n",
            "     |      TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n",
            "     |      \n",
            "     |      Instead, we can use `tensor.ref()`.\n",
            "     |      \n",
            "     |      >>> tensor_set = {x.ref(), y.ref(), z.ref()}\n",
            "     |      >>> x.ref() in tensor_set\n",
            "     |      True\n",
            "     |      >>> tensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\n",
            "     |      >>> tensor_dict[y.ref()]\n",
            "     |      'ten'\n",
            "     |      \n",
            "     |      Also, the reference object provides `.deref()` function that returns the\n",
            "     |      original Tensor.\n",
            "     |      \n",
            "     |      >>> x = tf.constant(5)\n",
            "     |      >>> x.ref().deref()\n",
            "     |      <tf.Tensor: shape=(), dtype=int32, numpy=5>\n",
            "     |  \n",
            "     |  set_shape(self, shape)\n",
            "     |      Updates the shape of this tensor.\n",
            "     |      \n",
            "     |      This method can be called multiple times, and will merge the given\n",
            "     |      `shape` with the current shape of this tensor. It can be used to\n",
            "     |      provide additional information about the shape of this tensor that\n",
            "     |      cannot be inferred from the graph alone. For example, this can be used\n",
            "     |      to provide additional information about the shapes of images:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      _, image_data = tf.compat.v1.TFRecordReader(...).read(...)\n",
            "     |      image = tf.image.decode_png(image_data, channels=3)\n",
            "     |      \n",
            "     |      # The height and width dimensions of `image` are data dependent, and\n",
            "     |      # cannot be computed without executing the op.\n",
            "     |      print(image.shape)\n",
            "     |      ==> TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n",
            "     |      \n",
            "     |      # We know that each image in this dataset is 28 x 28 pixels.\n",
            "     |      image.set_shape([28, 28, 3])\n",
            "     |      print(image.shape)\n",
            "     |      ==> TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n",
            "     |      ```\n",
            "     |      \n",
            "     |      NOTE: This shape is not enforced at runtime. Setting incorrect shapes can\n",
            "     |      result in inconsistencies between the statically-known graph and the runtime\n",
            "     |      value of tensors. For runtime validation of the shape, use `tf.ensure_shape`\n",
            "     |      instead.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: A `TensorShape` representing the shape of this tensor, a\n",
            "     |          `TensorShapeProto`, a list, a tuple, or None.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `shape` is not compatible with the current shape of\n",
            "     |          this tensor.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  device\n",
            "     |      The name of the device on which this tensor will be produced, or None.\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `DType` of elements in this tensor.\n",
            "     |  \n",
            "     |  graph\n",
            "     |      The `Graph` that contains this tensor.\n",
            "     |  \n",
            "     |  name\n",
            "     |      The string name of this tensor.\n",
            "     |  \n",
            "     |  op\n",
            "     |      The `Operation` that produces this tensor as an output.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      Returns the `TensorShape` that represents the shape of this tensor.\n",
            "     |      \n",
            "     |      The shape is computed using shape inference functions that are\n",
            "     |      registered in the Op for each `Operation`.  See\n",
            "     |      `tf.TensorShape`\n",
            "     |      for more details of what a shape represents.\n",
            "     |      \n",
            "     |      The inferred shape of a tensor is used to provide shape\n",
            "     |      information without having to execute the underlying kernel. This\n",
            "     |      can be used for debugging and providing early error messages. For\n",
            "     |      example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      >>> c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
            "     |      >>> print(c.shape) # will be TensorShape([2, 3])\n",
            "     |      (2, 3)\n",
            "     |      \n",
            "     |      >>> d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
            "     |      >>> print(d.shape)\n",
            "     |      (4, 2)\n",
            "     |      \n",
            "     |      # Raises a ValueError, because `c` and `d` do not have compatible\n",
            "     |      # inner dimensions.\n",
            "     |      >>> e = tf.matmul(c, d)\n",
            "     |      Traceback (most recent call last):\n",
            "     |          ...\n",
            "     |      tensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix\n",
            "     |      size-incompatible: In[0]: [2,3], In[1]: [4,2] [Op:MatMul] name: MatMul/\n",
            "     |      \n",
            "     |      # This works because we have compatible shapes.\n",
            "     |      >>> f = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n",
            "     |      >>> print(f.shape)\n",
            "     |      (3, 4)\n",
            "     |      \n",
            "     |      ```\n",
            "     |      \n",
            "     |      In some cases, the inferred shape may have unknown dimensions. If\n",
            "     |      the caller has additional information about the values of these\n",
            "     |      dimensions, `Tensor.set_shape()` can be used to augment the\n",
            "     |      inferred shape.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.TensorShape` representing the shape of this tensor.\n",
            "     |  \n",
            "     |  value_index\n",
            "     |      The index of this tensor in the outputs of its `Operation`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  OVERLOADABLE_OPERATORS = {'__abs__', '__add__', '__and__', '__div__', ...\n",
            "     |  \n",
            "     |  __array_priority__ = 100\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.framework.tensor_like._TensorLike:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class TensorArray(builtins.object)\n",
            "     |  Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.\n",
            "     |  \n",
            "     |  This class is meant to be used with dynamic iteration primitives such as\n",
            "     |  `while_loop` and `map_fn`.  It supports gradient back-propagation via special\n",
            "     |  \"flow\" control flow dependencies.\n",
            "     |  \n",
            "     |  Example 1: Plain reading and writing.\n",
            "     |  \n",
            "     |  >>> ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n",
            "     |  >>> ta = ta.write(0, 10)\n",
            "     |  >>> ta = ta.write(1, 20)\n",
            "     |  >>> ta = ta.write(2, 30)\n",
            "     |  >>>\n",
            "     |  >>> ta.read(0)\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=10.0>\n",
            "     |  >>> ta.read(1)\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=20.0>\n",
            "     |  >>> ta.read(2)\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=30.0>\n",
            "     |  >>> ta.stack()\n",
            "     |  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],\n",
            "     |  dtype=float32)>\n",
            "     |  \n",
            "     |  Example 2: Fibonacci sequence algorithm that writes in a loop then returns.\n",
            "     |  \n",
            "     |  >>> @tf.function\n",
            "     |  ... def fibonacci(n):\n",
            "     |  ...   ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
            "     |  ...   ta = ta.unstack([0., 1.])\n",
            "     |  ...\n",
            "     |  ...   for i in range(2, n):\n",
            "     |  ...     ta = ta.write(i, ta.read(i - 1) + ta.read(i - 2))\n",
            "     |  ...\n",
            "     |  ...   return ta.stack()\n",
            "     |  >>>\n",
            "     |  >>> fibonacci(7)\n",
            "     |  <tf.Tensor: shape=(7,), dtype=float32,\n",
            "     |  numpy=array([0., 1., 1., 2., 3., 5., 8.], dtype=float32)>\n",
            "     |  \n",
            "     |  Example 3: A simple loop interacting with a `tf.Variable`.\n",
            "     |  \n",
            "     |  >>> v = tf.Variable(1)\n",
            "     |  >>>\n",
            "     |  >>> @tf.function\n",
            "     |  ... def f(x):\n",
            "     |  ...   ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n",
            "     |  ...\n",
            "     |  ...   for i in tf.range(x):\n",
            "     |  ...     v.assign_add(i)\n",
            "     |  ...     ta = ta.write(i, v)\n",
            "     |  ...\n",
            "     |  ...   return ta.stack()\n",
            "     |  >>>\n",
            "     |  >>> f(5)\n",
            "     |  <tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 1,  2,  4,  7, 11],\n",
            "     |  dtype=int32)>\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, dtype, size=None, dynamic_size=None, clear_after_read=None, tensor_array_name=None, handle=None, flow=None, infer_shape=True, element_shape=None, colocate_with_first_write_call=True, name=None)\n",
            "     |      Construct a new TensorArray or wrap an existing TensorArray handle.\n",
            "     |      \n",
            "     |      A note about the parameter `name`:\n",
            "     |      \n",
            "     |      The name of the `TensorArray` (even if passed in) is uniquified: each time\n",
            "     |      a new `TensorArray` is created at runtime it is assigned its own name for\n",
            "     |      the duration of the run.  This avoids name collisions if a `TensorArray`\n",
            "     |      is created within a `while_loop`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        dtype: (required) data type of the TensorArray.\n",
            "     |        size: (optional) int32 scalar `Tensor`: the size of the TensorArray.\n",
            "     |          Required if handle is not provided.\n",
            "     |        dynamic_size: (optional) Python bool: If true, writes to the TensorArray\n",
            "     |          can grow the TensorArray past its initial size.  Default: False.\n",
            "     |        clear_after_read: Boolean (optional, default: True).  If True, clear\n",
            "     |          TensorArray values after reading them.  This disables read-many\n",
            "     |          semantics, but allows early release of memory.\n",
            "     |        tensor_array_name: (optional) Python string: the name of the TensorArray.\n",
            "     |          This is used when creating the TensorArray handle.  If this value is\n",
            "     |          set, handle should be None.\n",
            "     |        handle: (optional) A `Tensor` handle to an existing TensorArray.  If this\n",
            "     |          is set, tensor_array_name should be None. Only supported in graph mode.\n",
            "     |        flow: (optional) A float `Tensor` scalar coming from an existing\n",
            "     |          `TensorArray.flow`. Only supported in graph mode.\n",
            "     |        infer_shape: (optional, default: True) If True, shape inference\n",
            "     |          is enabled.  In this case, all elements must have the same shape.\n",
            "     |        element_shape: (optional, default: None) A `TensorShape` object specifying\n",
            "     |          the shape constraints of each of the elements of the TensorArray.\n",
            "     |          Need not be fully defined.\n",
            "     |        colocate_with_first_write_call: If `True`, the TensorArray will be\n",
            "     |          colocated on the same device as the Tensor used on its first write\n",
            "     |          (write operations include `write`, `unstack`, and `split`).  If `False`,\n",
            "     |          the TensorArray will be placed on the device determined by the\n",
            "     |          device context available during its initialization.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: if both handle and tensor_array_name are provided.\n",
            "     |        TypeError: if handle is provided but is not a Tensor.\n",
            "     |  \n",
            "     |  close(self, name=None)\n",
            "     |      Close the current TensorArray.\n",
            "     |      \n",
            "     |      **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "     |  \n",
            "     |  concat(self, name=None)\n",
            "     |      Return the values in the TensorArray as a concatenated `Tensor`.\n",
            "     |      \n",
            "     |      All of the values must have been written, their ranks must match, and\n",
            "     |      and their shapes must all match for all dimensions except the first.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        All the tensors in the TensorArray concatenated into one tensor.\n",
            "     |  \n",
            "     |  gather(self, indices, name=None)\n",
            "     |      Return selected values in the TensorArray as a packed `Tensor`.\n",
            "     |      \n",
            "     |      All of selected values must have been written and their shapes\n",
            "     |      must all match.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: A `1-D` `Tensor` taking values in `[0, max_value)`.  If\n",
            "     |          the `TensorArray` is not dynamic, `max_value=size()`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The tensors in the `TensorArray` selected by `indices`, packed into one\n",
            "     |        tensor.\n",
            "     |  \n",
            "     |  grad(self, source, flow=None, name=None)\n",
            "     |  \n",
            "     |  identity(self)\n",
            "     |      Returns a TensorArray with the same content and properties.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A new TensorArray object with flow that ensures the control dependencies\n",
            "     |        from the contexts will become control dependencies for writes, reads, etc.\n",
            "     |        Use this object all for subsequent operations.\n",
            "     |  \n",
            "     |  read(self, index, name=None)\n",
            "     |      Read the value at location `index` in the TensorArray.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        index: 0-D.  int32 tensor with the index to read from.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The tensor at index `index`.\n",
            "     |  \n",
            "     |  scatter(self, indices, value, name=None)\n",
            "     |      Scatter the values of a `Tensor` in specific indices of a `TensorArray`.\n",
            "     |      \n",
            "     |        Args:\n",
            "     |          indices: A `1-D` `Tensor` taking values in `[0, max_value)`.  If\n",
            "     |            the `TensorArray` is not dynamic, `max_value=size()`.\n",
            "     |          value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to unpack.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |        Returns:\n",
            "     |          A new TensorArray object with flow that ensures the scatter occurs.\n",
            "     |          Use this object all for subsequent operations.\n",
            "     |      \n",
            "     |        Raises:\n",
            "     |          ValueError: if the shape inference fails.\n",
            "     |        \n",
            "     |      \n",
            "     |      **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "     |  \n",
            "     |  size(self, name=None)\n",
            "     |      Return the size of the TensorArray.\n",
            "     |  \n",
            "     |  split(self, value, lengths, name=None)\n",
            "     |      Split the values of a `Tensor` into the TensorArray.\n",
            "     |      \n",
            "     |        Args:\n",
            "     |          value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to split.\n",
            "     |          lengths: 1-D.  int32 vector with the lengths to use when splitting\n",
            "     |            `value` along its first dimension.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |        Returns:\n",
            "     |          A new TensorArray object with flow that ensures the split occurs.\n",
            "     |          Use this object all for subsequent operations.\n",
            "     |      \n",
            "     |        Raises:\n",
            "     |          ValueError: if the shape inference fails.\n",
            "     |        \n",
            "     |      \n",
            "     |      **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "     |  \n",
            "     |  stack(self, name=None)\n",
            "     |      Return the values in the TensorArray as a stacked `Tensor`.\n",
            "     |      \n",
            "     |      All of the values must have been written and their shapes must all match.\n",
            "     |      If input shapes have rank-`R`, then output shape will have rank-`(R+1)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        All the tensors in the TensorArray stacked into one tensor.\n",
            "     |  \n",
            "     |  unstack(self, value, name=None)\n",
            "     |      Unstack the values of a `Tensor` in the TensorArray.\n",
            "     |      \n",
            "     |        If input value shapes have rank-`R`, then the output TensorArray will\n",
            "     |        contain elements whose shapes are rank-`(R-1)`.\n",
            "     |      \n",
            "     |        Args:\n",
            "     |          value: (N+1)-D.  Tensor of type `dtype`.  The Tensor to unstack.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |        Returns:\n",
            "     |          A new TensorArray object with flow that ensures the unstack occurs.\n",
            "     |          Use this object all for subsequent operations.\n",
            "     |      \n",
            "     |        Raises:\n",
            "     |          ValueError: if the shape inference fails.\n",
            "     |        \n",
            "     |      \n",
            "     |      **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "     |  \n",
            "     |  write(self, index, value, name=None)\n",
            "     |      Write `value` into index `index` of the TensorArray.\n",
            "     |      \n",
            "     |        Args:\n",
            "     |          index: 0-D.  int32 scalar with the index to write to.\n",
            "     |          value: N-D.  Tensor of type `dtype`.  The Tensor to write to this index.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |        Returns:\n",
            "     |          A new TensorArray object with flow that ensures the write occurs.\n",
            "     |          Use this object all for subsequent operations.\n",
            "     |      \n",
            "     |        Raises:\n",
            "     |          ValueError: if there are more writers than specified.\n",
            "     |        \n",
            "     |      \n",
            "     |      **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The data type of this TensorArray.\n",
            "     |  \n",
            "     |  dynamic_size\n",
            "     |      Python bool; if `True` the TensorArray can grow dynamically.\n",
            "     |  \n",
            "     |  element_shape\n",
            "     |      The `tf.TensorShape` of elements in this TensorArray.\n",
            "     |  \n",
            "     |  flow\n",
            "     |      The flow `Tensor` forcing ops leading to this TensorArray state.\n",
            "     |  \n",
            "     |  handle\n",
            "     |      The reference to the TensorArray.\n",
            "    \n",
            "    class TensorArraySpec(tensorflow.python.framework.type_spec.TypeSpec)\n",
            "     |  Type specification for a `tf.TensorArray`.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      TensorArraySpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, element_shape=None, dtype=tf.float32, dynamic_size=False, infer_shape=True)\n",
            "     |      Constructs a type specification for a `tf.TensorArray`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        element_shape: The shape of each element in the `TensorArray`.\n",
            "     |        dtype: Data type of the `TensorArray`.\n",
            "     |        dynamic_size: Whether the `TensorArray` can grow past its initial size.\n",
            "     |        infer_shape: Whether shape inference is enabled.\n",
            "     |  \n",
            "     |  is_compatible_with(self, other)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods defined here:\n",
            "     |  \n",
            "     |  from_value(value)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "    \n",
            "    class TensorShape(builtins.object)\n",
            "     |  Represents the shape of a `Tensor`.\n",
            "     |  \n",
            "     |  A `TensorShape` represents a possibly-partial shape specification for a\n",
            "     |  `Tensor`. It may be one of the following:\n",
            "     |  \n",
            "     |  * *Fully-known shape:* has a known number of dimensions and a known size\n",
            "     |    for each dimension. e.g. `TensorShape([16, 256])`\n",
            "     |  * *Partially-known shape:* has a known number of dimensions, and an unknown\n",
            "     |    size for one or more dimension. e.g. `TensorShape([None, 256])`\n",
            "     |  * *Unknown shape:* has an unknown number of dimensions, and an unknown\n",
            "     |    size in all dimensions. e.g. `TensorShape(None)`\n",
            "     |  \n",
            "     |  If a tensor is produced by an operation of type `\"Foo\"`, its shape\n",
            "     |  may be inferred if there is a registered shape function for\n",
            "     |  `\"Foo\"`. See [Shape\n",
            "     |  functions](https://tensorflow.org/extend/adding_an_op#shape_functions_in_c)\n",
            "     |  for details of shape functions and how to register them. Alternatively,\n",
            "     |  the shape may be set explicitly using `tf.Tensor.set_shape`.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __add__(self, other)\n",
            "     |  \n",
            "     |  __bool__(self)\n",
            "     |      Returns True if this shape contains non-zero information.\n",
            "     |  \n",
            "     |  __concat__(self, other)\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Returns True if `self` is equivalent to `other`.\n",
            "     |  \n",
            "     |  __getitem__(self, key)\n",
            "     |      Returns the value of a dimension or a shape, depending on the key.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        key: If `key` is an integer, returns the dimension at that index;\n",
            "     |          otherwise if `key` is a slice, returns a TensorShape whose dimensions\n",
            "     |          are those selected by the slice from `self`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An integer if `key` is an integer, or a `TensorShape` if `key` is a\n",
            "     |        slice.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `key` is a slice and `self` is completely unknown and\n",
            "     |          the step is set.\n",
            "     |  \n",
            "     |  __init__(self, dims)\n",
            "     |      Creates a new TensorShape with the given dimensions.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        dims: A list of Dimensions, or None if the shape is unspecified.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If dims cannot be converted to a list of dimensions.\n",
            "     |  \n",
            "     |  __iter__(self)\n",
            "     |      Returns `self.dims` if the rank is known, otherwise raises ValueError.\n",
            "     |  \n",
            "     |  __len__(self)\n",
            "     |      Returns the rank of this shape, or raises ValueError if unspecified.\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Returns True if `self` is known to be different from `other`.\n",
            "     |  \n",
            "     |  __nonzero__ = __bool__(self)\n",
            "     |  \n",
            "     |  __radd__(self, other)\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  as_list(self)\n",
            "     |      Returns a list of integers or `None` for each dimension.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list of integers or `None` for each dimension.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` is an unknown shape with an unknown rank.\n",
            "     |  \n",
            "     |  as_proto(self)\n",
            "     |      Returns this shape as a `TensorShapeProto`.\n",
            "     |  \n",
            "     |  assert_has_rank(self, rank)\n",
            "     |      Raises an exception if `self` is not compatible with the given `rank`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        rank: An integer.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` does not represent a shape with the given `rank`.\n",
            "     |  \n",
            "     |  assert_is_compatible_with(self, other)\n",
            "     |      Raises exception if `self` and `other` do not represent the same shape.\n",
            "     |      \n",
            "     |      This method can be used to assert that there exists a shape that both\n",
            "     |      `self` and `other` represent.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another TensorShape.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` and `other` do not represent the same shape.\n",
            "     |  \n",
            "     |  assert_is_fully_defined(self)\n",
            "     |      Raises an exception if `self` is not fully defined in every dimension.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` does not have a known value for every dimension.\n",
            "     |  \n",
            "     |  assert_same_rank(self, other)\n",
            "     |      Raises an exception if `self` and `other` do not have compatible ranks.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another `TensorShape`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` and `other` do not represent shapes with the\n",
            "     |          same rank.\n",
            "     |  \n",
            "     |  concatenate(self, other)\n",
            "     |      Returns the concatenation of the dimension in `self` and `other`.\n",
            "     |      \n",
            "     |      *N.B.* If either `self` or `other` is completely unknown,\n",
            "     |      concatenation will discard information about the other shape. In\n",
            "     |      future, we might support concatenation that preserves this\n",
            "     |      information for use with slicing.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another `TensorShape`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` whose dimensions are the concatenation of the\n",
            "     |        dimensions in `self` and `other`.\n",
            "     |  \n",
            "     |  is_compatible_with(self, other)\n",
            "     |      Returns True iff `self` is compatible with `other`.\n",
            "     |      \n",
            "     |      Two possibly-partially-defined shapes are compatible if there\n",
            "     |      exists a fully-defined shape that both shapes can represent. Thus,\n",
            "     |      compatibility allows the shape inference code to reason about\n",
            "     |      partially-defined shapes. For example:\n",
            "     |      \n",
            "     |      * TensorShape(None) is compatible with all shapes.\n",
            "     |      \n",
            "     |      * TensorShape([None, None]) is compatible with all two-dimensional\n",
            "     |        shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is\n",
            "     |        not compatible with, for example, TensorShape([None]) or\n",
            "     |        TensorShape([None, None, None]).\n",
            "     |      \n",
            "     |      * TensorShape([32, None]) is compatible with all two-dimensional shapes\n",
            "     |        with size 32 in the 0th dimension, and also TensorShape([None, None])\n",
            "     |        and TensorShape(None). It is not compatible with, for example,\n",
            "     |        TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]).\n",
            "     |      \n",
            "     |      * TensorShape([32, 784]) is compatible with itself, and also\n",
            "     |        TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None,\n",
            "     |        None]) and TensorShape(None). It is not compatible with, for example,\n",
            "     |        TensorShape([32, 1, 784]) or TensorShape([None]).\n",
            "     |      \n",
            "     |      The compatibility relation is reflexive and symmetric, but not\n",
            "     |      transitive. For example, TensorShape([32, 784]) is compatible with\n",
            "     |      TensorShape(None), and TensorShape(None) is compatible with\n",
            "     |      TensorShape([4, 4]), but TensorShape([32, 784]) is not compatible with\n",
            "     |      TensorShape([4, 4]).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another TensorShape.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        True iff `self` is compatible with `other`.\n",
            "     |  \n",
            "     |  is_fully_defined(self)\n",
            "     |      Returns True iff `self` is fully defined in every dimension.\n",
            "     |  \n",
            "     |  merge_with(self, other)\n",
            "     |      Returns a `TensorShape` combining the information in `self` and `other`.\n",
            "     |      \n",
            "     |      The dimensions in `self` and `other` are merged elementwise,\n",
            "     |      according to the rules defined for `Dimension.merge_with()`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another `TensorShape`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` containing the combined information of `self` and\n",
            "     |        `other`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` and `other` are not compatible.\n",
            "     |  \n",
            "     |  most_specific_compatible_shape(self, other)\n",
            "     |      Returns the most specific TensorShape compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      * TensorShape([None, 1]) is the most specific TensorShape compatible with\n",
            "     |        both TensorShape([2, 1]) and TensorShape([5, 1]). Note that\n",
            "     |        TensorShape(None) is also compatible with above mentioned TensorShapes.\n",
            "     |      \n",
            "     |      * TensorShape([1, 2, 3]) is the most specific TensorShape compatible with\n",
            "     |        both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more\n",
            "     |        less specific TensorShapes compatible with above mentioned TensorShapes,\n",
            "     |        e.g. TensorShape([1, 2, None]), TensorShape(None).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: Another `TensorShape`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape` which is the most specific compatible shape of `self`\n",
            "     |        and `other`.\n",
            "     |  \n",
            "     |  num_elements(self)\n",
            "     |      Returns the total number of elements, or none for incomplete shapes.\n",
            "     |  \n",
            "     |  with_rank(self, rank)\n",
            "     |      Returns a shape based on `self` with the given rank.\n",
            "     |      \n",
            "     |      This method promotes a completely unknown shape to one with a\n",
            "     |      known rank.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        rank: An integer.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A shape that is at least as specific as `self` with the given rank.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` does not represent a shape with the given `rank`.\n",
            "     |  \n",
            "     |  with_rank_at_least(self, rank)\n",
            "     |      Returns a shape based on `self` with at least the given rank.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        rank: An integer.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A shape that is at least as specific as `self` with at least the given\n",
            "     |        rank.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` does not represent a shape with at least the given\n",
            "     |          `rank`.\n",
            "     |  \n",
            "     |  with_rank_at_most(self, rank)\n",
            "     |      Returns a shape based on `self` with at most the given rank.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        rank: An integer.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A shape that is at least as specific as `self` with at most the given\n",
            "     |        rank.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `self` does not represent a shape with at most the given\n",
            "     |          `rank`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  dims\n",
            "     |      Deprecated.  Returns list of dimensions for this shape.\n",
            "     |      \n",
            "     |      Suggest `TensorShape.as_list` instead.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A list containing `tf.compat.v1.Dimension`s, or None if the shape is\n",
            "     |        unspecified.\n",
            "     |  \n",
            "     |  ndims\n",
            "     |      Deprecated accessor for `rank`.\n",
            "     |  \n",
            "     |  rank\n",
            "     |      Returns the rank of this shape, or None if it is unspecified.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __hash__ = None\n",
            "    \n",
            "    class TensorSpec(DenseSpec, tensorflow.python.framework.type_spec.BatchableTypeSpec)\n",
            "     |  Describes a tf.Tensor.\n",
            "     |  \n",
            "     |  Metadata for describing the `tf.Tensor` objects accepted or returned\n",
            "     |  by some TensorFlow APIs.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      TensorSpec\n",
            "     |      DenseSpec\n",
            "     |      tensorflow.python.framework.type_spec.BatchableTypeSpec\n",
            "     |      tensorflow.python.framework.type_spec.TypeSpec\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_tensor)\n",
            "     |      Returns True if spec_or_tensor is compatible with this TensorSpec.\n",
            "     |      \n",
            "     |      Two tensors are considered compatible if they have the same dtype\n",
            "     |      and their shapes are compatible (see `tf.TensorShape.is_compatible_with`).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        spec_or_tensor: A tf.TensorSpec or a tf.Tensor\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        True if spec_or_tensor is compatible with self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  from_tensor(tensor, name=None) from abc.ABCMeta\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from DenseSpec:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __init__(self, shape, dtype=tf.float32, name=None)\n",
            "     |      Creates a TensorSpec.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Value convertible to `tf.TensorShape`. The shape of the tensor.\n",
            "     |        dtype: Value convertible to `tf.DType`. The type of the tensor values.\n",
            "     |        name: Optional name for the Tensor.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If shape is not convertible to a `tf.TensorShape`, or dtype is\n",
            "     |          not convertible to a `tf.DType`.\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from DenseSpec:\n",
            "     |  \n",
            "     |  from_spec(spec, name=None) from abc.ABCMeta\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from DenseSpec:\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      Returns the `dtype` of elements in the tensor.\n",
            "     |  \n",
            "     |  name\n",
            "     |      Returns the (optionally provided) name of the described tensor.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      Returns the `TensorShape` that represents the shape of the tensor.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from tensorflow.python.framework.type_spec.TypeSpec:\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "    \n",
            "    class TypeSpec(builtins.object)\n",
            "     |  Specifies a TensorFlow value type.\n",
            "     |  \n",
            "     |  A `tf.TypeSpec` provides metadata describing an object accepted or returned\n",
            "     |  by TensorFlow APIs.  Concrete subclasses, such as `tf.TensorSpec` and\n",
            "     |  `tf.RaggedTensorSpec`, are used to describe different value types.\n",
            "     |  \n",
            "     |  For example, `tf.function`'s `input_signature` argument accepts a list\n",
            "     |  (or nested structure) of `TypeSpec`s.\n",
            "     |  \n",
            "     |  Creating new subclasses of TypeSpec (outside of TensorFlow core) is not\n",
            "     |  currently supported.  In particular, we may make breaking changes to the\n",
            "     |  private methods and properties defined by this base class.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __reduce__(self)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  is_compatible_with(self, spec_or_value)\n",
            "     |      Returns true if `spec_or_value` is compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  most_specific_compatible_type(self, other)\n",
            "     |      Returns the most specific TypeSpec compatible with `self` and `other`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        other: A `TypeSpec`.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If there is no TypeSpec that is compatible with both `self`\n",
            "     |          and `other`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  value_type\n",
            "     |      The Python type for values that are compatible with this TypeSpec.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset({'_component_specs', '_from_components...\n",
            "    \n",
            "    class UnconnectedGradients(enum.Enum)\n",
            "     |  Controls how gradient computation behaves when y does not depend on x.\n",
            "     |  \n",
            "     |  The gradient of y with respect to x can be zero in two different ways: there\n",
            "     |  could be no differentiable path in the graph connecting x to y (and so we can\n",
            "     |  statically prove that the gradient is zero) or it could be that runtime values\n",
            "     |  of tensors in a particular execution lead to a gradient of zero (say, if a\n",
            "     |  relu unit happens to not be activated). To allow you to distinguish between\n",
            "     |  these two cases you can choose what value gets returned for the gradient when\n",
            "     |  there is no path in the graph from x to y:\n",
            "     |  \n",
            "     |  * `NONE`: Indicates that [None] will be returned if there is no path from x\n",
            "     |    to y\n",
            "     |  * `ZERO`: Indicates that a zero tensor will be returned in the shape of x.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      UnconnectedGradients\n",
            "     |      enum.Enum\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  NONE = <UnconnectedGradients.NONE: 'none'>\n",
            "     |  \n",
            "     |  ZERO = <UnconnectedGradients.ZERO: 'zero'>\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.Enum:\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of the Enum member.\n",
            "     |  \n",
            "     |  value\n",
            "     |      The value of the Enum member.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.EnumMeta:\n",
            "     |  \n",
            "     |  __members__\n",
            "     |      Returns a mapping of member name->value.\n",
            "     |      \n",
            "     |      This mapping lists all enum members, including aliases. Note that this\n",
            "     |      is a read-only view of the internal mapping.\n",
            "    \n",
            "    class Variable(tensorflow.python.training.tracking.base.Trackable)\n",
            "     |  See the [variable guide](https://tensorflow.org/guide/variable).\n",
            "     |  \n",
            "     |  A variable maintains shared, persistent state manipulated by a program.\n",
            "     |  \n",
            "     |  The `Variable()` constructor requires an initial value for the variable, which\n",
            "     |  can be a `Tensor` of any type and shape. This initial value defines the type\n",
            "     |  and shape of the variable. After construction, the type and shape of the\n",
            "     |  variable are fixed. The value can be changed using one of the assign methods.\n",
            "     |  \n",
            "     |  >>> v = tf.Variable(1.)\n",
            "     |  >>> v.assign(2.)\n",
            "     |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
            "     |  >>> v.assign_add(0.5)\n",
            "     |  <tf.Variable ... shape=() dtype=float32, numpy=2.5>\n",
            "     |  \n",
            "     |  The `shape` argument to `Variable`'s constructor allows you to construct a\n",
            "     |  variable with a less defined shape than its `initial_value`:\n",
            "     |  \n",
            "     |  >>> v = tf.Variable(1., shape=tf.TensorShape(None))\n",
            "     |  >>> v.assign([[1.]])\n",
            "     |  <tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>\n",
            "     |  \n",
            "     |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
            "     |  inputs to operations. Additionally, all the operators overloaded for the\n",
            "     |  `Tensor` class are carried over to variables.\n",
            "     |  \n",
            "     |  >>> w = tf.Variable([[1.], [2.]])\n",
            "     |  >>> x = tf.constant([[3., 4.]])\n",
            "     |  >>> tf.matmul(w, x)\n",
            "     |  <tf.Tensor:... shape=(2, 2), ... numpy=\n",
            "     |    array([[3., 4.],\n",
            "     |           [6., 8.]], dtype=float32)>\n",
            "     |  >>> tf.sigmoid(w + x)\n",
            "     |  <tf.Tensor:... shape=(2, 2), ...>\n",
            "     |  \n",
            "     |  When building a machine learning model it is often convenient to distinguish\n",
            "     |  between variables holding trainable model parameters and other variables such\n",
            "     |  as a `step` variable used to count training steps. To make this easier, the\n",
            "     |  variable constructor supports a `trainable=<bool>`\n",
            "     |  parameter. `tf.GradientTape` watches trainable variables by default:\n",
            "     |  \n",
            "     |  >>> with tf.GradientTape(persistent=True) as tape:\n",
            "     |  ...   trainable = tf.Variable(1.)\n",
            "     |  ...   non_trainable = tf.Variable(2., trainable=False)\n",
            "     |  ...   x1 = trainable * 2.\n",
            "     |  ...   x2 = non_trainable * 3.\n",
            "     |  >>> tape.gradient(x1, trainable)\n",
            "     |  <tf.Tensor:... shape=(), dtype=float32, numpy=2.0>\n",
            "     |  >>> assert tape.gradient(x2, non_trainable) is None  # Unwatched\n",
            "     |  \n",
            "     |  Variables are automatically tracked when assigned to attributes of types\n",
            "     |  inheriting from `tf.Module`.\n",
            "     |  \n",
            "     |  >>> m = tf.Module()\n",
            "     |  >>> m.v = tf.Variable([1.])\n",
            "     |  >>> m.trainable_variables\n",
            "     |  (<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)\n",
            "     |  \n",
            "     |  This tracking then allows saving variable values to\n",
            "     |  [training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to\n",
            "     |  [SavedModels](https://www.tensorflow.org/guide/saved_model) which include\n",
            "     |  serialized TensorFlow graphs.\n",
            "     |  \n",
            "     |  Variables are often captured and manipulated by `tf.function`s. This works the\n",
            "     |  same way the un-decorated function would have:\n",
            "     |  \n",
            "     |  >>> v = tf.Variable(0.)\n",
            "     |  >>> read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n",
            "     |  >>> read_and_decrement()\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\n",
            "     |  >>> read_and_decrement()\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n",
            "     |  \n",
            "     |  Variables created inside a `tf.function` must be owned outside the function\n",
            "     |  and be created only once:\n",
            "     |  \n",
            "     |  >>> class M(tf.Module):\n",
            "     |  ...   @tf.function\n",
            "     |  ...   def __call__(self, x):\n",
            "     |  ...     if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n",
            "     |  ...       self.v = tf.Variable(x)\n",
            "     |  ...     return self.v * x\n",
            "     |  >>> m = M()\n",
            "     |  >>> m(2.)\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n",
            "     |  >>> m(3.)\n",
            "     |  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n",
            "     |  >>> m.v\n",
            "     |  <tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
            "     |  \n",
            "     |  See the `tf.function` documentation for details.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Variable\n",
            "     |      tensorflow.python.training.tracking.base.Trackable\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __abs__ = abs(x, name=None)\n",
            "     |      Computes the absolute value of a tensor.\n",
            "     |      \n",
            "     |      Given a tensor of integer or floating-point values, this operation returns a\n",
            "     |      tensor of the same type, where each element contains the absolute value of the\n",
            "     |      corresponding element in the input.\n",
            "     |      \n",
            "     |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
            "     |      `float32` or `float64` that is the absolute value of each element in `x`. For\n",
            "     |      a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
            "     |      + b^2}\\\\).  For example:\n",
            "     |      \n",
            "     |      >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
            "     |      >>> tf.abs(x)\n",
            "     |      <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
            "     |      array([[5.25594901],\n",
            "     |             [6.60492241]])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
            "     |          `int32`, `int64`, `complex64` or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
            "     |          with absolute values. Note, for `complex64` or `complex128` input, the\n",
            "     |          returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
            "     |  \n",
            "     |  __add__ = binary_op_wrapper(x, y)\n",
            "     |      Dispatches to add for strings and add_v2 for all other types.\n",
            "     |  \n",
            "     |  __and__ = binary_op_wrapper(x, y)\n",
            "     |      Returns the truth value of x AND y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __div__ = binary_op_wrapper(x, y)\n",
            "     |      Divide two values using Python 2 semantics.\n",
            "     |      \n",
            "     |      Used for Tensor.__div__.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Compares two variables element-wise for equality.\n",
            "     |  \n",
            "     |  __floordiv__ = binary_op_wrapper(x, y)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __ge__ = greater_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x >= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5, 2, 5, 10])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6, 7])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
            "     |      Creates a slice helper object given a variable.\n",
            "     |      \n",
            "     |      This allows creating a sub-tensor from part of the current contents\n",
            "     |      of a variable. See `tf.Tensor.__getitem__` for detailed examples\n",
            "     |      of slicing.\n",
            "     |      \n",
            "     |      This function in addition also allows assignment to a sliced range.\n",
            "     |      This is similar to `__setitem__` functionality in Python. However,\n",
            "     |      the syntax is different so that the user can capture the assignment\n",
            "     |      operation for grouping or passing to `sess.run()`.\n",
            "     |      For example,\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      import tensorflow as tf\n",
            "     |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
            "     |      with tf.compat.v1.Session() as sess:\n",
            "     |        sess.run(tf.compat.v1.global_variables_initializer())\n",
            "     |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
            "     |      \n",
            "     |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
            "     |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Note that assignments currently do not support NumPy broadcasting\n",
            "     |      semantics.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        var: An `ops.Variable` object.\n",
            "     |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
            "     |        As an operator. The operator also has a `assign()` method\n",
            "     |        that can be used to generate an assignment operator.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If a slice range is negative size.\n",
            "     |        TypeError: TypeError: If the slice indices aren't int, slice,\n",
            "     |          ellipsis, tf.newaxis or int32/int64 tensors.\n",
            "     |  \n",
            "     |  __gt__ = greater(x, y, name=None)\n",
            "     |      Returns the truth value of (x > y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 2, 5])\n",
            "     |      tf.math.greater(x, y) ==> [False, True, True]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.greater(x, y) ==> [False, False, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __hash__(self)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __init__(self, initial_value=None, trainable=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, import_scope=None, constraint=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, shape=None)\n",
            "     |      Creates a new variable with value `initial_value`. (deprecated arguments)\n",
            "     |      \n",
            "     |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(caching_device)`. They will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      A variable's value can be manually cached by calling tf.Variable.read_value() under a tf.device scope. The caching_device argument does not work properly.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
            "     |          which is the initial value for the Variable. The initial value must have\n",
            "     |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
            "     |          callable with no argument that returns the initial value when called. In\n",
            "     |          that case, `dtype` must be specified. (Note that initializer functions\n",
            "     |          from init_ops.py must first be bound to a shape before being used here.)\n",
            "     |        trainable: If `True`, GradientTapes automatically watch uses of this\n",
            "     |          variable. Defaults to `True`, unless `synchronization` is set to\n",
            "     |          `ON_READ`, in which case it defaults to `False`.\n",
            "     |        validate_shape: If `False`, allows the variable to be initialized with a\n",
            "     |          value of unknown shape. If `True`, the default, the shape of\n",
            "     |          `initial_value` must be known.\n",
            "     |        caching_device: Optional device string describing where the Variable\n",
            "     |          should be cached for reading.  Defaults to the Variable's device. If not\n",
            "     |          `None`, caches on another device.  Typical use is to cache on the device\n",
            "     |          where the Ops using the Variable reside, to deduplicate copying through\n",
            "     |          `Switch` and other conditional statements.\n",
            "     |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
            "     |          uniquified automatically.\n",
            "     |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates the\n",
            "     |          Variable object with its contents, referencing the variable's nodes in\n",
            "     |          the graph, which must already exist. The graph is not changed.\n",
            "     |          `variable_def` and the other arguments are mutually exclusive.\n",
            "     |        dtype: If set, initial_value will be converted to the given type. If\n",
            "     |          `None`, either the datatype will be kept (if `initial_value` is a\n",
            "     |          Tensor), or `convert_to_tensor` will decide.\n",
            "     |        import_scope: Optional `string`. Name scope to add to the `Variable.` Only\n",
            "     |          used when initializing from protocol buffer.\n",
            "     |        constraint: An optional projection function to be applied to the variable\n",
            "     |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
            "     |          constraints or value constraints for layer weights). The function must\n",
            "     |          take as input the unprojected Tensor representing the value of the\n",
            "     |          variable and return the Tensor for the projected value (which must have\n",
            "     |          the same shape). Constraints are not safe to use when doing asynchronous\n",
            "     |          distributed training.\n",
            "     |        synchronization: Indicates when a distributed a variable will be\n",
            "     |          aggregated. Accepted values are constants defined in the class\n",
            "     |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            "     |          `AUTO` and the current `DistributionStrategy` chooses when to\n",
            "     |          synchronize.\n",
            "     |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            "     |          Accepted values are constants defined in the class\n",
            "     |          `tf.VariableAggregation`.\n",
            "     |        shape: (optional) The shape of this variable. If None, the shape of\n",
            "     |          `initial_value` will be used. When setting this argument to\n",
            "     |          `tf.TensorShape(None)` (representing an unspecified shape), the variable\n",
            "     |          can be assigned with values of different shapes.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If both `variable_def` and initial_value are specified.\n",
            "     |        ValueError: If the initial value is not specified, or does not have a\n",
            "     |          shape and `validate_shape` is `True`.\n",
            "     |  \n",
            "     |  __invert__ = logical_not(x, name=None)\n",
            "     |      Returns the truth value of `NOT x` element-wise.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      >>> tf.math.logical_not(tf.constant([True, False]))\n",
            "     |      <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __iter__(self)\n",
            "     |      Dummy method to prevent iteration.\n",
            "     |      \n",
            "     |      Do not call.\n",
            "     |      \n",
            "     |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
            "     |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
            "     |      to infinity.  Declaring this method prevents this unintended behavior.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: when invoked.\n",
            "     |  \n",
            "     |  __le__ = less_equal(x, y, name=None)\n",
            "     |      Returns the truth value of (x <= y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 6])\n",
            "     |      tf.math.less_equal(x, y) ==> [True, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __lt__ = less(x, y, name=None)\n",
            "     |      Returns the truth value of (x < y) element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5])\n",
            "     |      tf.math.less(x, y) ==> [False, True, False]\n",
            "     |      \n",
            "     |      x = tf.constant([5, 4, 6])\n",
            "     |      y = tf.constant([5, 6, 7])\n",
            "     |      tf.math.less(x, y) ==> [False, True, True]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __matmul__ = binary_op_wrapper(x, y)\n",
            "     |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
            "     |      \n",
            "     |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
            "     |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
            "     |      and any further outer dimensions specify matching batch size.\n",
            "     |      \n",
            "     |      Both matrices must be of the same type. The supported types are:\n",
            "     |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
            "     |      \n",
            "     |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
            "     |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
            "     |      by default.\n",
            "     |      \n",
            "     |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
            "     |      multiplication algorithm can be used by setting the corresponding\n",
            "     |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
            "     |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
            "     |      datatypes `bfloat16` or `float32`.\n",
            "     |      \n",
            "     |      A simple 2-D tensor matrix multiplication:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "     |      >>> a  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "     |      array([[1, 2, 3],\n",
            "     |             [4, 5, 6]], dtype=int32)>\n",
            "     |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
            "     |      >>> b  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "     |      array([[ 7,  8],\n",
            "     |             [ 9, 10],\n",
            "     |             [11, 12]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "     |      array([[ 58,  64],\n",
            "     |             [139, 154]], dtype=int32)>\n",
            "     |      \n",
            "     |      A batch matrix multiplication with batch shape [2]:\n",
            "     |      \n",
            "     |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
            "     |      >>> a  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
            "     |      array([[[ 1,  2,  3],\n",
            "     |              [ 4,  5,  6]],\n",
            "     |             [[ 7,  8,  9],\n",
            "     |              [10, 11, 12]]], dtype=int32)>\n",
            "     |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
            "     |      >>> b  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "     |      array([[[13, 14],\n",
            "     |              [15, 16],\n",
            "     |              [17, 18]],\n",
            "     |             [[19, 20],\n",
            "     |              [21, 22],\n",
            "     |              [23, 24]]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
            "     |      array([[[ 94, 100],\n",
            "     |              [229, 244]],\n",
            "     |             [[508, 532],\n",
            "     |              [697, 730]]], dtype=int32)>\n",
            "     |      \n",
            "     |      Since python >= 3.5 the @ operator is supported\n",
            "     |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
            "     |      it simply calls the `tf.matmul()` function, so the following lines are\n",
            "     |      equivalent:\n",
            "     |      \n",
            "     |      >>> d = a @ b @ [[10], [11]]\n",
            "     |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
            "     |          `complex64`, `complex128` and rank > 1.\n",
            "     |        b: `tf.Tensor` with same type and rank as `a`.\n",
            "     |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
            "     |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
            "     |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        name: Name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
            "     |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
            "     |        transpose or adjoint attributes are `False`:\n",
            "     |      \n",
            "     |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
            "     |        for all indices `i`, `j`.\n",
            "     |      \n",
            "     |        Note: This is matrix product, not element-wise product.\n",
            "     |      \n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
            "     |          `adjoint_b` are both set to `True`.\n",
            "     |  \n",
            "     |  __mod__ = binary_op_wrapper(x, y)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __mul__ = binary_op_wrapper(x, y)\n",
            "     |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
            "     |  \n",
            "     |  __ne__(self, other)\n",
            "     |      Compares two variables element-wise for equality.\n",
            "     |  \n",
            "     |  __neg__ = neg(x, name=None)\n",
            "     |      Computes numerical negative value element-wise.\n",
            "     |      \n",
            "     |      I.e., \\\\(y = -x\\\\).\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __or__ = binary_op_wrapper(x, y)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __pow__ = binary_op_wrapper(x, y)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __radd__ = r_binary_op_wrapper(y, x)\n",
            "     |      Dispatches to add for strings and add_v2 for all other types.\n",
            "     |  \n",
            "     |  __rand__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns the truth value of x AND y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
            "     |      Divide two values using Python 2 semantics.\n",
            "     |      \n",
            "     |      Used for Tensor.__div__.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` returns the quotient of x and y.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
            "     |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
            "     |      \n",
            "     |      The same as `tf.compat.v1.div(x,y)` for integers, but uses\n",
            "     |      `tf.floor(tf.compat.v1.div(x,y))` for\n",
            "     |      floating point arguments so that the result is always an integer (though\n",
            "     |      possibly an integer represented as floating point).  This op is generated by\n",
            "     |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
            "     |      `from __future__ import division`.\n",
            "     |      \n",
            "     |      `x` and `y` must have the same type, and the result will have the same type\n",
            "     |      as well.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: `Tensor` numerator of real numeric type.\n",
            "     |        y: `Tensor` denominator of real numeric type.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        `x / y` rounded down.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the inputs are complex.\n",
            "     |  \n",
            "     |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
            "     |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
            "     |      \n",
            "     |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
            "     |      where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
            "     |      and any further outer dimensions specify matching batch size.\n",
            "     |      \n",
            "     |      Both matrices must be of the same type. The supported types are:\n",
            "     |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
            "     |      \n",
            "     |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
            "     |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
            "     |      by default.\n",
            "     |      \n",
            "     |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
            "     |      multiplication algorithm can be used by setting the corresponding\n",
            "     |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
            "     |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
            "     |      datatypes `bfloat16` or `float32`.\n",
            "     |      \n",
            "     |      A simple 2-D tensor matrix multiplication:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "     |      >>> a  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "     |      array([[1, 2, 3],\n",
            "     |             [4, 5, 6]], dtype=int32)>\n",
            "     |      >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
            "     |      >>> b  # 2-D tensor\n",
            "     |      <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "     |      array([[ 7,  8],\n",
            "     |             [ 9, 10],\n",
            "     |             [11, 12]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "     |      array([[ 58,  64],\n",
            "     |             [139, 154]], dtype=int32)>\n",
            "     |      \n",
            "     |      A batch matrix multiplication with batch shape [2]:\n",
            "     |      \n",
            "     |      >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
            "     |      >>> a  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
            "     |      array([[[ 1,  2,  3],\n",
            "     |              [ 4,  5,  6]],\n",
            "     |             [[ 7,  8,  9],\n",
            "     |              [10, 11, 12]]], dtype=int32)>\n",
            "     |      >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
            "     |      >>> b  # 3-D tensor\n",
            "     |      <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "     |      array([[[13, 14],\n",
            "     |              [15, 16],\n",
            "     |              [17, 18]],\n",
            "     |             [[19, 20],\n",
            "     |              [21, 22],\n",
            "     |              [23, 24]]], dtype=int32)>\n",
            "     |      >>> c = tf.matmul(a, b)\n",
            "     |      >>> c  # `a` * `b`\n",
            "     |      <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
            "     |      array([[[ 94, 100],\n",
            "     |              [229, 244]],\n",
            "     |             [[508, 532],\n",
            "     |              [697, 730]]], dtype=int32)>\n",
            "     |      \n",
            "     |      Since python >= 3.5 the @ operator is supported\n",
            "     |      (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
            "     |      it simply calls the `tf.matmul()` function, so the following lines are\n",
            "     |      equivalent:\n",
            "     |      \n",
            "     |      >>> d = a @ b @ [[10], [11]]\n",
            "     |      >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
            "     |          `complex64`, `complex128` and rank > 1.\n",
            "     |        b: `tf.Tensor` with same type and rank as `a`.\n",
            "     |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
            "     |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
            "     |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
            "     |          multiplication.\n",
            "     |        a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
            "     |          **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "     |          that assume most values in `a` are zero.\n",
            "     |          See `tf.sparse.sparse_dense_matmul`\n",
            "     |          for some support for `tf.SparseTensor` multiplication.\n",
            "     |        name: Name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
            "     |        is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
            "     |        transpose or adjoint attributes are `False`:\n",
            "     |      \n",
            "     |        `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
            "     |        for all indices `i`, `j`.\n",
            "     |      \n",
            "     |        Note: This is matrix product, not element-wise product.\n",
            "     |      \n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
            "     |          `adjoint_b` are both set to `True`.\n",
            "     |  \n",
            "     |  __rmod__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
            "     |      \n",
            "     |      true, this follows Python semantics in that the result here is consistent\n",
            "     |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
            "     |      \n",
            "     |      *NOTE*: `math.floormod` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rmul__ = r_binary_op_wrapper(y, x)\n",
            "     |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
            "     |  \n",
            "     |  __ror__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns the truth value of x OR y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `bool`.\n",
            "     |        y: A `Tensor` of type `bool`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` of type `bool`.\n",
            "     |  \n",
            "     |  __rpow__ = r_binary_op_wrapper(y, x)\n",
            "     |      Computes the power of one value to another.\n",
            "     |      \n",
            "     |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "     |      corresponding elements in `x` and `y`. For example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      x = tf.constant([[2, 2], [3, 3]])\n",
            "     |      y = tf.constant([[8, 16], [2, 3]])\n",
            "     |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "     |          `complex64`, or `complex128`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  __rsub__ = r_binary_op_wrapper(y, x)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
            "     |  \n",
            "     |  __rxor__ = r_binary_op_wrapper(y, x)\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  __sub__ = binary_op_wrapper(x, y)\n",
            "     |      Returns x - y element-wise.\n",
            "     |      \n",
            "     |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "     |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "     |        y: A `Tensor`. Must have the same type as `x`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `x`.\n",
            "     |  \n",
            "     |  __truediv__ = binary_op_wrapper(x, y)\n",
            "     |  \n",
            "     |  __xor__ = binary_op_wrapper(x, y)\n",
            "     |      Logical XOR function.\n",
            "     |      \n",
            "     |      x ^ y = (x | y) & ~(x & y)\n",
            "     |      \n",
            "     |      The operation works for the following input types:\n",
            "     |      \n",
            "     |      - Two single elements of type `bool`\n",
            "     |      - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "     |        be calculated by applying logical XOR with the single element to each\n",
            "     |        element in the larger Tensor.\n",
            "     |      - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "     |        the result will be the element-wise logical XOR of the two input tensors.\n",
            "     |      \n",
            "     |      Usage:\n",
            "     |      \n",
            "     |      >>> a = tf.constant([True])\n",
            "     |      >>> b = tf.constant([False])\n",
            "     |      >>> tf.math.logical_xor(a, b)\n",
            "     |      <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>\n",
            "     |      \n",
            "     |      >>> c = tf.constant([True])\n",
            "     |      >>> x = tf.constant([False, True, True, False])\n",
            "     |      >>> tf.math.logical_xor(c, x)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])>\n",
            "     |      \n",
            "     |      >>> y = tf.constant([False, False, True, True])\n",
            "     |      >>> z = tf.constant([False, True, False, True])\n",
            "     |      >>> tf.math.logical_xor(y, z)\n",
            "     |      <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          x: A `tf.Tensor` type bool.\n",
            "     |          y: A `tf.Tensor` of type bool.\n",
            "     |          name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "     |  \n",
            "     |  assign(self, value, use_locking=False, name=None, read_value=True)\n",
            "     |      Assigns a new value to the variable.\n",
            "     |      \n",
            "     |      This is essentially a shortcut for `assign(self, value)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        value: A `Tensor`. The new value for this variable.\n",
            "     |        use_locking: If `True`, use locking during the assignment.\n",
            "     |        name: The name of the operation to be created\n",
            "     |        read_value: if True, will return something which evaluates to the new\n",
            "     |          value of the variable; if False will return the assign op.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable. If `read_value` is false, instead returns None in\n",
            "     |        Eager mode and the assign op in graph mode.\n",
            "     |  \n",
            "     |  assign_add(self, delta, use_locking=False, name=None, read_value=True)\n",
            "     |      Adds a value to this variable.\n",
            "     |      \n",
            "     |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        delta: A `Tensor`. The value to add to this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: The name of the operation to be created\n",
            "     |        read_value: if True, will return something which evaluates to the new\n",
            "     |          value of the variable; if False will return the assign op.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable. If `read_value` is false, instead returns None in\n",
            "     |        Eager mode and the assign op in graph mode.\n",
            "     |  \n",
            "     |  assign_sub(self, delta, use_locking=False, name=None, read_value=True)\n",
            "     |      Subtracts a value from this variable.\n",
            "     |      \n",
            "     |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        delta: A `Tensor`. The value to subtract from this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: The name of the operation to be created\n",
            "     |        read_value: if True, will return something which evaluates to the new\n",
            "     |          value of the variable; if False will return the assign op.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable. If `read_value` is false, instead returns None in\n",
            "     |        Eager mode and the assign op in graph mode.\n",
            "     |  \n",
            "     |  batch_scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Assigns `tf.IndexedSlices` to this variable batch-wise.\n",
            "     |      \n",
            "     |      Analogous to `batch_gather`. This assumes that this variable and the\n",
            "     |      sparse_delta IndexedSlices have a series of leading dimensions that are the\n",
            "     |      same for all of them, and the updates are performed on the last dimension of\n",
            "     |      indices. In other words, the dimensions should be the following:\n",
            "     |      \n",
            "     |      `num_prefix_dims = sparse_delta.indices.ndims - 1`\n",
            "     |      `batch_dim = num_prefix_dims + 1`\n",
            "     |      `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[\n",
            "     |           batch_dim:]`\n",
            "     |      \n",
            "     |      where\n",
            "     |      \n",
            "     |      `sparse_delta.updates.shape[:num_prefix_dims]`\n",
            "     |      `== sparse_delta.indices.shape[:num_prefix_dims]`\n",
            "     |      `== var.shape[:num_prefix_dims]`\n",
            "     |      \n",
            "     |      And the operation performed can be expressed as:\n",
            "     |      \n",
            "     |      `var[i_1, ..., i_n,\n",
            "     |           sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[\n",
            "     |              i_1, ..., i_n, j]`\n",
            "     |      \n",
            "     |      When sparse_delta.indices is a 1D tensor, this operation is equivalent to\n",
            "     |      `scatter_update`.\n",
            "     |      \n",
            "     |      To avoid this operation one can looping over the first `ndims` of the\n",
            "     |      variable and using `scatter_update` on the subtensors that result of slicing\n",
            "     |      the first dimension. This is a valid option for `ndims = 1`, but less\n",
            "     |      efficient than this implementation.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  count_up_to(self, limit)\n",
            "     |      Increments this variable until it reaches `limit`. (deprecated)\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Prefer Dataset.range instead.\n",
            "     |      \n",
            "     |      When that Op is run it tries to increment the variable by `1`. If\n",
            "     |      incrementing the variable would bring it above `limit` then the Op raises\n",
            "     |      the exception `OutOfRangeError`.\n",
            "     |      \n",
            "     |      If no error is raised, the Op outputs the value of the variable before\n",
            "     |      the increment.\n",
            "     |      \n",
            "     |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        limit: value at which incrementing the variable raises an error.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` that will hold the variable value before the increment. If no\n",
            "     |        other Op modifies this variable, the values produced will all be\n",
            "     |        distinct.\n",
            "     |  \n",
            "     |  eval(self, session=None)\n",
            "     |      In a session, computes and returns the value of this variable.\n",
            "     |      \n",
            "     |      This is not a graph construction method, it does not add ops to the graph.\n",
            "     |      \n",
            "     |      This convenience method requires a session where the graph\n",
            "     |      containing this variable has been launched. If no session is\n",
            "     |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
            "     |      information on launching a graph and on sessions.\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      v = tf.Variable([1, 2])\n",
            "     |      init = tf.compat.v1.global_variables_initializer()\n",
            "     |      \n",
            "     |      with tf.compat.v1.Session() as sess:\n",
            "     |          sess.run(init)\n",
            "     |          # Usage passing the session explicitly.\n",
            "     |          print(v.eval(sess))\n",
            "     |          # Usage with the default session.  The 'with' block\n",
            "     |          # above makes 'sess' the default session.\n",
            "     |          print(v.eval())\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        session: The session to use to evaluate this variable. If none, the\n",
            "     |          default session is used.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A numpy `ndarray` with a copy of the value of this variable.\n",
            "     |  \n",
            "     |  experimental_ref(self)\n",
            "     |      DEPRECATED FUNCTION\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Use ref() instead.\n",
            "     |  \n",
            "     |  gather_nd(self, indices, name=None)\n",
            "     |      Gather slices from `params` into a Tensor with shape specified by `indices`.\n",
            "     |      \n",
            "     |      See tf.gather_nd for details.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "     |          Index tensor.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `params`.\n",
            "     |  \n",
            "     |  get_shape(self)\n",
            "     |      Alias of `Variable.shape`.\n",
            "     |  \n",
            "     |  initialized_value(self)\n",
            "     |      Returns the value of the initialized variable. (deprecated)\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "     |      \n",
            "     |      You should use this instead of the variable itself to initialize another\n",
            "     |      variable with a value that depends on the value of this variable.\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      # Initialize 'v' with a random tensor.\n",
            "     |      v = tf.Variable(tf.random.truncated_normal([10, 40]))\n",
            "     |      # Use `initialized_value` to guarantee that `v` has been\n",
            "     |      # initialized before its value is used to initialize `w`.\n",
            "     |      # The random values are picked only once.\n",
            "     |      w = tf.Variable(v.initialized_value() * 2.0)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` holding the value of this variable after its initializer\n",
            "     |        has run.\n",
            "     |  \n",
            "     |  load(self, value, session=None)\n",
            "     |      Load new value into this variable. (deprecated)\n",
            "     |      \n",
            "     |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
            "     |      Instructions for updating:\n",
            "     |      Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "     |      \n",
            "     |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
            "     |      \n",
            "     |      This convenience method requires a session where the graph\n",
            "     |      containing this variable has been launched. If no session is\n",
            "     |      passed, the default session is used.  See `tf.compat.v1.Session` for more\n",
            "     |      information on launching a graph and on sessions.\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      v = tf.Variable([1, 2])\n",
            "     |      init = tf.compat.v1.global_variables_initializer()\n",
            "     |      \n",
            "     |      with tf.compat.v1.Session() as sess:\n",
            "     |          sess.run(init)\n",
            "     |          # Usage passing the session explicitly.\n",
            "     |          v.load([2, 3], sess)\n",
            "     |          print(v.eval(sess)) # prints [2 3]\n",
            "     |          # Usage with the default session.  The 'with' block\n",
            "     |          # above makes 'sess' the default session.\n",
            "     |          v.load([3, 4], sess)\n",
            "     |          print(v.eval()) # prints [3 4]\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |          value: New variable value\n",
            "     |          session: The session to use to evaluate this variable. If none, the\n",
            "     |            default session is used.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |          ValueError: Session is not passed and no default session\n",
            "     |  \n",
            "     |  read_value(self)\n",
            "     |      Returns the value of this variable, read in the current context.\n",
            "     |      \n",
            "     |      Can be different from value() if it's on another device, with control\n",
            "     |      dependencies, etc.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` containing the value of the variable.\n",
            "     |  \n",
            "     |  ref(self)\n",
            "     |      Returns a hashable reference object to this Variable.\n",
            "     |      \n",
            "     |      The primary use case for this API is to put variables in a set/dictionary.\n",
            "     |      We can't put variables in a set/dictionary as `variable.__hash__()` is no\n",
            "     |      longer available starting Tensorflow 2.0.\n",
            "     |      \n",
            "     |      The following will raise an exception starting 2.0\n",
            "     |      \n",
            "     |      >>> x = tf.Variable(5)\n",
            "     |      >>> y = tf.Variable(10)\n",
            "     |      >>> z = tf.Variable(10)\n",
            "     |      >>> variable_set = {x, y, z}\n",
            "     |      Traceback (most recent call last):\n",
            "     |        ...\n",
            "     |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
            "     |      >>> variable_dict = {x: 'five', y: 'ten'}\n",
            "     |      Traceback (most recent call last):\n",
            "     |        ...\n",
            "     |      TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n",
            "     |      \n",
            "     |      Instead, we can use `variable.ref()`.\n",
            "     |      \n",
            "     |      >>> variable_set = {x.ref(), y.ref(), z.ref()}\n",
            "     |      >>> x.ref() in variable_set\n",
            "     |      True\n",
            "     |      >>> variable_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\n",
            "     |      >>> variable_dict[y.ref()]\n",
            "     |      'ten'\n",
            "     |      \n",
            "     |      Also, the reference object provides `.deref()` function that returns the\n",
            "     |      original Variable.\n",
            "     |      \n",
            "     |      >>> x = tf.Variable(5)\n",
            "     |      >>> x.ref().deref()\n",
            "     |      <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
            "     |  \n",
            "     |  scatter_add(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Adds `tf.IndexedSlices` to this variable.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to be added to this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_div(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Divide this variable by `tf.IndexedSlices`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to divide this variable by.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_max(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Updates this variable with the max of `tf.IndexedSlices` and itself.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to use as an argument of max with this\n",
            "     |          variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_min(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Updates this variable with the min of `tf.IndexedSlices` and itself.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to use as an argument of min with this\n",
            "     |          variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_mul(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Multiply this variable by `tf.IndexedSlices`.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to multiply this variable by.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_nd_add(self, indices, updates, name=None)\n",
            "     |      Applies sparse addition to individual values or slices in a Variable.\n",
            "     |      \n",
            "     |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
            "     |      \n",
            "     |      `indices` must be integer tensor, containing indices into self.\n",
            "     |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
            "     |      \n",
            "     |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
            "     |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
            "     |      dimension of self.\n",
            "     |      \n",
            "     |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
            "     |      \n",
            "     |      ```\n",
            "     |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
            "     |      ```\n",
            "     |      \n",
            "     |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
            "     |      8 elements. In Python, that update would look like this:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "     |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
            "     |          updates = tf.constant([9, 10, 11, 12])\n",
            "     |          add = v.scatter_nd_add(indices, updates)\n",
            "     |          with tf.compat.v1.Session() as sess:\n",
            "     |            print sess.run(add)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      The resulting update to v would look like this:\n",
            "     |      \n",
            "     |          [1, 13, 3, 14, 14, 6, 7, 20]\n",
            "     |      \n",
            "     |      See `tf.scatter_nd` for more details about how to make updates to\n",
            "     |      slices.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: The indices to be used in the operation.\n",
            "     |        updates: The values to be used in the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |  \n",
            "     |  scatter_nd_sub(self, indices, updates, name=None)\n",
            "     |      Applies sparse subtraction to individual values or slices in a Variable.\n",
            "     |      \n",
            "     |      Assuming the variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
            "     |      \n",
            "     |      `indices` must be integer tensor, containing indices into self.\n",
            "     |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
            "     |      \n",
            "     |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
            "     |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
            "     |      dimension of self.\n",
            "     |      \n",
            "     |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
            "     |      \n",
            "     |      ```\n",
            "     |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
            "     |      ```\n",
            "     |      \n",
            "     |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
            "     |      8 elements. In Python, that update would look like this:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "     |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
            "     |          updates = tf.constant([9, 10, 11, 12])\n",
            "     |          op = v.scatter_nd_sub(indices, updates)\n",
            "     |          with tf.compat.v1.Session() as sess:\n",
            "     |            print sess.run(op)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      The resulting update to v would look like this:\n",
            "     |      \n",
            "     |          [1, -9, 3, -6, -6, 6, 7, -4]\n",
            "     |      \n",
            "     |      See `tf.scatter_nd` for more details about how to make updates to\n",
            "     |      slices.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: The indices to be used in the operation.\n",
            "     |        updates: The values to be used in the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |  \n",
            "     |  scatter_nd_update(self, indices, updates, name=None)\n",
            "     |      Applies sparse assignment to individual values or slices in a Variable.\n",
            "     |      \n",
            "     |      The Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
            "     |      \n",
            "     |      `indices` must be integer tensor, containing indices into self.\n",
            "     |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
            "     |      \n",
            "     |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
            "     |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
            "     |      dimension of self.\n",
            "     |      \n",
            "     |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
            "     |      \n",
            "     |      ```\n",
            "     |      [d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n",
            "     |      ```\n",
            "     |      \n",
            "     |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
            "     |      8 elements. In Python, that update would look like this:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |          v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "     |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
            "     |          updates = tf.constant([9, 10, 11, 12])\n",
            "     |          op = v.scatter_nd_assign(indices, updates)\n",
            "     |          with tf.compat.v1.Session() as sess:\n",
            "     |            print sess.run(op)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      The resulting update to v would look like this:\n",
            "     |      \n",
            "     |          [1, 11, 3, 10, 9, 6, 7, 12]\n",
            "     |      \n",
            "     |      See `tf.scatter_nd` for more details about how to make updates to\n",
            "     |      slices.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: The indices to be used in the operation.\n",
            "     |        updates: The values to be used in the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |  \n",
            "     |  scatter_sub(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Subtracts `tf.IndexedSlices` from this variable.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
            "     |      Assigns `tf.IndexedSlices` to this variable.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        sparse_delta: `tf.IndexedSlices` to be assigned to this variable.\n",
            "     |        use_locking: If `True`, use locking during the operation.\n",
            "     |        name: the name of the operation.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The updated variable.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: if `sparse_delta` is not an `IndexedSlices`.\n",
            "     |  \n",
            "     |  set_shape(self, shape)\n",
            "     |      Overrides the shape for this variable.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: the `TensorShape` representing the overridden shape.\n",
            "     |  \n",
            "     |  sparse_read(self, indices, name=None)\n",
            "     |      Gather slices from params axis axis according to indices.\n",
            "     |      \n",
            "     |      This function supports a subset of tf.gather, see tf.gather for details on\n",
            "     |      usage.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        indices: The index `Tensor`.  Must be one of the following types: `int32`,\n",
            "     |          `int64`. Must be in range `[0, params.shape[axis])`.\n",
            "     |        name: A name for the operation (optional).\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`. Has the same type as `params`.\n",
            "     |  \n",
            "     |  to_proto(self, export_scope=None)\n",
            "     |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        export_scope: Optional `string`. Name scope to remove.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
            "     |        in the specified name scope.\n",
            "     |  \n",
            "     |  value(self)\n",
            "     |      Returns the last snapshot of this variable.\n",
            "     |      \n",
            "     |      You usually do not need to call this method as all ops that need the value\n",
            "     |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
            "     |      \n",
            "     |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
            "     |      assign a new value to this tensor as it is not a reference to the variable.\n",
            "     |      \n",
            "     |      To avoid copies, if the consumer of the returned value is on the same device\n",
            "     |      as the variable, this actually returns the live value of the variable, not\n",
            "     |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
            "     |      is on a different device it will get a copy of the variable.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor` containing the value of the variable.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods defined here:\n",
            "     |  \n",
            "     |  from_proto(variable_def, import_scope=None)\n",
            "     |      Returns a `Variable` object created from `variable_def`.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  aggregation\n",
            "     |  \n",
            "     |  constraint\n",
            "     |      Returns the constraint function associated with this variable.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The constraint function that was passed to the variable constructor.\n",
            "     |        Can be `None` if no constraint was passed.\n",
            "     |  \n",
            "     |  device\n",
            "     |      The device of this variable.\n",
            "     |  \n",
            "     |  dtype\n",
            "     |      The `DType` of this variable.\n",
            "     |  \n",
            "     |  graph\n",
            "     |      The `Graph` of this variable.\n",
            "     |  \n",
            "     |  initial_value\n",
            "     |      Returns the Tensor used as the initial value for the variable.\n",
            "     |      \n",
            "     |      Note that this is different from `initialized_value()` which runs\n",
            "     |      the op that initializes the variable before returning its value.\n",
            "     |      This method returns the tensor that is used by the op that initializes\n",
            "     |      the variable.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `Tensor`.\n",
            "     |  \n",
            "     |  initializer\n",
            "     |      The initializer operation for this variable.\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of this variable.\n",
            "     |  \n",
            "     |  op\n",
            "     |      The `Operation` of this variable.\n",
            "     |  \n",
            "     |  shape\n",
            "     |      The `TensorShape` of this variable.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A `TensorShape`.\n",
            "     |  \n",
            "     |  synchronization\n",
            "     |  \n",
            "     |  trainable\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
            "     |      Information on how to save this Variable as a slice.\n",
            "     |      \n",
            "     |      Provides internal support for saving variables as slices of a larger\n",
            "     |      variable.  This API is not public and is subject to change.\n",
            "     |      \n",
            "     |      Available properties:\n",
            "     |      \n",
            "     |      * full_name\n",
            "     |      * full_shape\n",
            "     |      * var_offset\n",
            "     |      * var_shape\n",
            "     |  \n",
            "     |  __array_priority__ = 100\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    VariableAggregation = class VariableAggregationV2(enum.Enum)\n",
            "     |  Indicates how a distributed variable will be aggregated.\n",
            "     |  \n",
            "     |  `tf.distribute.Strategy` distributes a model by making multiple copies\n",
            "     |  (called \"replicas\") acting data-parallel on different elements of the input\n",
            "     |  batch. When performing some variable-update operation, say\n",
            "     |  `var.assign_add(x)`, in a model, we need to resolve how to combine the\n",
            "     |  different values for `x` computed in the different replicas.\n",
            "     |  \n",
            "     |  * `NONE`: This is the default, giving an error if you use a\n",
            "     |    variable-update operation with multiple replicas.\n",
            "     |  * `SUM`: Add the updates across replicas.\n",
            "     |  * `MEAN`: Take the arithmetic mean (\"average\") of the updates across replicas.\n",
            "     |  * `ONLY_FIRST_REPLICA`: This is for when every replica is performing the same\n",
            "     |    update, but we only want to perform the update once. Used, e.g., for the\n",
            "     |    global step counter.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      VariableAggregationV2\n",
            "     |      enum.Enum\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  MEAN = <VariableAggregationV2.MEAN: 2>\n",
            "     |  \n",
            "     |  NONE = <VariableAggregationV2.NONE: 0>\n",
            "     |  \n",
            "     |  ONLY_FIRST_REPLICA = <VariableAggregationV2.ONLY_FIRST_REPLICA: 3>\n",
            "     |  \n",
            "     |  SUM = <VariableAggregationV2.SUM: 1>\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.Enum:\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of the Enum member.\n",
            "     |  \n",
            "     |  value\n",
            "     |      The value of the Enum member.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.EnumMeta:\n",
            "     |  \n",
            "     |  __members__\n",
            "     |      Returns a mapping of member name->value.\n",
            "     |      \n",
            "     |      This mapping lists all enum members, including aliases. Note that this\n",
            "     |      is a read-only view of the internal mapping.\n",
            "    \n",
            "    class VariableSynchronization(enum.Enum)\n",
            "     |  Indicates when a distributed variable will be synced.\n",
            "     |  \n",
            "     |  * `AUTO`: Indicates that the synchronization will be determined by the current\n",
            "     |    `DistributionStrategy` (eg. With `MirroredStrategy` this would be\n",
            "     |    `ON_WRITE`).\n",
            "     |  * `NONE`: Indicates that there will only be one copy of the variable, so\n",
            "     |    there is no need to sync.\n",
            "     |  * `ON_WRITE`: Indicates that the variable will be updated across devices\n",
            "     |    every time it is written.\n",
            "     |  * `ON_READ`: Indicates that the variable will be aggregated across devices\n",
            "     |    when it is read (eg. when checkpointing or when evaluating an op that uses\n",
            "     |    the variable).\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      VariableSynchronization\n",
            "     |      enum.Enum\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  AUTO = <VariableSynchronization.AUTO: 0>\n",
            "     |  \n",
            "     |  NONE = <VariableSynchronization.NONE: 1>\n",
            "     |  \n",
            "     |  ON_READ = <VariableSynchronization.ON_READ: 3>\n",
            "     |  \n",
            "     |  ON_WRITE = <VariableSynchronization.ON_WRITE: 2>\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.Enum:\n",
            "     |  \n",
            "     |  name\n",
            "     |      The name of the Enum member.\n",
            "     |  \n",
            "     |  value\n",
            "     |      The value of the Enum member.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from enum.EnumMeta:\n",
            "     |  \n",
            "     |  __members__\n",
            "     |      Returns a mapping of member name->value.\n",
            "     |      \n",
            "     |      This mapping lists all enum members, including aliases. Note that this\n",
            "     |      is a read-only view of the internal mapping.\n",
            "    \n",
            "    constant_initializer = class Constant(Initializer)\n",
            "     |  Initializer that generates tensors with constant values.\n",
            "     |  \n",
            "     |  Initializers allow you to pre-specify an initialization strategy, encoded in\n",
            "     |  the Initializer object, without knowing the shape and dtype of the variable\n",
            "     |  being initialized.\n",
            "     |  \n",
            "     |  `tf.constant_initializer` returns an object which when called returns a tensor\n",
            "     |  populated with the `value` specified in the constructor. This `value` must be\n",
            "     |  convertible to the requested `dtype`.\n",
            "     |  \n",
            "     |  The argument `value` can be a scalar constant value, or a list of\n",
            "     |  values. Scalars broadcast to whichever shape is requested from the\n",
            "     |  initializer.\n",
            "     |  \n",
            "     |  If `value` is a list, then the length of the list must be equal to the number\n",
            "     |  of elements implied by the desired shape of the tensor. If the total number of\n",
            "     |  elements in `value` is not equal to the number of elements required by the\n",
            "     |  tensor shape, the initializer will raise a `TypeError`.\n",
            "     |  \n",
            "     |  Examples:\n",
            "     |  \n",
            "     |  >>> def make_variables(k, initializer):\n",
            "     |  ...   return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),\n",
            "     |  ...           tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))\n",
            "     |  >>> v1, v2 = make_variables(3, tf.constant_initializer(2.))\n",
            "     |  >>> v1\n",
            "     |  <tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>\n",
            "     |  >>> v2\n",
            "     |  <tf.Variable ... shape=(3, 3) ... numpy=\n",
            "     |  array([[2., 2., 2.],\n",
            "     |         [2., 2., 2.],\n",
            "     |         [2., 2., 2.]], dtype=float32)>\n",
            "     |  >>> make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
            "     |  (<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...\n",
            "     |  \n",
            "     |  >>> value = [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "     |  >>> init = tf.constant_initializer(value)\n",
            "     |  >>> # Fitting shape\n",
            "     |  >>> tf.Variable(init(shape=[2, 4], dtype=tf.float32))\n",
            "     |  <tf.Variable ...\n",
            "     |  array([[0., 1., 2., 3.],\n",
            "     |         [4., 5., 6., 7.]], dtype=float32)>\n",
            "     |  >>> # Larger shape\n",
            "     |  >>> tf.Variable(init(shape=[3, 4], dtype=tf.float32))\n",
            "     |  Traceback (most recent call last):\n",
            "     |  ...\n",
            "     |  TypeError: ...value has 8 elements, shape is (3, 4) with 12 elements...\n",
            "     |  >>> # Smaller shape\n",
            "     |  >>> tf.Variable(init(shape=[2, 3], dtype=tf.float32))\n",
            "     |  Traceback (most recent call last):\n",
            "     |  ...\n",
            "     |  TypeError: ...value has 8 elements, shape is (2, 3) with 6 elements...\n",
            "     |  \n",
            "     |  Args:\n",
            "     |    value: A Python scalar, list or tuple of values, or a N-dimensional numpy\n",
            "     |      array. All elements of the initialized variable will be set to the\n",
            "     |      corresponding value in the `value` argument.\n",
            "     |  \n",
            "     |  Raises:\n",
            "     |    TypeError: If the input `value` is not one of the expected types.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Constant\n",
            "     |      Initializer\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, shape, dtype=None)\n",
            "     |      Returns a tensor object initialized as specified by the initializer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Shape of the tensor.\n",
            "     |        dtype: Optional dtype of the tensor. If not provided the dtype of the\n",
            "     |         tensor created will be the type of the inital value.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        TypeError: If the initializer cannot create a tensor of the requested\n",
            "     |         dtype.\n",
            "     |  \n",
            "     |  __init__(self, value=0)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  get_config(self)\n",
            "     |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A JSON-serializable Python dict.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from Initializer:\n",
            "     |  \n",
            "     |  from_config(config) from builtins.type\n",
            "     |      Instantiates an initializer from a configuration dictionary.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      initializer = RandomUniform(-1, 1)\n",
            "     |      config = initializer.get_config()\n",
            "     |      initializer = RandomUniform.from_config(config)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        config: A Python dictionary.\n",
            "     |          It will typically be the output of `get_config`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An Initializer instance.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from Initializer:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    name_scope = class name_scope_v2(builtins.object)\n",
            "     |  A context manager for use when defining a Python op.\n",
            "     |  \n",
            "     |  This context manager pushes a name scope, which will make the name of all\n",
            "     |  operations added within it have a prefix.\n",
            "     |  \n",
            "     |  For example, to define a new Python op called `my_op`:\n",
            "     |  \n",
            "     |  ```python\n",
            "     |  def my_op(a, b, c, name=None):\n",
            "     |    with tf.name_scope(\"MyOp\") as scope:\n",
            "     |      a = tf.convert_to_tensor(a, name=\"a\")\n",
            "     |      b = tf.convert_to_tensor(b, name=\"b\")\n",
            "     |      c = tf.convert_to_tensor(c, name=\"c\")\n",
            "     |      # Define some computation that uses `a`, `b`, and `c`.\n",
            "     |      return foo_op(..., name=scope)\n",
            "     |  ```\n",
            "     |  \n",
            "     |  When executed, the Tensors `a`, `b`, `c`, will have names `MyOp/a`, `MyOp/b`,\n",
            "     |  and `MyOp/c`.\n",
            "     |  \n",
            "     |  If the scope name already exists, the name will be made unique by appending\n",
            "     |  `_n`. For example, calling `my_op` the second time will generate `MyOp_1/a`,\n",
            "     |  etc.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __enter__(self)\n",
            "     |      Start the scope block.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        The scope name.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: if neither `name` nor `default_name` is provided\n",
            "     |          but `values` are.\n",
            "     |  \n",
            "     |  __exit__(self, type_arg, value_arg, traceback_arg)\n",
            "     |  \n",
            "     |  __init__(self, name)\n",
            "     |      Initialize the context manager.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        name: The prefix to use on all names created within the name scope.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If name is None, or not a string.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  name\n",
            "    \n",
            "    ones_initializer = class Ones(Initializer)\n",
            "     |  Initializer that generates tensors initialized to 1.\n",
            "     |  \n",
            "     |  Initializers allow you to pre-specify an initialization strategy, encoded in\n",
            "     |  the Initializer object, without knowing the shape and dtype of the variable\n",
            "     |  being initialized.\n",
            "     |  \n",
            "     |  Examples:\n",
            "     |  \n",
            "     |  >>> def make_variables(k, initializer):\n",
            "     |  ...   return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),\n",
            "     |  ...           tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))\n",
            "     |  >>> v1, v2 = make_variables(3, tf.ones_initializer())\n",
            "     |  >>> v1\n",
            "     |  <tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>\n",
            "     |  >>> v2\n",
            "     |  <tf.Variable ... shape=(3, 3) ... numpy=\n",
            "     |  array([[1., 1., 1.],\n",
            "     |         [1., 1., 1.],\n",
            "     |         [1., 1., 1.]], dtype=float32)>\n",
            "     |  >>> make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
            "     |  (<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Ones\n",
            "     |      Initializer\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, shape, dtype=tf.float32)\n",
            "     |      Returns a tensor object initialized as specified by the initializer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Shape of the tensor.\n",
            "     |        dtype: Optional dtype of the tensor. Only numeric or boolean dtypes are\n",
            "     |         supported.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValuesError: If the dtype is not numeric or boolean.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from Initializer:\n",
            "     |  \n",
            "     |  get_config(self)\n",
            "     |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A JSON-serializable Python dict.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from Initializer:\n",
            "     |  \n",
            "     |  from_config(config) from builtins.type\n",
            "     |      Instantiates an initializer from a configuration dictionary.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      initializer = RandomUniform(-1, 1)\n",
            "     |      config = initializer.get_config()\n",
            "     |      initializer = RandomUniform.from_config(config)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        config: A Python dictionary.\n",
            "     |          It will typically be the output of `get_config`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An Initializer instance.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from Initializer:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    random_normal_initializer = class RandomNormal(Initializer)\n",
            "     |  Initializer that generates tensors with a normal distribution.\n",
            "     |  \n",
            "     |  Initializers allow you to pre-specify an initialization strategy, encoded in\n",
            "     |  the Initializer object, without knowing the shape and dtype of the variable\n",
            "     |  being initialized.\n",
            "     |  \n",
            "     |  Examples:\n",
            "     |  \n",
            "     |  >>> def make_variables(k, initializer):\n",
            "     |  ...   return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),\n",
            "     |  ...           tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))\n",
            "     |  >>> v1, v2 = make_variables(3,\n",
            "     |  ...                         tf.random_normal_initializer(mean=1., stddev=2.))\n",
            "     |  >>> v1\n",
            "     |  <tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>\n",
            "     |  >>> v2\n",
            "     |  <tf.Variable ... shape=(3, 3) ... numpy=\n",
            "     |  ...\n",
            "     |  >>> make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
            "     |  (<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...\n",
            "     |  \n",
            "     |  Args:\n",
            "     |    mean: a python scalar or a scalar tensor. Mean of the random values to\n",
            "     |      generate.\n",
            "     |    stddev: a python scalar or a scalar tensor. Standard deviation of the random\n",
            "     |      values to generate.\n",
            "     |    seed: A Python integer. Used to create random seeds. See\n",
            "     |      `tf.random.set_seed` for behavior.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      RandomNormal\n",
            "     |      Initializer\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, shape, dtype=tf.float32)\n",
            "     |      Returns a tensor object initialized as specified by the initializer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Shape of the tensor.\n",
            "     |        dtype: Optional dtype of the tensor. Only floating point types are\n",
            "     |         supported.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If the dtype is not floating point\n",
            "     |  \n",
            "     |  __init__(self, mean=0.0, stddev=0.05, seed=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  get_config(self)\n",
            "     |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A JSON-serializable Python dict.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from Initializer:\n",
            "     |  \n",
            "     |  from_config(config) from builtins.type\n",
            "     |      Instantiates an initializer from a configuration dictionary.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      initializer = RandomUniform(-1, 1)\n",
            "     |      config = initializer.get_config()\n",
            "     |      initializer = RandomUniform.from_config(config)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        config: A Python dictionary.\n",
            "     |          It will typically be the output of `get_config`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An Initializer instance.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from Initializer:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    random_uniform_initializer = class RandomUniform(Initializer)\n",
            "     |  Initializer that generates tensors with a uniform distribution.\n",
            "     |  \n",
            "     |  Initializers allow you to pre-specify an initialization strategy, encoded in\n",
            "     |  the Initializer object, without knowing the shape and dtype of the variable\n",
            "     |  being initialized.\n",
            "     |  \n",
            "     |  Examples:\n",
            "     |  \n",
            "     |  >>> def make_variables(k, initializer):\n",
            "     |  ...   return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),\n",
            "     |  ...           tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))\n",
            "     |  >>> v1, v2 = make_variables(3, tf.ones_initializer())\n",
            "     |  >>> v1\n",
            "     |  <tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>\n",
            "     |  >>> v2\n",
            "     |  <tf.Variable ... shape=(3, 3) ... numpy=\n",
            "     |  array([[1., 1., 1.],\n",
            "     |         [1., 1., 1.],\n",
            "     |         [1., 1., 1.]], dtype=float32)>\n",
            "     |  >>> make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
            "     |  (<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...\n",
            "     |  \n",
            "     |  Args:\n",
            "     |    minval: A python scalar or a scalar tensor. Lower bound of the range of\n",
            "     |      random values to generate (inclusive).\n",
            "     |    maxval: A python scalar or a scalar tensor. Upper bound of the range of\n",
            "     |      random values to generate (exclusive).\n",
            "     |    seed: A Python integer. Used to create random seeds. See\n",
            "     |      `tf.random.set_seed` for behavior.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      RandomUniform\n",
            "     |      Initializer\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, shape, dtype=tf.float32)\n",
            "     |      Returns a tensor object initialized as specified by the initializer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Shape of the tensor.\n",
            "     |        dtype: Optional dtype of the tensor. Only floating point and integer\n",
            "     |        types are supported.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValueError: If the dtype is not numeric.\n",
            "     |  \n",
            "     |  __init__(self, minval=-0.05, maxval=0.05, seed=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  get_config(self)\n",
            "     |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A JSON-serializable Python dict.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from Initializer:\n",
            "     |  \n",
            "     |  from_config(config) from builtins.type\n",
            "     |      Instantiates an initializer from a configuration dictionary.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      initializer = RandomUniform(-1, 1)\n",
            "     |      config = initializer.get_config()\n",
            "     |      initializer = RandomUniform.from_config(config)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        config: A Python dictionary.\n",
            "     |          It will typically be the output of `get_config`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An Initializer instance.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from Initializer:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    zeros_initializer = class Zeros(Initializer)\n",
            "     |  Initializer that generates tensors initialized to 0.\n",
            "     |  \n",
            "     |  Initializers allow you to pre-specify an initialization strategy, encoded in\n",
            "     |  the Initializer object, without knowing the shape and dtype of the variable\n",
            "     |  being initialized.\n",
            "     |  \n",
            "     |  Examples:\n",
            "     |  \n",
            "     |  >>> def make_variables(k, initializer):\n",
            "     |  ...   return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),\n",
            "     |  ...           tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))\n",
            "     |  >>> v1, v2 = make_variables(3, tf.zeros_initializer())\n",
            "     |  >>> v1\n",
            "     |  <tf.Variable ... shape=(3,) ... numpy=array([0., 0., 0.], dtype=float32)>\n",
            "     |  >>> v2\n",
            "     |  <tf.Variable ... shape=(3, 3) ... numpy=\n",
            "     |  array([[0., 0., 0.],\n",
            "     |         [0., 0., 0.],\n",
            "     |         [0., 0., 0.]], dtype=float32)>\n",
            "     |  >>> make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))\n",
            "     |  (<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ...\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Zeros\n",
            "     |      Initializer\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__(self, shape, dtype=tf.float32)\n",
            "     |      Returns a tensor object initialized as specified by the initializer.\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        shape: Shape of the tensor.\n",
            "     |        dtype: Optional dtype of the tensor. Only numeric or boolean dtypes are\n",
            "     |         supported.\n",
            "     |      \n",
            "     |      Raises:\n",
            "     |        ValuesError: If the dtype is not numeric or boolean.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from Initializer:\n",
            "     |  \n",
            "     |  get_config(self)\n",
            "     |      Returns the configuration of the initializer as a JSON-serializable dict.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        A JSON-serializable Python dict.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from Initializer:\n",
            "     |  \n",
            "     |  from_config(config) from builtins.type\n",
            "     |      Instantiates an initializer from a configuration dictionary.\n",
            "     |      \n",
            "     |      Example:\n",
            "     |      \n",
            "     |      ```python\n",
            "     |      initializer = RandomUniform(-1, 1)\n",
            "     |      config = initializer.get_config()\n",
            "     |      initializer = RandomUniform.from_config(config)\n",
            "     |      ```\n",
            "     |      \n",
            "     |      Args:\n",
            "     |        config: A Python dictionary.\n",
            "     |          It will typically be the output of `get_config`.\n",
            "     |      \n",
            "     |      Returns:\n",
            "     |        An Initializer instance.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from Initializer:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "FUNCTIONS\n",
            "    Assert(condition, data, summarize=None, name=None)\n",
            "        Asserts that the given condition is true.\n",
            "        \n",
            "        If `condition` evaluates to false, print the list of tensors in `data`.\n",
            "        `summarize` determines how many entries of the tensors to print.\n",
            "        \n",
            "        NOTE: In graph mode, to ensure that Assert executes, one usually attaches\n",
            "        a dependency:\n",
            "        \n",
            "        ```python\n",
            "        # Ensure maximum element of x is smaller or equal to 1\n",
            "        assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])\n",
            "        with tf.control_dependencies([assert_op]):\n",
            "          ... code using x ...\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          condition: The condition to evaluate.\n",
            "          data: The tensors to print out when condition is false.\n",
            "          summarize: Print this many entries of each tensor.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          assert_op: An `Operation` that, when executed, raises a\n",
            "          `tf.errors.InvalidArgumentError` if `condition` is not true.\n",
            "          @compatibility(eager)\n",
            "          returns None\n",
            "          @end_compatibility\n",
            "        \n",
            "        Raises:\n",
            "          @compatibility(eager)\n",
            "          `tf.errors.InvalidArgumentError` if `condition` is not true\n",
            "          @end_compatibility\n",
            "        \n",
            "        \n",
            "        **NOTE** The output of this function should be used.  If it is not, a warning will be logged or an error may be raised.  To mark the output as used, call its .mark_used() method.\n",
            "    \n",
            "    abs(x, name=None)\n",
            "        Computes the absolute value of a tensor.\n",
            "        \n",
            "        Given a tensor of integer or floating-point values, this operation returns a\n",
            "        tensor of the same type, where each element contains the absolute value of the\n",
            "        corresponding element in the input.\n",
            "        \n",
            "        Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
            "        `float32` or `float64` that is the absolute value of each element in `x`. For\n",
            "        a complex number \\\\(a + bj\\\\), its absolute value is computed as \\\\(\\sqrt{a^2\n",
            "        + b^2}\\\\).  For example:\n",
            "        \n",
            "        >>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
            "        >>> tf.abs(x)\n",
            "        <tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n",
            "        array([[5.25594901],\n",
            "               [6.60492241]])>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
            "            `int32`, `int64`, `complex64` or `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n",
            "            with absolute values. Note, for `complex64` or `complex128` input, the\n",
            "            returned `Tensor` will be of type `float32` or `float64`, respectively.\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    acos(x, name=None)\n",
            "        Computes acos of x element-wise.\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    acosh(x, name=None)\n",
            "        Computes inverse hyperbolic cosine of x element-wise.\n",
            "        \n",
            "        Given an input tensor, the function computes inverse hyperbolic cosine of every element.\n",
            "        Input range is `[1, inf]`. It returns `nan` if the input lies outside the range.\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n",
            "        tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    add(x, y, name=None)\n",
            "        Returns x + y element-wise.\n",
            "        \n",
            "        *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    add_n(inputs, name=None)\n",
            "        Adds all input tensors element-wise.\n",
            "        \n",
            "        `tf.math.add_n` performs the same operation as `tf.math.accumulate_n`, but it\n",
            "        waits for all of its inputs to be ready before beginning to sum.\n",
            "        This buffering can result in higher memory consumption when inputs are ready\n",
            "        at different times, since the minimum temporary storage required is\n",
            "        proportional to the input size rather than the output size.\n",
            "        \n",
            "        This op does not [broadcast](\n",
            "        https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html)\n",
            "        its inputs. If you need broadcasting, use `tf.math.add` (or the `+` operator)\n",
            "        instead.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> a = tf.constant([[3, 5], [4, 8]])\n",
            "        >>> b = tf.constant([[1, 6], [2, 9]])\n",
            "        >>> tf.math.add_n([a, b, a])\n",
            "        <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "        array([[ 7, 16],\n",
            "               [10, 25]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          inputs: A list of `tf.Tensor` or `tf.IndexedSlices` objects, each with the\n",
            "            same shape and type. `tf.IndexedSlices` objects will be converted into\n",
            "            dense tensors prior to adding.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of the same shape and type as the elements of `inputs`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `inputs` don't all have same shape and dtype or the shape\n",
            "          cannot be inferred.\n",
            "    \n",
            "    argmax = argmax_v2(input, axis=None, output_type=tf.int64, name=None)\n",
            "        Returns the index with the largest value across axes of a tensor.\n",
            "        \n",
            "        Note that in case of ties the identity of the return value is not guaranteed.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> A = tf.constant([2, 20, 30, 3, 6])\n",
            "        >>> tf.math.argmax(A)  # A[2] is maximum in tensor A\n",
            "        <tf.Tensor: shape=(), dtype=int64, numpy=2>\n",
            "        >>> B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
            "        ...                  [14, 45, 23, 5, 27]])\n",
            "        >>> tf.math.argmax(B, 0)\n",
            "        <tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>\n",
            "        >>> tf.math.argmax(B, 1)\n",
            "        <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          axis: An integer, the axis to reduce across. Default to 0.\n",
            "          output_type: An optional output dtype (`tf.int32` or `tf.int64`). Defaults\n",
            "            to `tf.int64`.\n",
            "          name: An optional name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `output_type`.\n",
            "    \n",
            "    argmin = argmin_v2(input, axis=None, output_type=tf.int64, name=None)\n",
            "        Returns the index with the smallest value across axes of a tensor.\n",
            "        \n",
            "        Note that in case of ties the identity of the return value is not guaranteed.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. Must be one of the following types: `float32`, `float64`,\n",
            "            `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`,\n",
            "            `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`,\n",
            "            `uint64`.\n",
            "          axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            int32 or int64, must be in the range `-rank(input), rank(input))`.\n",
            "            Describes which axis of the input Tensor to reduce across. For vectors,\n",
            "            use axis = 0.\n",
            "          output_type: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to\n",
            "            `tf.int64`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `output_type`.\n",
            "        \n",
            "        Usage:\n",
            "        ```python\n",
            "        import tensorflow as tf\n",
            "        a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n",
            "        b = tf.math.argmin(input = a)\n",
            "        c = tf.keras.backend.eval(b)\n",
            "        # c = 0\n",
            "        # here a[0] = 1 which is the smallest element of a across axis 0\n",
            "        ```\n",
            "    \n",
            "    argsort(values, axis=-1, direction='ASCENDING', stable=False, name=None)\n",
            "        Returns the indices of a tensor that give its sorted order along an axis.\n",
            "        \n",
            "        For a 1D tensor, `tf.gather(values, tf.argsort(values))` is equivalent to\n",
            "        `tf.sort(values)`. For higher dimensions, the output has the same shape as\n",
            "        `values`, but along the given axis, values represent the index of the sorted\n",
            "        element in that slice of the tensor at the given position.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```python\n",
            "        import tensorflow as tf\n",
            "        a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n",
            "        b = tf.argsort(a,axis=-1,direction='ASCENDING',stable=False,name=None)\n",
            "        c = tf.keras.backend.eval(b)\n",
            "        # Here, c = [0 3 1 2 5 4]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          values: 1-D or higher numeric `Tensor`.\n",
            "          axis: The axis along which to sort. The default is -1, which sorts the last\n",
            "            axis.\n",
            "          direction: The direction in which to sort the values (`'ASCENDING'` or\n",
            "            `'DESCENDING'`).\n",
            "          stable: If True, equal elements in the original tensor will not be\n",
            "            re-ordered in the returned order. Unstable sort is not yet implemented,\n",
            "            but will eventually be the default for performance reasons. If you require\n",
            "            a stable order, pass `stable=True` for forwards compatibility.\n",
            "          name: Optional name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          An int32 `Tensor` with the same shape as `values`. The indices that would\n",
            "              sort each slice of the given `values` along the given `axis`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If axis is not a constant scalar, or the direction is invalid.\n",
            "    \n",
            "    as_dtype(type_value)\n",
            "        Converts the given `type_value` to a `DType`.\n",
            "        \n",
            "        Args:\n",
            "          type_value: A value that can be converted to a `tf.DType` object. This may\n",
            "            currently be a `tf.DType` object, a [`DataType`\n",
            "            enum](https://www.tensorflow.org/code/tensorflow/core/framework/types.proto),\n",
            "              a string type name, or a `numpy.dtype`.\n",
            "        \n",
            "        Returns:\n",
            "          A `DType` corresponding to `type_value`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `type_value` cannot be converted to a `DType`.\n",
            "    \n",
            "    as_string(input, precision=-1, scientific=False, shortest=False, width=-1, fill='', name=None)\n",
            "        Converts each entry in the given tensor to strings.\n",
            "        \n",
            "        Supports many numeric types and boolean.\n",
            "        \n",
            "        For Unicode, see the\n",
            "        [https://www.tensorflow.org/tutorials/representation/unicode](Working with Unicode text)\n",
            "        tutorial.\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        >>> tf.strings.as_string([3, 2])\n",
            "        <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>\n",
            "        >>> tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()\n",
            "        array([b'3.14', b'2.72'], dtype=object)\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `float32`, `float64`, `bool`.\n",
            "          precision: An optional `int`. Defaults to `-1`.\n",
            "            The post-decimal precision to use for floating point numbers.\n",
            "            Only used if precision > -1.\n",
            "          scientific: An optional `bool`. Defaults to `False`.\n",
            "            Use scientific notation for floating point numbers.\n",
            "          shortest: An optional `bool`. Defaults to `False`.\n",
            "            Use shortest representation (either scientific or standard) for\n",
            "            floating point numbers.\n",
            "          width: An optional `int`. Defaults to `-1`.\n",
            "            Pad pre-decimal numbers to this width.\n",
            "            Applies to both floating point and integer numbers.\n",
            "            Only used if width > -1.\n",
            "          fill: An optional `string`. Defaults to `\"\"`.\n",
            "            The value to pad if width > -1.  If empty, pads with spaces.\n",
            "            Another typical value is '0'.  String cannot be longer than 1 character.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `string`.\n",
            "    \n",
            "    asin(x, name=None)\n",
            "        Computes the trignometric inverse sine of x element-wise.\n",
            "        \n",
            "        The `tf.math.asin` operation returns the inverse of `tf.math.sin`, such that\n",
            "        if `y = tf.math.sin(x)` then, `x = tf.math.asin(y)`.\n",
            "        \n",
            "        **Note**: The output of `tf.math.asin` will lie within the invertible range\n",
            "        of sine, i.e [-pi/2, pi/2].\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\n",
            "        x = tf.constant([1.047, 0.785])\n",
            "        y = tf.math.sin(x) # [0.8659266, 0.7068252]\n",
            "        \n",
            "        tf.math.asin(y) # [1.047, 0.785] = x\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    asinh(x, name=None)\n",
            "        Computes inverse hyperbolic sine of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes inverse hyperbolic sine\n",
            "          for every element in the tensor. Both input and output has a range of\n",
            "          `[-inf, inf]`.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -2, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n",
            "          tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    assert_equal = assert_equal_v2(x, y, message=None, summarize=None, name=None)\n",
            "        Assert the condition `x == y` holds element-wise.\n",
            "        \n",
            "        This Op checks that `x[i] == y[i]` holds for every pair of (possibly\n",
            "        broadcast) elements of `x` and `y`. If both `x` and `y` are empty, this is\n",
            "        trivially satisfied.\n",
            "        \n",
            "        If `x` and `y` are not equal, `message`, as well as the first `summarize`\n",
            "        entries of `x` and `y` are printed, and `InvalidArgumentError` is raised.\n",
            "        \n",
            "        Args:\n",
            "          x:  Numeric `Tensor`.\n",
            "          y:  Numeric `Tensor`, same dtype as and broadcastable to `x`.\n",
            "          message: A string to prefix to the default message.\n",
            "          summarize: Print this many entries of each tensor.\n",
            "          name: A name for this operation (optional).  Defaults to \"assert_equal\".\n",
            "        \n",
            "        Returns:\n",
            "          Op that raises `InvalidArgumentError` if `x == y` is False. This can be\n",
            "            used with `tf.control_dependencies` inside of `tf.function`s to block\n",
            "            followup computation until the check has executed.\n",
            "          @compatibility(eager)\n",
            "          returns None\n",
            "          @end_compatibility\n",
            "        \n",
            "        Raises:\n",
            "          InvalidArgumentError: if the check can be performed immediately and\n",
            "            `x == y` is False. The check can be performed immediately during eager\n",
            "            execution or if `x` and `y` are statically known.\n",
            "    \n",
            "    assert_greater = assert_greater_v2(x, y, message=None, summarize=None, name=None)\n",
            "        Assert the condition `x > y` holds element-wise.\n",
            "        \n",
            "        This Op checks that `x[i] > y[i]` holds for every pair of (possibly\n",
            "        broadcast) elements of `x` and `y`. If both `x` and `y` are empty, this is\n",
            "        trivially satisfied.\n",
            "        \n",
            "        If `x` is not greater than `y` element-wise, `message`, as well as the first\n",
            "        `summarize` entries of `x` and `y` are printed, and `InvalidArgumentError` is\n",
            "        raised.\n",
            "        \n",
            "        Args:\n",
            "          x:  Numeric `Tensor`.\n",
            "          y:  Numeric `Tensor`, same dtype as and broadcastable to `x`.\n",
            "          message: A string to prefix to the default message.\n",
            "          summarize: Print this many entries of each tensor.\n",
            "          name: A name for this operation (optional).  Defaults to \"assert_greater\".\n",
            "        \n",
            "        Returns:\n",
            "          Op that raises `InvalidArgumentError` if `x > y` is False. This can be\n",
            "            used with `tf.control_dependencies` inside of `tf.function`s to block\n",
            "            followup computation until the check has executed.\n",
            "          @compatibility(eager)\n",
            "          returns None\n",
            "          @end_compatibility\n",
            "        \n",
            "        Raises:\n",
            "          InvalidArgumentError: if the check can be performed immediately and\n",
            "            `x > y` is False. The check can be performed immediately during eager\n",
            "            execution or if `x` and `y` are statically known.\n",
            "    \n",
            "    assert_less = assert_less_v2(x, y, message=None, summarize=None, name=None)\n",
            "        Assert the condition `x < y` holds element-wise.\n",
            "        \n",
            "        This Op checks that `x[i] < y[i]` holds for every pair of (possibly\n",
            "        broadcast) elements of `x` and `y`. If both `x` and `y` are empty, this is\n",
            "        trivially satisfied.\n",
            "        \n",
            "        If `x` is not less than `y` element-wise, `message`, as well as the first\n",
            "        `summarize` entries of `x` and `y` are printed, and `InvalidArgumentError` is\n",
            "        raised.\n",
            "        \n",
            "        Args:\n",
            "          x:  Numeric `Tensor`.\n",
            "          y:  Numeric `Tensor`, same dtype as and broadcastable to `x`.\n",
            "          message: A string to prefix to the default message.\n",
            "          summarize: Print this many entries of each tensor.\n",
            "          name: A name for this operation (optional).  Defaults to \"assert_less\".\n",
            "        \n",
            "        Returns:\n",
            "          Op that raises `InvalidArgumentError` if `x < y` is False.\n",
            "          This can be used with `tf.control_dependencies` inside of `tf.function`s\n",
            "          to block followup computation until the check has executed.\n",
            "          @compatibility(eager)\n",
            "          returns None\n",
            "          @end_compatibility\n",
            "        \n",
            "        Raises:\n",
            "          InvalidArgumentError: if the check can be performed immediately and\n",
            "            `x < y` is False. The check can be performed immediately during eager\n",
            "            execution or if `x` and `y` are statically known.\n",
            "    \n",
            "    assert_rank = assert_rank_v2(x, rank, message=None, name=None)\n",
            "        Assert that `x` has rank equal to `rank`.\n",
            "        \n",
            "        This Op checks that the rank of `x` is equal to `rank`.\n",
            "        \n",
            "        If `x` has a different rank, `message`, as well as the shape of `x` are\n",
            "        printed, and `InvalidArgumentError` is raised.\n",
            "        \n",
            "        Args:\n",
            "          x: `Tensor`.\n",
            "          rank: Scalar integer `Tensor`.\n",
            "          message: A string to prefix to the default message.\n",
            "          name: A name for this operation (optional). Defaults to\n",
            "            \"assert_rank\".\n",
            "        \n",
            "        Returns:\n",
            "          Op raising `InvalidArgumentError` unless `x` has specified rank.\n",
            "          If static checks determine `x` has correct rank, a `no_op` is returned.\n",
            "          This can be used with `tf.control_dependencies` inside of `tf.function`s\n",
            "          to block followup computation until the check has executed.\n",
            "          @compatibility(eager)\n",
            "          returns None\n",
            "          @end_compatibility\n",
            "        \n",
            "        Raises:\n",
            "          InvalidArgumentError: if the check can be performed immediately and\n",
            "            `x` does not have rank `rank`. The check can be performed immediately\n",
            "            during eager execution or if the shape of `x` is statically known.\n",
            "    \n",
            "    atan(x, name=None)\n",
            "        Computes the trignometric inverse tangent of x element-wise.\n",
            "        \n",
            "        The `tf.math.atan` operation returns the inverse of `tf.math.tan`, such that\n",
            "        if `y = tf.math.tan(x)` then, `x = tf.math.atan(y)`.\n",
            "        \n",
            "        **Note**: The output of `tf.math.atan` will lie within the invertible range\n",
            "        of tan, i.e (-pi/2, pi/2).\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]\n",
            "        x = tf.constant([1.047, 0.785])\n",
            "        y = tf.math.tan(x) # [1.731261, 0.99920404]\n",
            "        \n",
            "        tf.math.atan(y) # [1.047, 0.785] = x\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    atan2(y, x, name=None)\n",
            "        Computes arctangent of `y/x` element-wise, respecting signs of the arguments.\n",
            "        \n",
            "        This is the angle \\( \\theta \\in [-\\pi, \\pi] \\) such that\n",
            "        \\[ x = r \\cos(\\theta) \\]\n",
            "        and\n",
            "        \\[ y = r \\sin(\\theta) \\]\n",
            "        where \\(r = \\sqrt(x^2 + y^2) \\).\n",
            "        \n",
            "        Args:\n",
            "          y: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n",
            "          x: A `Tensor`. Must have the same type as `y`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `y`.\n",
            "    \n",
            "    atanh(x, name=None)\n",
            "        Computes inverse hyperbolic tangent of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes inverse hyperbolic tangent\n",
            "          for every element in the tensor. Input range is `[-1,1]` and output range is\n",
            "          `[-inf, inf]`. If input is `-1`, output will be `-inf` and if the\n",
            "          input is `1`, output will be `inf`. Values outside the range will have\n",
            "          `nan` as output.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -1, -0.5, 1, 0, 0.5, 10, float(\"inf\")])\n",
            "          tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    batch_to_space = batch_to_space_v2(input, block_shape, crops, name=None)\n",
            "        BatchToSpace for N-D tensors of type T.\n",
            "        \n",
            "        This operation reshapes the \"batch\" dimension 0 into `M + 1` dimensions of\n",
            "        shape `block_shape + [batch]`, interleaves these blocks back into the grid\n",
            "        defined by the spatial dimensions `[1, ..., M]`, to obtain a result with the\n",
            "        same rank as the input.  The spatial dimensions of this intermediate result\n",
            "        are then optionally cropped according to `crops` to produce the output.  This\n",
            "        is the reverse of SpaceToBatch (see `tf.space_to_batch`).\n",
            "        \n",
            "        Args:\n",
            "          input: A N-D `Tensor` with shape `input_shape = [batch] + spatial_shape +\n",
            "            remaining_shape`, where `spatial_shape` has M dimensions.\n",
            "          block_shape: A 1-D `Tensor` with shape [M]. Must be one of the following\n",
            "            types: `int32`, `int64`. All values must be >= 1. For backwards\n",
            "            compatibility with TF 1.0, this parameter may be an int, in which case it\n",
            "            is converted to\n",
            "            `numpy.array([block_shape, block_shape],\n",
            "            dtype=numpy.int64)`.\n",
            "          crops: A  2-D `Tensor` with shape `[M, 2]`. Must be one of the\n",
            "            following types: `int32`, `int64`. All values must be >= 0.\n",
            "            `crops[i] = [crop_start, crop_end]` specifies the amount to crop from\n",
            "            input dimension `i + 1`, which corresponds to spatial dimension `i`.\n",
            "            It is required that\n",
            "            `crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1]`.\n",
            "            This operation is equivalent to the following steps:\n",
            "            1. Reshape `input` to `reshaped` of shape: [block_shape[0], ...,\n",
            "              block_shape[M-1], batch / prod(block_shape), input_shape[1], ...,\n",
            "              input_shape[N-1]]\n",
            "            2. Permute dimensions of `reshaped` to produce `permuted` of shape\n",
            "               [batch / prod(block_shape),  input_shape[1], block_shape[0], ...,\n",
            "               input_shape[M], block_shape[M-1], input_shape[M+1],\n",
            "              ..., input_shape[N-1]]\n",
            "            3. Reshape `permuted` to produce `reshaped_permuted` of shape\n",
            "               [batch / prod(block_shape), input_shape[1] * block_shape[0], ...,\n",
            "               input_shape[M] * block_shape[M-1], input_shape[M+1], ...,\n",
            "               input_shape[N-1]]\n",
            "            4. Crop the start and end of dimensions `[1, ..., M]` of\n",
            "               `reshaped_permuted` according to `crops` to produce the output\n",
            "               of shape:\n",
            "               [batch / prod(block_shape),  input_shape[1] *\n",
            "                 block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] *\n",
            "                 block_shape[M-1] - crops[M-1,0] - crops[M-1,1],  input_shape[M+1],\n",
            "                 ..., input_shape[N-1]]\n",
            "            Some Examples:\n",
            "            (1) For the following input of shape `[4, 1, 1, 1]`,\n",
            "               `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:\n",
            "               ```python\n",
            "               [[[[1]]],\n",
            "                [[[2]]],\n",
            "                [[[3]]],\n",
            "                [[[4]]]]\n",
            "               ```\n",
            "               The output tensor has shape `[1, 2, 2, 1]` and value:\n",
            "               ``` x = [[[[1], [2]],\n",
            "                         [[3], [4]]]] ```\n",
            "            (2) For the following input of shape `[4, 1, 1, 3]`,\n",
            "               `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:\n",
            "               ```python\n",
            "               [[[1,  2,   3]],\n",
            "                [[4,  5,   6]],\n",
            "                [[7,  8,   9]],\n",
            "                [[10, 11, 12]]]\n",
            "               ```\n",
            "               The output tensor has shape `[1, 2, 2, 3]` and value:\n",
            "               ```python\n",
            "               x = [[[[1, 2, 3], [4,  5,  6 ]],\n",
            "                     [[7, 8, 9], [10, 11, 12]]]]\n",
            "               ```\n",
            "            (3) For the following\n",
            "               input of shape `[4, 2, 2, 1]`,\n",
            "               `block_shape = [2, 2]`, and `crops = [[0, 0], [0, 0]]`:\n",
            "               ```python\n",
            "               x = [[[[1], [3]], [[ 9], [11]]],\n",
            "                    [[[2], [4]], [[10], [12]]],\n",
            "                    [[[5], [7]], [[13], [15]]],\n",
            "                    [[[6], [8]], [[14], [16]]]]\n",
            "               ```\n",
            "               The output tensor has shape `[1, 4, 4, 1]` and value:\n",
            "               ```python\n",
            "               x = [[[1],  [2],  [ 3], [ 4]],\n",
            "                    [[5],  [6],  [ 7], [ 8]],\n",
            "                    [[9],  [10], [11], [12]],\n",
            "                    [[13], [14], [15], [16]]]\n",
            "               ```\n",
            "             (4) For the following input of shape\n",
            "                `[8, 1, 3, 1]`,\n",
            "                `block_shape = [2, 2]`, and `crops = [[0, 0], [2, 0]]`:\n",
            "                ```python\n",
            "                x = [[[[0], [ 1], [ 3]]],\n",
            "                     [[[0], [ 9], [11]]],\n",
            "                     [[[0], [ 2], [ 4]]],\n",
            "                     [[[0], [10], [12]]],\n",
            "                     [[[0], [ 5], [ 7]]],\n",
            "                     [[[0], [13], [15]]],\n",
            "                     [[[0], [ 6], [ 8]]],\n",
            "                     [[[0], [14], [16]]]]\n",
            "                ```\n",
            "                The output tensor has shape `[2, 2, 4, 1]` and value:\n",
            "                ```python\n",
            "                x = [[[[ 1], [ 2], [ 3], [ 4]],\n",
            "                      [[ 5], [ 6], [ 7], [ 8]]],\n",
            "                     [[[ 9], [10], [11], [12]],\n",
            "                      [[13], [14], [15], [16]]]] ```\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    bitcast(input, type, name=None)\n",
            "        Bitcasts a tensor from one type to another without copying data.\n",
            "        \n",
            "        Given a tensor `input`, this operation returns a tensor that has the same buffer\n",
            "        data as `input` with datatype `type`.\n",
            "        \n",
            "        If the input datatype `T` is larger than the output datatype `type` then the\n",
            "        shape changes from [...] to [..., sizeof(`T`)/sizeof(`type`)].\n",
            "        \n",
            "        If `T` is smaller than `type`, the operator requires that the rightmost\n",
            "        dimension be equal to sizeof(`type`)/sizeof(`T`). The shape then goes from\n",
            "        [..., sizeof(`type`)/sizeof(`T`)] to [...].\n",
            "        \n",
            "        tf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype\n",
            "        (e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast()\n",
            "        gives module error.\n",
            "        For example,\n",
            "        \n",
            "        Example 1:\n",
            "        \n",
            "        >>> a = [1., 2., 3.]\n",
            "        >>> equality_bitcast = tf.bitcast(a, tf.complex128)\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]\n",
            "        >>> equality_cast = tf.cast(a, tf.complex128)\n",
            "        >>> print(equality_cast)\n",
            "        tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)\n",
            "        \n",
            "        Example 2:\n",
            "        \n",
            "        >>> tf.bitcast(tf.constant(0xffffffff, dtype=tf.uint32), tf.uint8)\n",
            "        <tf.Tensor: shape=(4,), dtype=uint8, numpy=array([255, 255, 255, 255], dtype=uint8)>\n",
            "        \n",
            "        Example 3:\n",
            "        \n",
            "        >>> x = [1., 2., 3.]\n",
            "        >>> y = [0., 2., 3.]\n",
            "        >>> equality= tf.equal(x,y)\n",
            "        >>> equality_cast = tf.cast(equality,tf.float32)\n",
            "        >>> equality_bitcast = tf.bitcast(equality_cast,tf.uint8)\n",
            "        >>> print(equality)\n",
            "        tf.Tensor([False True True], shape=(3,), dtype=bool)\n",
            "        >>> print(equality_cast)\n",
            "        tf.Tensor([0. 1. 1.], shape=(3,), dtype=float32)\n",
            "        >>> print(equality_bitcast)\n",
            "        tf.Tensor(\n",
            "            [[  0   0   0   0]\n",
            "             [  0   0 128  63]\n",
            "             [  0   0 128  63]], shape=(3, 4), dtype=uint8)\n",
            "        \n",
            "        *NOTE*: Bitcast is implemented as a low-level cast, so machines with different\n",
            "        endian orderings will give different results.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int64`, `int32`, `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `complex64`, `complex128`, `qint8`, `quint8`, `qint16`, `quint16`, `qint32`.\n",
            "          type: A `tf.DType` from: `tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `type`.\n",
            "    \n",
            "    boolean_mask = boolean_mask_v2(tensor, mask, axis=None, name='boolean_mask')\n",
            "        Apply boolean mask to tensor.\n",
            "        \n",
            "        Numpy equivalent is `tensor[mask]`.\n",
            "        \n",
            "        ```python\n",
            "        # 1-D example\n",
            "        tensor = [0, 1, 2, 3]\n",
            "        mask = np.array([True, False, True, False])\n",
            "        boolean_mask(tensor, mask)  # [0, 2]\n",
            "        ```\n",
            "        \n",
            "        In general, `0 < dim(mask) = K <= dim(tensor)`, and `mask`'s shape must match\n",
            "        the first K dimensions of `tensor`'s shape.  We then have:\n",
            "          `boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]`\n",
            "        where `(i1,...,iK)` is the ith `True` entry of `mask` (row-major order).\n",
            "        The `axis` could be used with `mask` to indicate the axis to mask from.\n",
            "        In that case, `axis + dim(mask) <= dim(tensor)` and `mask`'s shape must match\n",
            "        the first `axis + dim(mask)` dimensions of `tensor`'s shape.\n",
            "        \n",
            "        See also: `tf.ragged.boolean_mask`, which can be applied to both dense and\n",
            "        ragged tensors, and can be used if you need to preserve the masked dimensions\n",
            "        of `tensor` (rather than flattening them, as `tf.boolean_mask` does).\n",
            "        \n",
            "        Args:\n",
            "          tensor:  N-D tensor.\n",
            "          mask:  K-D boolean tensor, K <= N and K must be known statically.\n",
            "          axis:  A 0-D int Tensor representing the axis in `tensor` to mask from. By\n",
            "            default, axis is 0 which will mask from the first dimension. Otherwise K +\n",
            "            axis <= N.\n",
            "          name:  A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          (N-K+1)-dimensional tensor populated by entries in `tensor` corresponding\n",
            "          to `True` values in `mask`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError:  If shapes do not conform.\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        ```python\n",
            "        # 2-D example\n",
            "        tensor = [[1, 2], [3, 4], [5, 6]]\n",
            "        mask = np.array([True, False, True])\n",
            "        boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]\n",
            "        ```\n",
            "    \n",
            "    broadcast_dynamic_shape(shape_x, shape_y)\n",
            "        Computes the shape of a broadcast given symbolic shapes.\n",
            "        \n",
            "        When shape_x and shape_y are Tensors representing shapes (i.e. the result of\n",
            "        calling tf.shape on another Tensor) this computes a Tensor which is the shape\n",
            "        of the result of a broadcasting op applied in tensors of shapes shape_x and\n",
            "        shape_y.\n",
            "        \n",
            "        For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a\n",
            "        Tensor whose value is [5, 2, 3].\n",
            "        \n",
            "        This is useful when validating the result of a broadcasting operation when the\n",
            "        tensors do not have statically known shapes.\n",
            "        \n",
            "        Args:\n",
            "          shape_x: A rank 1 integer `Tensor`, representing the shape of x.\n",
            "          shape_y: A rank 1 integer `Tensor`, representing the shape of y.\n",
            "        \n",
            "        Returns:\n",
            "          A rank 1 integer `Tensor` representing the broadcasted shape.\n",
            "    \n",
            "    broadcast_static_shape(shape_x, shape_y)\n",
            "        Computes the shape of a broadcast given known shapes.\n",
            "        \n",
            "        When shape_x and shape_y are fully known TensorShapes this computes a\n",
            "        TensorShape which is the shape of the result of a broadcasting op applied in\n",
            "        tensors of shapes shape_x and shape_y.\n",
            "        \n",
            "        For example, if shape_x is [1, 2, 3] and shape_y is [5, 1, 3], the result is a\n",
            "        TensorShape whose value is [5, 2, 3].\n",
            "        \n",
            "        This is useful when validating the result of a broadcasting operation when the\n",
            "        tensors have statically known shapes.\n",
            "        \n",
            "        Args:\n",
            "          shape_x: A `TensorShape`\n",
            "          shape_y: A `TensorShape`\n",
            "        \n",
            "        Returns:\n",
            "          A `TensorShape` representing the broadcasted shape.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If the two shapes can not be broadcasted.\n",
            "    \n",
            "    broadcast_to(input, shape, name=None)\n",
            "        Broadcast an array for a compatible shape.\n",
            "        \n",
            "        Broadcasting is the process of making arrays to have compatible shapes\n",
            "        for arithmetic operations. Two shapes are compatible if for each\n",
            "        dimension pair they are either equal or one of them is one. When trying\n",
            "        to broadcast a Tensor to a shape, it starts with the trailing dimensions,\n",
            "        and works its way forward.\n",
            "        \n",
            "        For example,\n",
            "        \n",
            "        >>> x = tf.constant([1, 2, 3])\n",
            "        >>> y = tf.broadcast_to(x, [3, 3])\n",
            "        >>> print(y)\n",
            "        tf.Tensor(\n",
            "            [[1 2 3]\n",
            "             [1 2 3]\n",
            "             [1 2 3]], shape=(3, 3), dtype=int32)\n",
            "        \n",
            "        In the above example, the input Tensor with the shape of `[1, 3]`\n",
            "        is broadcasted to output Tensor with shape of `[3, 3]`.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. A Tensor to broadcast.\n",
            "          shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            An 1-D `int` Tensor. The shape of the desired output.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    case = case_v2(pred_fn_pairs, default=None, exclusive=False, strict=False, name='case')\n",
            "        Create a case operation.\n",
            "        \n",
            "        See also `tf.switch_case`.\n",
            "        \n",
            "        The `pred_fn_pairs` parameter is a list of pairs of size N.\n",
            "        Each pair contains a boolean scalar tensor and a python callable that\n",
            "        creates the tensors to be returned if the boolean evaluates to True.\n",
            "        `default` is a callable generating a list of tensors. All the callables\n",
            "        in `pred_fn_pairs` as well as `default` (if provided) should return the same\n",
            "        number and types of tensors.\n",
            "        \n",
            "        If `exclusive==True`, all predicates are evaluated, and an exception is\n",
            "        thrown if more than one of the predicates evaluates to `True`.\n",
            "        If `exclusive==False`, execution stops at the first predicate which\n",
            "        evaluates to True, and the tensors generated by the corresponding function\n",
            "        are returned immediately. If none of the predicates evaluate to True, this\n",
            "        operation returns the tensors generated by `default`.\n",
            "        \n",
            "        `tf.case` supports nested structures as implemented in\n",
            "        `tf.contrib.framework.nest`. All of the callables must return the same\n",
            "        (possibly nested) value structure of lists, tuples, and/or named tuples.\n",
            "        Singleton lists and tuples form the only exceptions to this: when returned by\n",
            "        a callable, they are implicitly unpacked to single values. This\n",
            "        behavior is disabled by passing `strict=True`.\n",
            "        \n",
            "        @compatibility(v2)\n",
            "        `pred_fn_pairs` could be a dictionary in v1. However, tf.Tensor and\n",
            "        tf.Variable are no longer hashable in v2, so cannot be used as a key for a\n",
            "        dictionary.  Please use a list or a tuple instead.\n",
            "        @end_compatibility\n",
            "        \n",
            "        \n",
            "        **Example 1:**\n",
            "        \n",
            "        Pseudocode:\n",
            "        \n",
            "        ```\n",
            "        if (x < y) return 17;\n",
            "        else return 23;\n",
            "        ```\n",
            "        \n",
            "        Expressions:\n",
            "        \n",
            "        ```python\n",
            "        f1 = lambda: tf.constant(17)\n",
            "        f2 = lambda: tf.constant(23)\n",
            "        r = tf.case([(tf.less(x, y), f1)], default=f2)\n",
            "        ```\n",
            "        \n",
            "        **Example 2:**\n",
            "        \n",
            "        Pseudocode:\n",
            "        \n",
            "        ```\n",
            "        if (x < y && x > z) raise OpError(\"Only one predicate may evaluate to True\");\n",
            "        if (x < y) return 17;\n",
            "        else if (x > z) return 23;\n",
            "        else return -1;\n",
            "        ```\n",
            "        \n",
            "        Expressions:\n",
            "        \n",
            "        ```python\n",
            "        def f1(): return tf.constant(17)\n",
            "        def f2(): return tf.constant(23)\n",
            "        def f3(): return tf.constant(-1)\n",
            "        r = tf.case([(tf.less(x, y), f1), (tf.greater(x, z), f2)],\n",
            "                 default=f3, exclusive=True)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          pred_fn_pairs: List of pairs of a boolean scalar tensor and a callable which\n",
            "            returns a list of tensors.\n",
            "          default: Optional callable that returns a list of tensors.\n",
            "          exclusive: True iff at most one predicate is allowed to evaluate to `True`.\n",
            "          strict: A boolean that enables/disables 'strict' mode; see above.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The tensors returned by the first pair whose predicate evaluated to True, or\n",
            "          those returned by `default` if none does.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `pred_fn_pairs` is not a list/tuple.\n",
            "          TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.\n",
            "          TypeError: If `fns[i]` is not callable for any i, or `default` is not\n",
            "                     callable.\n",
            "    \n",
            "    cast(x, dtype, name=None)\n",
            "        Casts a tensor to a new type.\n",
            "        \n",
            "        The operation casts `x` (in case of `Tensor`) or `x.values`\n",
            "        (in case of `SparseTensor` or `IndexedSlices`) to `dtype`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
            "        >>> tf.dtypes.cast(x, tf.int32)\n",
            "        <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n",
            "        \n",
            "        The operation supports data types (for `x` and `dtype`) of\n",
            "        `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,\n",
            "        `float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.\n",
            "        In case of casting from complex types (`complex64`, `complex128`) to real\n",
            "        types, only the real part of `x` is returned. In case of casting from real\n",
            "        types to complex types (`complex64`, `complex128`), the imaginary part of the\n",
            "        returned value is set to `0`. The handling of complex types here matches the\n",
            "        behavior of numpy.\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could\n",
            "            be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,\n",
            "            `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,\n",
            "            `bfloat16`.\n",
            "          dtype: The destination type. The list of supported dtypes is the same as\n",
            "            `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and\n",
            "            same type as `dtype`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `x` cannot be cast to the `dtype`.\n",
            "    \n",
            "    clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None)\n",
            "        Clips values of multiple tensors by the ratio of the sum of their norms.\n",
            "        \n",
            "        Given a tuple or list of tensors `t_list`, and a clipping ratio `clip_norm`,\n",
            "        this operation returns a list of clipped tensors `list_clipped`\n",
            "        and the global norm (`global_norm`) of all tensors in `t_list`. Optionally,\n",
            "        if you've already computed the global norm for `t_list`, you can specify\n",
            "        the global norm with `use_norm`.\n",
            "        \n",
            "        To perform the clipping, the values `t_list[i]` are set to:\n",
            "        \n",
            "            t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
            "        \n",
            "        where:\n",
            "        \n",
            "            global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))\n",
            "        \n",
            "        If `clip_norm > global_norm` then the entries in `t_list` remain as they are,\n",
            "        otherwise they're all shrunk by the global ratio.\n",
            "        \n",
            "        If `global_norm == infinity` then the entries in `t_list` are all set to `NaN`\n",
            "        to signal that an error occurred.\n",
            "        \n",
            "        Any of the entries of `t_list` that are of type `None` are ignored.\n",
            "        \n",
            "        This is the correct way to perform gradient clipping (Pascanu et al., 2012).\n",
            "        \n",
            "        However, it is slower than `clip_by_norm()` because all the parameters must be\n",
            "        ready before the clipping operation can be performed.\n",
            "        \n",
            "        Args:\n",
            "          t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None.\n",
            "          clip_norm: A 0-D (scalar) `Tensor` > 0. The clipping ratio.\n",
            "          use_norm: A 0-D (scalar) `Tensor` of type `float` (optional). The global\n",
            "            norm to use. If not provided, `global_norm()` is used to compute the norm.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          list_clipped: A list of `Tensors` of the same type as `list_t`.\n",
            "          global_norm: A 0-D (scalar) `Tensor` representing the global norm.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `t_list` is not a sequence.\n",
            "        \n",
            "        References:\n",
            "          On the difficulty of training Recurrent Neural Networks:\n",
            "            [Pascanu et al., 2012](http://proceedings.mlr.press/v28/pascanu13.html)\n",
            "            ([pdf](http://proceedings.mlr.press/v28/pascanu13.pdf))\n",
            "    \n",
            "    clip_by_norm(t, clip_norm, axes=None, name=None)\n",
            "        Clips tensor values to a maximum L2-norm.\n",
            "        \n",
            "        Given a tensor `t`, and a maximum clip value `clip_norm`, this operation\n",
            "        normalizes `t` so that its L2-norm is less than or equal to `clip_norm`,\n",
            "        along the dimensions given in `axes`. Specifically, in the default case\n",
            "        where all dimensions are used for calculation, if the L2-norm of `t` is\n",
            "        already less than or equal to `clip_norm`, then `t` is not modified. If\n",
            "        the L2-norm is greater than `clip_norm`, then this operation returns a\n",
            "        tensor of the same type and shape as `t` with its values set to:\n",
            "        \n",
            "        `t * clip_norm / l2norm(t)`\n",
            "        \n",
            "        In this case, the L2-norm of the output tensor is `clip_norm`.\n",
            "        \n",
            "        As another example, if `t` is a matrix and `axes == [1]`, then each row\n",
            "        of the output will have L2-norm less than or equal to `clip_norm`. If\n",
            "        `axes == [0]` instead, each column of the output will be clipped.\n",
            "        \n",
            "        This operation is typically used to clip gradients before applying them with\n",
            "        an optimizer.\n",
            "        \n",
            "        Args:\n",
            "          t: A `Tensor` or `IndexedSlices`.\n",
            "          clip_norm: A 0-D (scalar) `Tensor` > 0. A maximum clipping value.\n",
            "          axes: A 1-D (vector) `Tensor` of type int32 containing the dimensions\n",
            "            to use for computing the L2-norm. If `None` (the default), uses all\n",
            "            dimensions.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A clipped `Tensor` or `IndexedSlices`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If the clip_norm tensor is not a 0-D scalar tensor.\n",
            "          TypeError: If dtype of the input is not a floating point or\n",
            "            complex type.\n",
            "    \n",
            "    clip_by_value(t, clip_value_min, clip_value_max, name=None)\n",
            "        Clips tensor values to a specified min and max.\n",
            "        \n",
            "        Given a tensor `t`, this operation returns a tensor of the same type and\n",
            "        shape as `t` with its values clipped to `clip_value_min` and `clip_value_max`.\n",
            "        Any values less than `clip_value_min` are set to `clip_value_min`. Any values\n",
            "        greater than `clip_value_max` are set to `clip_value_max`.\n",
            "        \n",
            "        Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for\n",
            "        correct results.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        Basic usage passes a scalar as the min and max value.\n",
            "        \n",
            "        >>> t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\n",
            "        >>> t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)\n",
            "        >>> t2.numpy()\n",
            "        array([[-1., -1.,  0.],\n",
            "               [ 0.,  1.,  1.]], dtype=float32)\n",
            "        \n",
            "        The min and max can be the same size as `t`, or broadcastable to that size.\n",
            "        \n",
            "        >>> t = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\n",
            "        >>> clip_min = [[2],[1]]\n",
            "        >>> t3 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\n",
            "        >>> t3.numpy()\n",
            "        array([[ 2.,  2., 10.],\n",
            "               [ 1.,  1., 10.]], dtype=float32)\n",
            "        \n",
            "        Broadcasting fails, intentionally, if you would expand the dimensions of `t`\n",
            "        \n",
            "        >>> t = tf.constant([[-1, 0., 10.], [-1, 0, 10]])\n",
            "        >>> clip_min = [[[2, 1]]] # Has a third axis\n",
            "        >>> t4 = tf.clip_by_value(t, clip_value_min=clip_min, clip_value_max=100)\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        InvalidArgumentError: Incompatible shapes: [2,3] vs. [1,1,2]\n",
            "        \n",
            "        It throws a `TypeError` if you try to clip an `int` to a `float` value\n",
            "        (`tf.cast` the input to `float` first).\n",
            "        \n",
            "        >>> t = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)\n",
            "        >>> t5 = tf.clip_by_value(t, clip_value_min=-3.1, clip_value_max=3.1)\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        TypeError: Cannot convert ...\n",
            "        \n",
            "        \n",
            "        Args:\n",
            "          t: A `Tensor` or `IndexedSlices`.\n",
            "          clip_value_min: The minimum value to clip to. A scalar `Tensor` or one that\n",
            "            is broadcastable to the shape of `t`.\n",
            "          clip_value_max: The minimum value to clip to. A scalar `Tensor` or one that\n",
            "            is broadcastable to the shape of `t`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A clipped `Tensor` or `IndexedSlices`.\n",
            "        \n",
            "        Raises:\n",
            "          `tf.errors.InvalidArgumentError`: If the clip tensors would trigger array\n",
            "            broadcasting that would make the returned tensor larger than the input.\n",
            "          TypeError: If dtype of the input is `int32` and dtype of\n",
            "            the `clip_value_min` or `clip_value_max` is `float32`\n",
            "    \n",
            "    complex(real, imag, name=None)\n",
            "        Converts two real numbers to a complex number.\n",
            "        \n",
            "        Given a tensor `real` representing the real part of a complex number, and a\n",
            "        tensor `imag` representing the imaginary part of a complex number, this\n",
            "        operation returns complex numbers elementwise of the form \\\\(a + bj\\\\), where\n",
            "        *a* represents the `real` part and *b* represents the `imag` part.\n",
            "        \n",
            "        The input tensors `real` and `imag` must have the same shape.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        real = tf.constant([2.25, 3.25])\n",
            "        imag = tf.constant([4.75, 5.75])\n",
            "        tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          real: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n",
            "          imag: A `Tensor`. Must have the same type as `real`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `complex64` or `complex128`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: Real and imag must be correct types\n",
            "    \n",
            "    concat(values, axis, name='concat')\n",
            "        Concatenates tensors along one dimension.\n",
            "        \n",
            "        See also `tf.tile`, `tf.stack`, `tf.repeat`.\n",
            "        \n",
            "        Concatenates the list of tensors `values` along dimension `axis`.  If\n",
            "        `values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated\n",
            "        result has shape\n",
            "        \n",
            "            [D0, D1, ... Raxis, ...Dn]\n",
            "        \n",
            "        where\n",
            "        \n",
            "            Raxis = sum(Daxis(i))\n",
            "        \n",
            "        That is, the data from the input tensors is joined along the `axis`\n",
            "        dimension.\n",
            "        \n",
            "        The number of dimensions of the input tensors must match, and all dimensions\n",
            "        except `axis` must be equal.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> t1 = [[1, 2, 3], [4, 5, 6]]\n",
            "        >>> t2 = [[7, 8, 9], [10, 11, 12]]\n",
            "        >>> concat([t1, t2], 0)\n",
            "        <tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
            "        array([[ 1,  2,  3],\n",
            "               [ 4,  5,  6],\n",
            "               [ 7,  8,  9],\n",
            "               [10, 11, 12]], dtype=int32)>\n",
            "        \n",
            "        >>> concat([t1, t2], 1)\n",
            "        <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
            "        array([[ 1,  2,  3,  7,  8,  9],\n",
            "               [ 4,  5,  6, 10, 11, 12]], dtype=int32)>\n",
            "        \n",
            "        As in Python, the `axis` could also be negative numbers. Negative `axis`\n",
            "        are interpreted as counting from the end of the rank, i.e.,\n",
            "         `axis + rank(values)`-th dimension.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\n",
            "        >>> t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\n",
            "        >>> tf.concat([t1, t2], -1)\n",
            "        <tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=\n",
            "          array([[[ 1,  2,  7,  4],\n",
            "                  [ 2,  3,  8,  4]],\n",
            "                 [[ 4,  4,  2, 10],\n",
            "                  [ 5,  3, 15, 11]]], dtype=int32)>\n",
            "        \n",
            "        Note: If you are concatenating along a new axis consider using stack.\n",
            "        E.g.\n",
            "        \n",
            "        ```python\n",
            "        tf.concat([tf.expand_dims(t, axis) for t in tensors], axis)\n",
            "        ```\n",
            "        \n",
            "        can be rewritten as\n",
            "        \n",
            "        ```python\n",
            "        tf.stack(tensors, axis=axis)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          values: A list of `Tensor` objects or a single `Tensor`.\n",
            "          axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be\n",
            "            in the range `[-rank(values), rank(values))`. As in Python, indexing for\n",
            "            axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers\n",
            "            to `axis`-th dimension. And negative axis refers to `axis +\n",
            "            rank(values)`-th dimension.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` resulting from concatenation of the input tensors.\n",
            "    \n",
            "    cond = cond_for_tf_v2(pred, true_fn=None, false_fn=None, name=None)\n",
            "        Return `true_fn()` if the predicate `pred` is true else `false_fn()`.\n",
            "        \n",
            "        `true_fn` and `false_fn` both return lists of output tensors. `true_fn` and\n",
            "        `false_fn` must have the same non-zero number and type of outputs.\n",
            "        \n",
            "        **WARNING**: Any Tensors or Operations created outside of `true_fn` and\n",
            "        `false_fn` will be executed regardless of which branch is selected at runtime.\n",
            "        \n",
            "        Although this behavior is consistent with the dataflow model of TensorFlow,\n",
            "        it has frequently surprised users who expected a lazier semantics.\n",
            "        Consider the following simple program:\n",
            "        \n",
            "        ```python\n",
            "        z = tf.multiply(a, b)\n",
            "        result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\n",
            "        ```\n",
            "        \n",
            "        If `x < y`, the `tf.add` operation will be executed and `tf.square`\n",
            "        operation will not be executed. Since `z` is needed for at least one\n",
            "        branch of the `cond`, the `tf.multiply` operation is always executed,\n",
            "        unconditionally.\n",
            "        \n",
            "        Note that `cond` calls `true_fn` and `false_fn` *exactly once* (inside the\n",
            "        call to `cond`, and not at all during `Session.run()`). `cond`\n",
            "        stitches together the graph fragments created during the `true_fn` and\n",
            "        `false_fn` calls with some additional graph nodes to ensure that the right\n",
            "        branch gets executed depending on the value of `pred`.\n",
            "        \n",
            "        `tf.cond` supports nested structures as implemented in\n",
            "        `tensorflow.python.util.nest`. Both `true_fn` and `false_fn` must return the\n",
            "        same (possibly nested) value structure of lists, tuples, and/or named tuples.\n",
            "        Singleton lists and tuples form the only exceptions to this: when returned by\n",
            "        `true_fn` and/or `false_fn`, they are implicitly unpacked to single values.\n",
            "        \n",
            "        Note: It is illegal to \"directly\" use tensors created inside a cond branch\n",
            "        outside it, e.g. by storing a reference to a branch tensor in the python\n",
            "        state. If you need to use a tensor created in a branch function you should\n",
            "        return it as an output of the branch function and use the output from\n",
            "        `tf.cond` instead.\n",
            "        \n",
            "        Args:\n",
            "          pred: A scalar determining whether to return the result of `true_fn` or\n",
            "            `false_fn`.\n",
            "          true_fn: The callable to be performed if pred is true.\n",
            "          false_fn: The callable to be performed if pred is false.\n",
            "          name: Optional name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          Tensors returned by the call to either `true_fn` or `false_fn`. If the\n",
            "          callables return a singleton list, the element is extracted from the list.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `true_fn` or `false_fn` is not callable.\n",
            "          ValueError: if `true_fn` and `false_fn` do not return the same number of\n",
            "            tensors, or return tensors of different types.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant(2)\n",
            "        y = tf.constant(5)\n",
            "        def f1(): return tf.multiply(x, 17)\n",
            "        def f2(): return tf.add(y, 23)\n",
            "        r = tf.cond(tf.less(x, y), f1, f2)\n",
            "        # r is set to f1().\n",
            "        # Operations in f2 (e.g., tf.add) are not executed.\n",
            "        ```\n",
            "    \n",
            "    constant(value, dtype=None, shape=None, name='Const')\n",
            "        Creates a constant tensor from a tensor-like object.\n",
            "        \n",
            "        Note: All eager `tf.Tensor` values are immutable (in contrast to\n",
            "        `tf.Variable`). There is nothing especially _constant_ about the value\n",
            "        returned from `tf.constant`. This function it is not fundamentally different\n",
            "        from `tf.convert_to_tensor`. The name `tf.constant` comes from the symbolic\n",
            "        APIs (like `tf.data` or keras functional models) where the `value` is embeded\n",
            "        in a `Const` node in the `tf.Graph`. `tf.constant` is useful for asserting\n",
            "        that the value can be embedded that way.\n",
            "        \n",
            "        If the argument `dtype` is not specified, then the type is inferred from\n",
            "        the type of `value`.\n",
            "        \n",
            "        >>> # Constant 1-D Tensor from a python list.\n",
            "        >>> tf.constant([1, 2, 3, 4, 5, 6])\n",
            "        <tf.Tensor: shape=(6,), dtype=int32,\n",
            "            numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
            "        >>> # Or a numpy array\n",
            "        >>> a = np.array([[1, 2, 3], [4, 5, 6]])\n",
            "        >>> tf.constant(a)\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
            "          array([[1, 2, 3],\n",
            "                 [4, 5, 6]])>\n",
            "        \n",
            "        If `dtype` is specified the resulting tensor values are cast to the requested\n",
            "        `dtype`.\n",
            "        \n",
            "        >>> tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float64)\n",
            "        <tf.Tensor: shape=(6,), dtype=float64,\n",
            "            numpy=array([1., 2., 3., 4., 5., 6.])>\n",
            "        \n",
            "        If `shape` is set, the `value` is reshaped to match. Scalars are expanded to\n",
            "        fill the `shape`:\n",
            "        \n",
            "        >>> tf.constant(0, shape=(2, 3))\n",
            "          <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "          array([[0, 0, 0],\n",
            "                 [0, 0, 0]], dtype=int32)>\n",
            "        >>> tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "          array([[1, 2, 3],\n",
            "                 [4, 5, 6]], dtype=int32)>\n",
            "        \n",
            "        `tf.constant` has no effect if an eager Tensor is passed as the `value`, it\n",
            "        even transmits gradients:\n",
            "        \n",
            "        >>> v = tf.Variable([0.0])\n",
            "        >>> with tf.GradientTape() as g:\n",
            "        ...     loss = tf.constant(v + v)\n",
            "        >>> g.gradient(loss, v).numpy()\n",
            "        array([2.], dtype=float32)\n",
            "        \n",
            "        But, since `tf.constant` embeds the value in the `tf.Graph` this fails for\n",
            "        symbolic tensors:\n",
            "        \n",
            "        >>> i = tf.keras.layers.Input(shape=[None, None])\n",
            "        >>> t = tf.constant(i)\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        NotImplementedError: ...\n",
            "        \n",
            "        `tf.constant` will _always_ create CPU (host) tensors. In order to create\n",
            "        tensors on other devices, use `tf.identity`. (If the `value` is an eager\n",
            "        Tensor, however, the tensor will be returned unmodified as mentioned above.)\n",
            "        \n",
            "        Related Ops:\n",
            "        \n",
            "        * `tf.convert_to_tensor` is similar but:\n",
            "          * It has no `shape` argument.\n",
            "          * Symbolic tensors are allowed to pass through.\n",
            "        \n",
            "            >>> i = tf.keras.layers.Input(shape=[None, None])\n",
            "            >>> t = tf.convert_to_tensor(i)\n",
            "        \n",
            "        * `tf.fill`: differs in a few ways:\n",
            "          *   `tf.constant` supports arbitrary constants, not just uniform scalar\n",
            "              Tensors like `tf.fill`.\n",
            "          *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n",
            "              can efficiently represent large tensors.\n",
            "          *   Since `tf.fill` does not embed the value, it can produce dynamically\n",
            "              sized outputs.\n",
            "        \n",
            "        Args:\n",
            "          value: A constant value (or list) of output type `dtype`.\n",
            "          dtype: The type of the elements of the resulting tensor.\n",
            "          shape: Optional dimensions of resulting tensor.\n",
            "          name: Optional name for the tensor.\n",
            "        \n",
            "        Returns:\n",
            "          A Constant Tensor.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if shape is incorrectly specified or unsupported.\n",
            "          ValueError: if called on a symbolic tensor.\n",
            "    \n",
            "    control_dependencies(control_inputs)\n",
            "        Wrapper for `Graph.control_dependencies()` using the default graph.\n",
            "        \n",
            "        See `tf.Graph.control_dependencies`\n",
            "        for more details.\n",
            "        \n",
            "        When eager execution is enabled, any callable object in the `control_inputs`\n",
            "        list will be called.\n",
            "        \n",
            "        Args:\n",
            "          control_inputs: A list of `Operation` or `Tensor` objects which must be\n",
            "            executed or computed before running the operations defined in the context.\n",
            "            Can also be `None` to clear the control dependencies. If eager execution\n",
            "            is enabled, any callable object in the `control_inputs` list will be\n",
            "            called.\n",
            "        \n",
            "        Returns:\n",
            "         A context manager that specifies control dependencies for all\n",
            "         operations constructed within the context.\n",
            "    \n",
            "    convert_to_tensor = convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None)\n",
            "        Converts the given `value` to a `Tensor`.\n",
            "        \n",
            "        This function converts Python objects of various types to `Tensor`\n",
            "        objects. It accepts `Tensor` objects, numpy arrays, Python lists,\n",
            "        and Python scalars. For example:\n",
            "        \n",
            "        >>> def my_func(arg):\n",
            "        ...   arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
            "        ...   return arg\n",
            "        \n",
            "        >>> # The following calls are equivalent.\n",
            "        >>> value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\n",
            "        >>> print(value_1)\n",
            "        tf.Tensor(\n",
            "          [[1. 2.]\n",
            "           [3. 4.]], shape=(2, 2), dtype=float32)\n",
            "        >>> value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\n",
            "        >>> print(value_2)\n",
            "        tf.Tensor(\n",
            "          [[1. 2.]\n",
            "           [3. 4.]], shape=(2, 2), dtype=float32)\n",
            "        >>> value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n",
            "        >>> print(value_3)\n",
            "        tf.Tensor(\n",
            "          [[1. 2.]\n",
            "           [3. 4.]], shape=(2, 2), dtype=float32)\n",
            "        \n",
            "        This function can be useful when composing a new operation in Python\n",
            "        (such as `my_func` in the example above). All standard Python op\n",
            "        constructors apply this function to each of their Tensor-valued\n",
            "        inputs, which allows those ops to accept numpy arrays, Python lists,\n",
            "        and scalars in addition to `Tensor` objects.\n",
            "        \n",
            "        Note: This function diverges from default Numpy behavior for `float` and\n",
            "          `string` types when `None` is present in a Python list or scalar. Rather\n",
            "          than silently converting `None` values, an error will be thrown.\n",
            "        \n",
            "        Args:\n",
            "          value: An object whose type has a registered `Tensor` conversion function.\n",
            "          dtype: Optional element type for the returned tensor. If missing, the type\n",
            "            is inferred from the type of `value`.\n",
            "          dtype_hint: Optional element type for the returned tensor, used when dtype\n",
            "            is None. In some cases, a caller may not have a dtype in mind when\n",
            "            converting to a tensor, so dtype_hint can be used as a soft preference.\n",
            "            If the conversion to `dtype_hint` is not possible, this argument has no\n",
            "            effect.\n",
            "          name: Optional name to use if a new `Tensor` is created.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` based on `value`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If no conversion function is registered for `value` to `dtype`.\n",
            "          RuntimeError: If a registered conversion function returns an invalid value.\n",
            "          ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\n",
            "    \n",
            "    cos(x, name=None)\n",
            "        Computes cos of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes cosine of every\n",
            "          element in the tensor. Input range is `(-inf, inf)` and\n",
            "          output range is `[-1,1]`. If input lies outside the boundary, `nan`\n",
            "          is returned.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n",
            "          tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    cosh(x, name=None)\n",
            "        Computes hyperbolic cosine of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes hyperbolic cosine of every\n",
            "          element in the tensor. Input range is `[-inf, inf]` and output range\n",
            "          is `[1, inf]`.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n",
            "          tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    cumsum(x, axis=0, exclusive=False, reverse=False, name=None)\n",
            "        Compute the cumulative sum of the tensor `x` along `axis`.\n",
            "        \n",
            "        By default, this op performs an inclusive cumsum, which means that the first\n",
            "        element of the input is identical to the first element of the output:\n",
            "        For example:\n",
            "        \n",
            "        >>> # tf.cumsum([a, b, c])   # [a, a + b, a + b + c]\n",
            "        >>> x = tf.constant([2, 4, 6, 8])\n",
            "        >>> tf.cumsum(x)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32,\n",
            "        numpy=array([ 2,  6, 12, 20], dtype=int32)>\n",
            "        \n",
            "        >>> # using varying `axis` values\n",
            "        >>> y = tf.constant([[2, 4, 6, 8], [1,3,5,7]])\n",
            "        >>> tf.cumsum(y, axis=0)\n",
            "        <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
            "        array([[ 2,  4,  6,  8],\n",
            "               [ 3,  7, 11, 15]], dtype=int32)>\n",
            "        >>> tf.cumsum(y, axis=1)\n",
            "        <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
            "        array([[ 2,  6, 12, 20],\n",
            "               [ 1,  4,  9, 16]], dtype=int32)>\n",
            "        \n",
            "        By setting the `exclusive` kwarg to `True`, an exclusive cumsum is performed\n",
            "        instead:\n",
            "        \n",
            "        >>> # tf.cumsum([a, b, c], exclusive=True)  => [0, a, a + b]\n",
            "        >>> x = tf.constant([2, 4, 6, 8])\n",
            "        >>> tf.cumsum(x, exclusive=True)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32,\n",
            "        numpy=array([ 0,  2,  6, 12], dtype=int32)>\n",
            "        \n",
            "        By setting the `reverse` kwarg to `True`, the cumsum is performed in the\n",
            "        opposite direction:\n",
            "        \n",
            "        >>> # tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c]\n",
            "        >>> x = tf.constant([2, 4, 6, 8])\n",
            "        >>> tf.cumsum(x, reverse=True)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32,\n",
            "        numpy=array([20, 18, 14,  8], dtype=int32)>\n",
            "        \n",
            "        This is more efficient than using separate `tf.reverse` ops.\n",
            "        The `reverse` and `exclusive` kwargs can also be combined:\n",
            "        \n",
            "        >>> # tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0]\n",
            "        >>> x = tf.constant([2, 4, 6, 8])\n",
            "        >>> tf.cumsum(x, exclusive=True, reverse=True)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32,\n",
            "        numpy=array([18, 14,  8,  0], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `float32`, `float64`,\n",
            "            `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,\n",
            "            `complex128`, `qint8`, `quint8`, `qint32`, `half`.\n",
            "          axis: A `Tensor` of type `int32` (default: 0). Must be in the range\n",
            "            `[-rank(x), rank(x))`.\n",
            "          exclusive: If `True`, perform exclusive cumsum.\n",
            "          reverse: A `bool` (default: False).\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    custom_gradient(f=None)\n",
            "        Decorator to define a function with a custom gradient.\n",
            "        \n",
            "        This decorator allows fine grained control over the gradients of a sequence\n",
            "        for operations.  This may be useful for multiple reasons, including providing\n",
            "        a more efficient or numerically stable gradient for a sequence of operations.\n",
            "        \n",
            "        For example, consider the following function that commonly occurs in the\n",
            "        computation of cross entropy and log likelihoods:\n",
            "        \n",
            "        ```python\n",
            "        def log1pexp(x):\n",
            "          return tf.math.log(1 + tf.exp(x))\n",
            "        ```\n",
            "        \n",
            "        Due to numerical instability, the gradient of this function evaluated at x=100\n",
            "        is NaN.  For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant(100.)\n",
            "        y = log1pexp(x)\n",
            "        dy = tf.gradients(y, x) # Will be NaN when evaluated.\n",
            "        ```\n",
            "        \n",
            "        The gradient expression can be analytically simplified to provide numerical\n",
            "        stability:\n",
            "        \n",
            "        ```python\n",
            "        @tf.custom_gradient\n",
            "        def log1pexp(x):\n",
            "          e = tf.exp(x)\n",
            "          def grad(dy):\n",
            "            return dy * (1 - 1 / (1 + e))\n",
            "          return tf.math.log(1 + e), grad\n",
            "        ```\n",
            "        \n",
            "        With this definition, the gradient at x=100 will be correctly evaluated as\n",
            "        1.0.\n",
            "        \n",
            "        Nesting custom gradients can lead to unintuitive results. The default\n",
            "        behavior does not correspond to n-th order derivatives. For example\n",
            "        \n",
            "        ```python\n",
            "        @tf.custom_gradient\n",
            "        def op(x):\n",
            "          y = op1(x)\n",
            "          @tf.custom_gradient\n",
            "          def grad_fn(dy):\n",
            "            gdy = op2(x, y, dy)\n",
            "            def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.\n",
            "              return op3(x, y, dy, ddy)\n",
            "            return gdy, grad_grad_fn\n",
            "          return y, grad_fn\n",
            "        ```\n",
            "        \n",
            "        The function `grad_grad_fn` will be calculating the first order gradient\n",
            "        of `grad_fn` with respect to `dy`, which is used to generate forward-mode\n",
            "        gradient graphs from backward-mode gradient graphs, but is not the same as\n",
            "        the second order gradient of `op` with respect to `x`.\n",
            "        \n",
            "        Instead, wrap nested `@tf.custom_gradients` in another function:\n",
            "        \n",
            "        ```python\n",
            "        @tf.custom_gradient\n",
            "        def op_with_fused_backprop(x):\n",
            "          y, x_grad = fused_op(x)\n",
            "          def first_order_gradient(dy):\n",
            "            @tf.custom_gradient\n",
            "            def first_order_custom(unused_x):\n",
            "              def second_order_and_transpose(ddy):\n",
            "                return second_order_for_x(...), gradient_wrt_dy(...)\n",
            "              return x_grad, second_order_and_transpose\n",
            "            return dy * first_order_custom(x)\n",
            "          return y, first_order_gradient\n",
            "        ```\n",
            "        \n",
            "        Additional arguments to the inner `@tf.custom_gradient`-decorated function\n",
            "        control the expected return values of the innermost function.\n",
            "        \n",
            "        See also `tf.RegisterGradient` which registers a gradient function for a\n",
            "        primitive TensorFlow operation. `tf.custom_gradient` on the other hand allows\n",
            "        for fine grained control over the gradient computation of a sequence of\n",
            "        operations.\n",
            "        \n",
            "        Note that if the decorated function uses `Variable`s, the enclosing variable\n",
            "        scope must be using `ResourceVariable`s.\n",
            "        \n",
            "        Args:\n",
            "          f: function `f(*x)` that returns a tuple `(y, grad_fn)` where:\n",
            "             - `x` is a sequence of `Tensor` inputs to the function.\n",
            "             - `y` is a `Tensor` or sequence of `Tensor` outputs of applying\n",
            "               TensorFlow operations in `f` to `x`.\n",
            "             - `grad_fn` is a function with the signature `g(*grad_ys)` which returns\n",
            "               a list of `Tensor`s - the derivatives of `Tensor`s in `y` with respect\n",
            "               to the `Tensor`s in `x`.  `grad_ys` is a `Tensor` or sequence of\n",
            "               `Tensor`s the same size as `y` holding the initial value gradients for\n",
            "               each `Tensor` in `y`. In a pure mathematical sense, a vector-argument\n",
            "               vector-valued function `f`'s derivatives should be its Jacobian matrix\n",
            "               `J`. Here we are expressing the Jacobian `J` as a function `grad_fn`\n",
            "               which defines how `J` will transform a vector `grad_ys` when\n",
            "               left-multiplied with it (`grad_ys * J`). This functional representation\n",
            "               of a matrix is convenient to use for chain-rule calculation\n",
            "               (in e.g. the back-propagation algorithm).\n",
            "        \n",
            "               If `f` uses `Variable`s (that are not part of the\n",
            "               inputs), i.e. through `get_variable`, then `grad_fn` should have\n",
            "               signature `g(*grad_ys, variables=None)`, where `variables` is a list of\n",
            "               the `Variable`s, and return a 2-tuple `(grad_xs, grad_vars)`, where\n",
            "               `grad_xs` is the same as above, and `grad_vars` is a `list<Tensor>`\n",
            "               with the derivatives of `Tensor`s in `y` with respect to the variables\n",
            "               (that is, grad_vars has one Tensor per variable in variables).\n",
            "        \n",
            "        Returns:\n",
            "          A function `h(x)` which returns the same value as `f(x)[0]` and whose\n",
            "          gradient (as calculated by `tf.gradients`) is determined by `f(x)[1]`.\n",
            "    \n",
            "    device = device_v2(device_name)\n",
            "        Specifies the device for ops created/executed in this context.\n",
            "        \n",
            "        This function specifies the device to be used for ops created/executed in a\n",
            "        particular context. Nested contexts will inherit and also create/execute\n",
            "        their ops on the specified device. If a specific device is not required,\n",
            "        consider not using this function so that a device can be automatically\n",
            "        assigned.  In general the use of this function is optional. `device_name` can\n",
            "        be fully specified, as in \"/job:worker/task:1/device:cpu:0\", or partially\n",
            "        specified, containing only a subset of the \"/\"-separated fields. Any fields\n",
            "        which are specified will override device annotations from outer scopes.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        with tf.device('/job:foo'):\n",
            "          # ops created here have devices with /job:foo\n",
            "          with tf.device('/job:bar/task:0/device:gpu:2'):\n",
            "            # ops created here have the fully specified device above\n",
            "          with tf.device('/device:gpu:1'):\n",
            "            # ops created here have the device '/job:foo/device:gpu:1'\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          device_name: The device name to use in the context.\n",
            "        \n",
            "        Returns:\n",
            "          A context manager that specifies the default device to use for newly\n",
            "          created ops.\n",
            "        \n",
            "        Raises:\n",
            "          RuntimeError: If a function is passed in.\n",
            "    \n",
            "    divide(x, y, name=None)\n",
            "        Computes Python style division of `x` by `y`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([16, 12, 11])\n",
            "        >>> y = tf.constant([4, 6, 2])\n",
            "        >>> tf.divide(x,y)\n",
            "        <tf.Tensor: shape=(3,), dtype=float64,\n",
            "        numpy=array([4. , 2. , 5.5])>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`\n",
            "          y: A `Tensor`\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with same shape as input\n",
            "    \n",
            "    dynamic_partition(data, partitions, num_partitions, name=None)\n",
            "        Partitions `data` into `num_partitions` tensors using indices from `partitions`.\n",
            "        \n",
            "        For each index tuple `js` of size `partitions.ndim`, the slice `data[js, ...]`\n",
            "        becomes part of `outputs[partitions[js]]`.  The slices with `partitions[js] = i`\n",
            "        are placed in `outputs[i]` in lexicographic order of `js`, and the first\n",
            "        dimension of `outputs[i]` is the number of entries in `partitions` equal to `i`.\n",
            "        In detail,\n",
            "        \n",
            "        ```python\n",
            "            outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]\n",
            "        \n",
            "            outputs[i] = pack([data[js, ...] for js if partitions[js] == i])\n",
            "        ```\n",
            "        \n",
            "        `data.shape` must start with `partitions.shape`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "            # Scalar partitions.\n",
            "            partitions = 1\n",
            "            num_partitions = 2\n",
            "            data = [10, 20]\n",
            "            outputs[0] = []  # Empty with shape [0, 2]\n",
            "            outputs[1] = [[10, 20]]\n",
            "        \n",
            "            # Vector partitions.\n",
            "            partitions = [0, 0, 1, 1, 0]\n",
            "            num_partitions = 2\n",
            "            data = [10, 20, 30, 40, 50]\n",
            "            outputs[0] = [10, 20, 50]\n",
            "            outputs[1] = [30, 40]\n",
            "        ```\n",
            "        \n",
            "        See `dynamic_stitch` for an example on how to merge partitions back.\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicPartition.png\" alt>\n",
            "        </div>\n",
            "        \n",
            "        Args:\n",
            "          data: A `Tensor`.\n",
            "          partitions: A `Tensor` of type `int32`.\n",
            "            Any shape.  Indices in the range `[0, num_partitions)`.\n",
            "          num_partitions: An `int` that is `>= 1`.\n",
            "            The number of partitions to output.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A list of `num_partitions` `Tensor` objects with the same type as `data`.\n",
            "    \n",
            "    dynamic_stitch(indices, data, name=None)\n",
            "        Interleave the values from the `data` tensors into a single tensor.\n",
            "        \n",
            "        Builds a merged tensor such that\n",
            "        \n",
            "        ```python\n",
            "            merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n",
            "        ```\n",
            "        \n",
            "        For example, if each `indices[m]` is scalar or vector, we have\n",
            "        \n",
            "        ```python\n",
            "            # Scalar indices:\n",
            "            merged[indices[m], ...] = data[m][...]\n",
            "        \n",
            "            # Vector indices:\n",
            "            merged[indices[m][i], ...] = data[m][i, ...]\n",
            "        ```\n",
            "        \n",
            "        Each `data[i].shape` must start with the corresponding `indices[i].shape`,\n",
            "        and the rest of `data[i].shape` must be constant w.r.t. `i`.  That is, we\n",
            "        must have `data[i].shape = indices[i].shape + constant`.  In terms of this\n",
            "        `constant`, the output shape is\n",
            "        \n",
            "            merged.shape = [max(indices)] + constant\n",
            "        \n",
            "        Values are merged in order, so if an index appears in both `indices[m][i]` and\n",
            "        `indices[n][j]` for `(m,i) < (n,j)` the slice `data[n][j]` will appear in the\n",
            "        merged result. If you do not need this guarantee, ParallelDynamicStitch might\n",
            "        perform better on some devices.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "            indices[0] = 6\n",
            "            indices[1] = [4, 1]\n",
            "            indices[2] = [[5, 2], [0, 3]]\n",
            "            data[0] = [61, 62]\n",
            "            data[1] = [[41, 42], [11, 12]]\n",
            "            data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\n",
            "            merged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n",
            "                      [51, 52], [61, 62]]\n",
            "        ```\n",
            "        \n",
            "        This method can be used to merge partitions created by `dynamic_partition`\n",
            "        as illustrated on the following example:\n",
            "        \n",
            "        ```python\n",
            "            # Apply function (increments x_i) on elements for which a certain condition\n",
            "            # apply (x_i != -1 in this example).\n",
            "            x=tf.constant([0.1, -1., 5.2, 4.3, -1., 7.4])\n",
            "            condition_mask=tf.not_equal(x,tf.constant(-1.))\n",
            "            partitioned_data = tf.dynamic_partition(\n",
            "                x, tf.cast(condition_mask, tf.int32) , 2)\n",
            "            partitioned_data[1] = partitioned_data[1] + 1.0\n",
            "            condition_indices = tf.dynamic_partition(\n",
            "                tf.range(tf.shape(x)[0]), tf.cast(condition_mask, tf.int32) , 2)\n",
            "            x = tf.dynamic_stitch(condition_indices, partitioned_data)\n",
            "            # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain\n",
            "            # unchanged.\n",
            "        ```\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/DynamicStitch.png\" alt>\n",
            "        </div>\n",
            "        \n",
            "        Args:\n",
            "          indices: A list of at least 1 `Tensor` objects with type `int32`.\n",
            "          data: A list with the same length as `indices` of `Tensor` objects with the same type.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `data`.\n",
            "    \n",
            "    edit_distance(hypothesis, truth, normalize=True, name='edit_distance')\n",
            "        Computes the Levenshtein distance between sequences.\n",
            "        \n",
            "        This operation takes variable-length sequences (`hypothesis` and `truth`),\n",
            "        each provided as a `SparseTensor`, and computes the Levenshtein distance.\n",
            "        You can normalize the edit distance by length of `truth` by setting\n",
            "        `normalize` to true.\n",
            "        \n",
            "        For example, given the following input:\n",
            "        \n",
            "        ```python\n",
            "        # 'hypothesis' is a tensor of shape `[2, 1]` with variable-length values:\n",
            "        #   (0,0) = [\"a\"]\n",
            "        #   (1,0) = [\"b\"]\n",
            "        hypothesis = tf.SparseTensor(\n",
            "            [[0, 0, 0],\n",
            "             [1, 0, 0]],\n",
            "            [\"a\", \"b\"],\n",
            "            (2, 1, 1))\n",
            "        \n",
            "        # 'truth' is a tensor of shape `[2, 2]` with variable-length values:\n",
            "        #   (0,0) = []\n",
            "        #   (0,1) = [\"a\"]\n",
            "        #   (1,0) = [\"b\", \"c\"]\n",
            "        #   (1,1) = [\"a\"]\n",
            "        truth = tf.SparseTensor(\n",
            "            [[0, 1, 0],\n",
            "             [1, 0, 0],\n",
            "             [1, 0, 1],\n",
            "             [1, 1, 0]],\n",
            "            [\"a\", \"b\", \"c\", \"a\"],\n",
            "            (2, 2, 2))\n",
            "        \n",
            "        normalize = True\n",
            "        ```\n",
            "        \n",
            "        This operation would return the following:\n",
            "        \n",
            "        ```python\n",
            "        # 'output' is a tensor of shape `[2, 2]` with edit distances normalized\n",
            "        # by 'truth' lengths.\n",
            "        output ==> [[inf, 1.0],  # (0,0): no truth, (0,1): no hypothesis\n",
            "                   [0.5, 1.0]]  # (1,0): addition, (1,1): no hypothesis\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          hypothesis: A `SparseTensor` containing hypothesis sequences.\n",
            "          truth: A `SparseTensor` containing truth sequences.\n",
            "          normalize: A `bool`. If `True`, normalizes the Levenshtein distance by\n",
            "            length of `truth.`\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A dense `Tensor` with rank `R - 1`, where R is the rank of the\n",
            "          `SparseTensor` inputs `hypothesis` and `truth`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If either `hypothesis` or `truth` are not a `SparseTensor`.\n",
            "    \n",
            "    eig(tensor, name=None)\n",
            "        Computes the eigen decomposition of a batch of matrices.\n",
            "        \n",
            "        The eigenvalues\n",
            "        and eigenvectors for a non-Hermitian matrix in general are complex. The\n",
            "        eigenvectors are not guaranteed to be linearly independent.\n",
            "        \n",
            "        Computes the eigenvalues and right eigenvectors of the innermost\n",
            "        N-by-N matrices in `tensor` such that\n",
            "        `tensor[...,:,:] * v[..., :,i] = e[..., i] * v[...,:,i]`, for i=0...N-1.\n",
            "        \n",
            "        Args:\n",
            "          tensor: `Tensor` of shape `[..., N, N]`. Only the lower triangular part of\n",
            "            each inner inner matrix is referenced.\n",
            "          name: string, optional name of the operation.\n",
            "        \n",
            "        Returns:\n",
            "          e: Eigenvalues. Shape is `[..., N]`. Sorted in non-decreasing order.\n",
            "          v: Eigenvectors. Shape is `[..., N, N]`. The columns of the inner most\n",
            "            matrices contain eigenvectors of the corresponding matrices in `tensor`\n",
            "    \n",
            "    eigvals(tensor, name=None)\n",
            "        Computes the eigenvalues of one or more matrices.\n",
            "        \n",
            "        Note: If your program backpropagates through this function, you should replace\n",
            "        it with a call to tf.linalg.eig (possibly ignoring the second output) to\n",
            "        avoid computing the eigen decomposition twice. This is because the\n",
            "        eigenvectors are used to compute the gradient w.r.t. the eigenvalues. See\n",
            "        _SelfAdjointEigV2Grad in linalg_grad.py.\n",
            "        \n",
            "        Args:\n",
            "          tensor: `Tensor` of shape `[..., N, N]`.\n",
            "          name: string, optional name of the operation.\n",
            "        \n",
            "        Returns:\n",
            "          e: Eigenvalues. Shape is `[..., N]`. The vector `e[..., :]` contains the `N`\n",
            "            eigenvalues of `tensor[..., :, :]`.\n",
            "    \n",
            "    einsum(equation, *inputs, **kwargs)\n",
            "        Tensor contraction over specified indices and outer product.\n",
            "        \n",
            "        Einsum allows defining Tensors by defining their element-wise computation.\n",
            "        This computation is defined by `equation`, a shorthand form based on Einstein\n",
            "        summation. As an example, consider multiplying two matrices A and B to form a\n",
            "        matrix C.  The elements of C are given by:\n",
            "        \n",
            "        ```\n",
            "          C[i,k] = sum_j A[i,j] * B[j,k]\n",
            "        ```\n",
            "        \n",
            "        The corresponding `equation` is:\n",
            "        \n",
            "        ```\n",
            "          ij,jk->ik\n",
            "        ```\n",
            "        \n",
            "        In general, to convert the element-wise equation into the `equation` string,\n",
            "        use the following procedure (intermediate strings for matrix multiplication\n",
            "        example provided in parentheses):\n",
            "        \n",
            "        1. remove variable names, brackets, and commas, (`ik = sum_j ij * jk`)\n",
            "        2. replace \"*\" with \",\", (`ik = sum_j ij , jk`)\n",
            "        3. drop summation signs, and (`ik = ij, jk`)\n",
            "        4. move the output to the right, while replacing \"=\" with \"->\". (`ij,jk->ik`)\n",
            "        \n",
            "        Many common operations can be expressed in this way.  For example:\n",
            "        \n",
            "        ```python\n",
            "        # Matrix multiplication\n",
            "        einsum('ij,jk->ik', m0, m1)  # output[i,k] = sum_j m0[i,j] * m1[j, k]\n",
            "        \n",
            "        # Dot product\n",
            "        einsum('i,i->', u, v)  # output = sum_i u[i]*v[i]\n",
            "        \n",
            "        # Outer product\n",
            "        einsum('i,j->ij', u, v)  # output[i,j] = u[i]*v[j]\n",
            "        \n",
            "        # Transpose\n",
            "        einsum('ij->ji', m)  # output[j,i] = m[i,j]\n",
            "        \n",
            "        # Trace\n",
            "        einsum('ii', m)  # output[j,i] = trace(m) = sum_i m[i, i]\n",
            "        \n",
            "        # Batch matrix multiplication\n",
            "        einsum('aij,ajk->aik', s, t)  # out[a,i,k] = sum_j s[a,i,j] * t[a, j, k]\n",
            "        ```\n",
            "        \n",
            "        To enable and control broadcasting, use an ellipsis.  For example, to perform\n",
            "        batch matrix multiplication with NumPy-style broadcasting across the batch\n",
            "        dimensions, use:\n",
            "        \n",
            "        ```python\n",
            "        einsum('...ij,...jk->...ik', u, v)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          equation: a `str` describing the contraction, in the same format as\n",
            "            `numpy.einsum`.\n",
            "          *inputs: the inputs to contract (each one a `Tensor`), whose shapes should\n",
            "            be consistent with `equation`.\n",
            "          **kwargs:\n",
            "            - optimize: Optimization strategy to use to find contraction path using\n",
            "              opt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or\n",
            "                'auto'. (optional, default: 'greedy').\n",
            "            - name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The contracted `Tensor`, with shape determined by `equation`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If\n",
            "            - the format of `equation` is incorrect,\n",
            "            - number of inputs or their shapes are inconsistent with `equation`.\n",
            "    \n",
            "    ensure_shape(x, shape, name=None)\n",
            "        Updates the shape of a tensor and checks at runtime that the shape holds.\n",
            "        \n",
            "        For example:\n",
            "        ```python\n",
            "        x = tf.compat.v1.placeholder(tf.int32)\n",
            "        print(x.shape)\n",
            "        ==> TensorShape(None)\n",
            "        y = x * 2\n",
            "        print(y.shape)\n",
            "        ==> TensorShape(None)\n",
            "        \n",
            "        y = tf.ensure_shape(y, (None, 3, 3))\n",
            "        print(y.shape)\n",
            "        ==> TensorShape([Dimension(None), Dimension(3), Dimension(3)])\n",
            "        \n",
            "        with tf.compat.v1.Session() as sess:\n",
            "          # Raises tf.errors.InvalidArgumentError, because the shape (3,) is not\n",
            "          # compatible with the shape (None, 3, 3)\n",
            "          sess.run(y, feed_dict={x: [1, 2, 3]})\n",
            "        \n",
            "        ```\n",
            "        \n",
            "        NOTE: This differs from `Tensor.set_shape` in that it sets the static shape\n",
            "        of the resulting tensor and enforces it at runtime, raising an error if the\n",
            "        tensor's runtime shape is incompatible with the specified shape.\n",
            "        `Tensor.set_shape` sets the static shape of the tensor without enforcing it\n",
            "        at runtime, which may result in inconsistencies between the statically-known\n",
            "        shape of tensors and the runtime value of tensors.\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`.\n",
            "          shape: A `TensorShape` representing the shape of this tensor, a\n",
            "            `TensorShapeProto`, a list, a tuple, or None.\n",
            "          name: A name for this operation (optional). Defaults to \"EnsureShape\".\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type and contents as `x`. At runtime, raises a\n",
            "          `tf.errors.InvalidArgumentError` if `shape` is incompatible with the shape\n",
            "          of `x`.\n",
            "    \n",
            "    equal(x, y, name=None)\n",
            "        Returns the truth value of (x == y) element-wise.\n",
            "        \n",
            "        Performs a [broadcast](\n",
            "        https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the\n",
            "        arguments and then an element-wise equality comparison, returning a Tensor of\n",
            "        boolean values.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([2, 4])\n",
            "        >>> y = tf.constant(2)\n",
            "        >>> tf.math.equal(x, y)\n",
            "        <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>\n",
            "        \n",
            "        >>> x = tf.constant([2, 4])\n",
            "        >>> y = tf.constant([2, 4])\n",
            "        >>> tf.math.equal(x, y)\n",
            "        <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>\n",
            "        \n",
            "        Args:\n",
            "          x: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.\n",
            "          y: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "        \n",
            "        Raises:\n",
            "          `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible\n",
            "    \n",
            "    executing_eagerly()\n",
            "        Checks whether the current thread has eager execution enabled.\n",
            "        \n",
            "        Eager execution is enabled by default and this API returns `True`\n",
            "        in most of cases. However, this API might return `False` in the following use\n",
            "        cases.\n",
            "        \n",
            "        *  Executing inside `tf.function`, unless under `tf.init_scope` or\n",
            "           `tf.config.experimental_run_functions_eagerly(True)` is previously called.\n",
            "        *  Executing inside a transformation function for `tf.dataset`.\n",
            "        *  `tf.compat.v1.disable_eager_execution()` is called.\n",
            "        \n",
            "        General case:\n",
            "        \n",
            "        >>> print(tf.executing_eagerly())\n",
            "        True\n",
            "        \n",
            "        Inside `tf.function`:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def fn():\n",
            "        ...   with tf.init_scope():\n",
            "        ...     print(tf.executing_eagerly())\n",
            "        ...   print(tf.executing_eagerly())\n",
            "        >>> fn()\n",
            "        True\n",
            "        False\n",
            "        \n",
            "        Inside `tf.function` after\n",
            "        \n",
            "        `tf.config.experimental_run_functions_eagerly(True)` is called:\n",
            "        >>> tf.config.experimental_run_functions_eagerly(True)\n",
            "        >>> @tf.function\n",
            "        ... def fn():\n",
            "        ...   with tf.init_scope():\n",
            "        ...     print(tf.executing_eagerly())\n",
            "        ...   print(tf.executing_eagerly())\n",
            "        >>> fn()\n",
            "        True\n",
            "        True\n",
            "        >>> tf.config.experimental_run_functions_eagerly(False)\n",
            "        \n",
            "        Inside a transformation function for `tf.dataset`:\n",
            "        \n",
            "        >>> def data_fn(x):\n",
            "        ...   print(tf.executing_eagerly())\n",
            "        ...   return x\n",
            "        >>> dataset = tf.data.Dataset.range(100)\n",
            "        >>> dataset = dataset.map(data_fn)\n",
            "        False\n",
            "        \n",
            "        Returns:\n",
            "          `True` if the current thread has eager execution enabled.\n",
            "    \n",
            "    exp(x, name=None)\n",
            "        Computes exponential of x element-wise.  \\\\(y = e^x\\\\).\n",
            "        \n",
            "        This function computes the exponential of the input tensor element-wise.\n",
            "        i.e. `math.exp(x)` or \\\\(e^x\\\\), where `x` is the input tensor.\n",
            "        \\\\(e\\\\) denotes Euler's number and is approximately equal to 2.718281.\n",
            "        Output is positive for any real input.\n",
            "        \n",
            "        >>> x = tf.constant(2.0)\n",
            "        >>> tf.math.exp(x)\n",
            "        <tf.Tensor: shape=(), dtype=float32, numpy=7.389056>\n",
            "        \n",
            "        >>> x = tf.constant([2.0, 8.0])\n",
            "        >>> tf.math.exp(x)\n",
            "        <tf.Tensor: shape=(2,), dtype=float32,\n",
            "        numpy=array([   7.389056, 2980.958   ], dtype=float32)>\n",
            "        \n",
            "        For complex numbers, the exponential value is calculated as\n",
            "        \\\\(e^{x+iy}={e^x}{e^{iy}}={e^x}{\\\\cos(y)+i\\\\sin(y)}\\\\)\n",
            "        \n",
            "        For `1+1j` the value would be computed as:\n",
            "        \\\\(e^1{\\\\cos(1)+i\\\\sin(1)} = 2.7182817 \\\\times (0.5403023+0.84147096j)\\\\)\n",
            "        \n",
            "        >>> x = tf.constant(1 + 1j)\n",
            "        >>> tf.math.exp(x)\n",
            "        <tf.Tensor: shape=(), dtype=complex128,\n",
            "        numpy=(1.4686939399158851+2.2873552871788423j)>\n",
            "        \n",
            "        Args:\n",
            "          x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,\n",
            "            `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor`. Has the same type as `x`.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.exp\n",
            "        @end_compatibility\n",
            "    \n",
            "    expand_dims = expand_dims_v2(input, axis, name=None)\n",
            "        Returns a tensor with an additional dimension inserted at index `axis`.\n",
            "        \n",
            "        Given a tensor `input`, this operation inserts a dimension of size 1 at the\n",
            "        dimension index `axis` of `input`'s shape. The dimension index `axis` starts\n",
            "        at zero; if you specify a negative number for `axis` it is counted backward\n",
            "        from the end.\n",
            "        \n",
            "        This operation is useful if you want to add a batch dimension to a single\n",
            "        element. For example, if you have a single image of shape `[height, width,\n",
            "        channels]`, you can make it a batch of one image with `expand_dims(image, 0)`,\n",
            "        which will make the shape `[1, height, width, channels]`.\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        >>> t = [[1, 2, 3],[4, 5, 6]] # shape [2, 3]\n",
            "        \n",
            "        >>> tf.expand_dims(t, 0)\n",
            "        <tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=\n",
            "        array([[[1, 2, 3],\n",
            "                [4, 5, 6]]], dtype=int32)>\n",
            "        \n",
            "        >>> tf.expand_dims(t, 1)\n",
            "        <tf.Tensor: shape=(2, 1, 3), dtype=int32, numpy=\n",
            "        array([[[1, 2, 3]],\n",
            "               [[4, 5, 6]]], dtype=int32)>\n",
            "        \n",
            "        >>> tf.expand_dims(t, 2)\n",
            "        <tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=\n",
            "        array([[[1],\n",
            "                [2],\n",
            "                [3]],\n",
            "               [[4],\n",
            "                [5],\n",
            "                [6]]], dtype=int32)>\n",
            "        \n",
            "        >>> tf.expand_dims(t, -1) # Last dimension index. In this case, same as 2.\n",
            "        <tf.Tensor: shape=(2, 3, 1), dtype=int32, numpy=\n",
            "        array([[[1],\n",
            "                [2],\n",
            "                [3]],\n",
            "               [[4],\n",
            "                [5],\n",
            "                [6]]], dtype=int32)>\n",
            "        \n",
            "        This operation is related to:\n",
            "        \n",
            "        *   `tf.squeeze`, which removes dimensions of size 1.\n",
            "        *   `tf.reshape`, which provides more flexible reshaping capability\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          axis: Integer specifying the dimension index at which to expand the\n",
            "            shape of `input`. Given an input of D dimensions, `axis` must be in range\n",
            "            `[-(D+1), D]` (inclusive).\n",
            "          name: Optional string. The name of the output `Tensor`.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor with the same data as `input`, with an additional dimension\n",
            "          inserted at the index specified by `axis`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `axis` is not specified.\n",
            "          InvalidArgumentError: If `axis` is out of range `[-(D+1), D]`.\n",
            "    \n",
            "    extract_volume_patches(input, ksizes, strides, padding, name=None)\n",
            "        Extract `patches` from `input` and put them in the \"depth\" output dimension. 3D extension of `extract_image_patches`.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "            5-D Tensor with shape `[batch, in_planes, in_rows, in_cols, depth]`.\n",
            "          ksizes: A list of `ints` that has length `>= 5`.\n",
            "            The size of the sliding window for each dimension of `input`.\n",
            "          strides: A list of `ints` that has length `>= 5`.\n",
            "            1-D of length 5. How far the centers of two consecutive patches are in\n",
            "            `input`. Must be: `[1, stride_planes, stride_rows, stride_cols, 1]`.\n",
            "          padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
            "            The type of padding algorithm to use.\n",
            "        \n",
            "            We specify the size-related attributes as:\n",
            "        \n",
            "            ```python\n",
            "                  ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]\n",
            "                  strides = [1, stride_planes, strides_rows, strides_cols, 1]\n",
            "            ```\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    eye(num_rows, num_columns=None, batch_shape=None, dtype=tf.float32, name=None)\n",
            "        Construct an identity matrix, or a batch of matrices.\n",
            "        \n",
            "        ```python\n",
            "        # Construct one identity matrix.\n",
            "        tf.eye(2)\n",
            "        ==> [[1., 0.],\n",
            "             [0., 1.]]\n",
            "        \n",
            "        # Construct a batch of 3 identity matrices, each 2 x 2.\n",
            "        # batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.\n",
            "        batch_identity = tf.eye(2, batch_shape=[3])\n",
            "        \n",
            "        # Construct one 2 x 3 \"identity\" matrix\n",
            "        tf.eye(2, num_columns=3)\n",
            "        ==> [[ 1.,  0.,  0.],\n",
            "             [ 0.,  1.,  0.]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          num_rows: Non-negative `int32` scalar `Tensor` giving the number of rows\n",
            "            in each batch matrix.\n",
            "          num_columns: Optional non-negative `int32` scalar `Tensor` giving the number\n",
            "            of columns in each batch matrix.  Defaults to `num_rows`.\n",
            "          batch_shape:  A list or tuple of Python integers or a 1-D `int32` `Tensor`.\n",
            "            If provided, the returned `Tensor` will have leading batch dimensions of\n",
            "            this shape.\n",
            "          dtype:  The type of an element in the resulting `Tensor`\n",
            "          name:  A name for this `Op`.  Defaults to \"eye\".\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of shape `batch_shape + [num_rows, num_columns]`\n",
            "    \n",
            "    fill(dims, value, name=None)\n",
            "        Creates a tensor filled with a scalar value.\n",
            "        \n",
            "        This operation creates a tensor of shape `dims` and fills it with `value`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> tf.fill([2, 3], 9)\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "        array([[9, 9, 9],\n",
            "               [9, 9, 9]], dtype=int32)>\n",
            "        \n",
            "        `tf.fill` evaluates at graph runtime and supports dynamic shapes based on\n",
            "        other runtime `tf.Tensors`, unlike `tf.constant(value, shape=dims)`, which\n",
            "        embeds the value as a `Const` node.\n",
            "        \n",
            "        Args:\n",
            "          dims: A 1-D sequence of non-negative numbers. Represents the shape of the\n",
            "            output `tf.Tensor`. Entries should be of type: `int32`, `int64`.\n",
            "          value: A value to fill the returned `tf.Tensor`.\n",
            "          name: Optional string. The name of the output `tf.Tensor`.\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` with shape `dims` and the same dtype as `value`.\n",
            "        \n",
            "        Raises:\n",
            "          InvalidArgumentError: `dims` contains negative entries.\n",
            "          NotFoundError: `dims` contains non-integer entries.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Similar to `np.full`. In `numpy`, more parameters are supported. Passing a\n",
            "        number argument as the shape (`np.full(5, value)`) is valid in `numpy` for\n",
            "        specifying a 1-D shaped result, while TensorFlow does not support this syntax.\n",
            "        @end_compatibility\n",
            "    \n",
            "    fingerprint(data, method='farmhash64', name=None)\n",
            "        Generates fingerprint values.\n",
            "        \n",
            "        Generates fingerprint values of `data`.\n",
            "        \n",
            "        Fingerprint op considers the first dimension of `data` as the batch dimension,\n",
            "        and `output[i]` contains the fingerprint value generated from contents in\n",
            "        `data[i, ...]` for all `i`.\n",
            "        \n",
            "        Fingerprint op writes fingerprint values as byte arrays. For example, the\n",
            "        default method `farmhash64` generates a 64-bit fingerprint value at a time.\n",
            "        This 8-byte value is written out as an `tf.uint8` array of size 8, in\n",
            "        little-endian order.\n",
            "        \n",
            "        For example, suppose that `data` has data type `tf.int32` and shape (2, 3, 4),\n",
            "        and that the fingerprint method is `farmhash64`. In this case, the output\n",
            "        shape is (2, 8), where 2 is the batch dimension size of `data`, and 8 is the\n",
            "        size of each fingerprint value in bytes. `output[0, :]` is generated from\n",
            "        12 integers in `data[0, :, :]` and similarly `output[1, :]` is generated from\n",
            "        other 12 integers in `data[1, :, :]`.\n",
            "        \n",
            "        Note that this op fingerprints the raw underlying buffer, and it does not\n",
            "        fingerprint Tensor's metadata such as data type and/or shape. For example, the\n",
            "        fingerprint values are invariant under reshapes and bitcasts as long as the\n",
            "        batch dimension remain the same:\n",
            "        \n",
            "        ```python\n",
            "        tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))\n",
            "        tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))\n",
            "        ```\n",
            "        \n",
            "        For string data, one should expect `tf.fingerprint(data) !=\n",
            "        tf.fingerprint(tf.string.reduce_join(data))` in general.\n",
            "        \n",
            "        Args:\n",
            "          data: A `Tensor`. Must have rank 1 or higher.\n",
            "          method: A `Tensor` of type `tf.string`. Fingerprint method used by this op.\n",
            "            Currently available method is `farmhash64`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A two-dimensional `Tensor` of type `tf.uint8`. The first dimension equals to\n",
            "          `data`'s first dimension, and the second dimension size depends on the\n",
            "          fingerprint algorithm.\n",
            "    \n",
            "    floor(x, name=None)\n",
            "        Returns element-wise largest integer not greater than x.\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    foldl = foldl_v2(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)\n",
            "        foldl on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n",
            "        \n",
            "        Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(back_prop=False)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "        Instead of:\n",
            "        results = tf.foldl(fn, elems, back_prop=False)\n",
            "        Use:\n",
            "        results = tf.nest.map_structure(tf.stop_gradient, tf.foldl(fn, elems))\n",
            "        \n",
            "        This foldl operator repeatedly applies the callable `fn` to a sequence\n",
            "        of elements from first to last. The elements are made of the tensors\n",
            "        unpacked from `elems` on dimension 0. The callable fn takes two tensors as\n",
            "        arguments. The first argument is the accumulated value computed from the\n",
            "        preceding invocation of fn, and the second is the value at the current\n",
            "        position of `elems`. If `initializer` is None, `elems` must contain at least\n",
            "        one element, and its first element is used as the initializer.\n",
            "        \n",
            "        Suppose that `elems` is unpacked into `values`, a list of tensors. The shape\n",
            "        of the result tensor is fn(initializer, values[0]).shape`.\n",
            "        \n",
            "        This method also allows multi-arity `elems` and output of `fn`.  If `elems`\n",
            "        is a (possibly nested) list or tuple of tensors, then each of these tensors\n",
            "        must have a matching first (unpack) dimension.  The signature of `fn` may\n",
            "        match the structure of `elems`.  That is, if `elems` is\n",
            "        `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:\n",
            "        `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n",
            "        \n",
            "        Args:\n",
            "          fn: The callable to be performed.\n",
            "          elems: A tensor or (possibly nested) sequence of tensors, each of which will\n",
            "            be unpacked along their first dimension.  The nested sequence of the\n",
            "            resulting slices will be the first argument to `fn`.\n",
            "          initializer: (optional) A tensor or (possibly nested) sequence of tensors,\n",
            "            as the initial value for the accumulator.\n",
            "          parallel_iterations: (optional) The number of iterations allowed to run in\n",
            "            parallel.\n",
            "          back_prop: (optional) Deprecated. False disables support for back\n",
            "            propagation. Prefer using `tf.stop_gradient` instead.\n",
            "          swap_memory: (optional) True enables GPU-CPU memory swapping.\n",
            "          name: (optional) Name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor or (possibly nested) sequence of tensors, resulting from applying\n",
            "          `fn` consecutively to the list of tensors unpacked from `elems`, from first\n",
            "          to last.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `fn` is not callable.\n",
            "        \n",
            "        Example:\n",
            "          ```python\n",
            "          elems = tf.constant([1, 2, 3, 4, 5, 6])\n",
            "          sum = foldl(lambda a, x: a + x, elems)\n",
            "          # sum == 21\n",
            "          ```\n",
            "    \n",
            "    foldr = foldr_v2(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, name=None)\n",
            "        foldr on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n",
            "        \n",
            "        Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(back_prop=False)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "        Instead of:\n",
            "        results = tf.foldr(fn, elems, back_prop=False)\n",
            "        Use:\n",
            "        results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "        \n",
            "        This foldr operator repeatedly applies the callable `fn` to a sequence\n",
            "        of elements from last to first. The elements are made of the tensors\n",
            "        unpacked from `elems`. The callable fn takes two tensors as arguments.\n",
            "        The first argument is the accumulated value computed from the preceding\n",
            "        invocation of fn, and the second is the value at the current position of\n",
            "        `elems`. If `initializer` is None, `elems` must contain at least one element,\n",
            "        and its first element is used as the initializer.\n",
            "        \n",
            "        Suppose that `elems` is unpacked into `values`, a list of tensors. The shape\n",
            "        of the result tensor is `fn(initializer, values[0]).shape`.\n",
            "        \n",
            "        This method also allows multi-arity `elems` and output of `fn`.  If `elems`\n",
            "        is a (possibly nested) list or tuple of tensors, then each of these tensors\n",
            "        must have a matching first (unpack) dimension.  The signature of `fn` may\n",
            "        match the structure of `elems`.  That is, if `elems` is\n",
            "        `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:\n",
            "        `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n",
            "        \n",
            "        Args:\n",
            "          fn: The callable to be performed.\n",
            "          elems: A tensor or (possibly nested) sequence of tensors, each of which will\n",
            "            be unpacked along their first dimension.  The nested sequence of the\n",
            "            resulting slices will be the first argument to `fn`.\n",
            "          initializer: (optional) A tensor or (possibly nested) sequence of tensors,\n",
            "            as the initial value for the accumulator.\n",
            "          parallel_iterations: (optional) The number of iterations allowed to run in\n",
            "            parallel.\n",
            "          back_prop: (optional) Deprecated. False disables support for back\n",
            "            propagation. Prefer using `tf.stop_gradient` instead.\n",
            "          swap_memory: (optional) True enables GPU-CPU memory swapping.\n",
            "          name: (optional) Name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor or (possibly nested) sequence of tensors, resulting from applying\n",
            "          `fn` consecutively to the list of tensors unpacked from `elems`, from last\n",
            "          to first.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `fn` is not callable.\n",
            "        \n",
            "        Example:\n",
            "          ```python\n",
            "          elems = [1, 2, 3, 4, 5, 6]\n",
            "          sum = foldr(lambda a, x: a + x, elems)\n",
            "          # sum == 21\n",
            "          ```\n",
            "    \n",
            "    function(func=None, input_signature=None, autograph=True, experimental_implements=None, experimental_autograph_options=None, experimental_relax_shapes=False, experimental_compile=None)\n",
            "        Compiles a function into a callable TensorFlow graph.\n",
            "        \n",
            "        `tf.function` constructs a callable that executes a TensorFlow graph\n",
            "        (`tf.Graph`) created by trace-compiling the TensorFlow operations in `func`,\n",
            "        effectively executing `func` as a TensorFlow graph.\n",
            "        \n",
            "        Example usage:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x, y):\n",
            "        ...   return x ** 2 + y\n",
            "        >>> x = tf.constant([2, 3])\n",
            "        >>> y = tf.constant([3, -2])\n",
            "        >>> f(x, y)\n",
            "        <tf.Tensor: ... numpy=array([7, 7], ...)>\n",
            "        \n",
            "        _Features_\n",
            "        \n",
            "        `func` may use data-dependent control flow, including `if`, `for`, `while`\n",
            "        `break`, `continue` and `return` statements:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   if tf.reduce_sum(x) > 0:\n",
            "        ...     return x * x\n",
            "        ...   else:\n",
            "        ...     return -x // 2\n",
            "        >>> f(tf.constant(-2))\n",
            "        <tf.Tensor: ... numpy=1>\n",
            "        \n",
            "        `func`'s closure may include `tf.Tensor` and `tf.Variable` objects:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f():\n",
            "        ...   return x ** 2 + y\n",
            "        >>> x = tf.constant([-2, -3])\n",
            "        >>> y = tf.Variable([3, -2])\n",
            "        >>> f()\n",
            "        <tf.Tensor: ... numpy=array([7, 7], ...)>\n",
            "        \n",
            "        `func` may also use ops with side effects, such as `tf.print`, `tf.Variable`\n",
            "        and others:\n",
            "        \n",
            "        >>> v = tf.Variable(1)\n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   for i in tf.range(x):\n",
            "        ...     v.assign_add(i)\n",
            "        >>> f(3)\n",
            "        >>> v\n",
            "        <tf.Variable ... numpy=4>\n",
            "        \n",
            "        Important: Any Python side-effects (appending to a list, printing with\n",
            "        `print`, etc) will only happen once, when `func` is traced. To have\n",
            "        side-effects executed into your `tf.function` they need to be written\n",
            "        as TF ops:\n",
            "        \n",
            "        >>> l = []\n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   for i in x:\n",
            "        ...     l.append(i + 1)    # Caution! Will only happen once when tracing\n",
            "        >>> f(tf.constant([1, 2, 3]))\n",
            "        >>> l\n",
            "        [<tf.Tensor ...>]\n",
            "        \n",
            "        Instead, use TensorFlow collections like `tf.TensorArray`:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
            "        ...   for i in range(len(x)):\n",
            "        ...     ta = ta.write(i, x[i] + 1)\n",
            "        ...   return ta.stack()\n",
            "        >>> f(tf.constant([1, 2, 3]))\n",
            "        <tf.Tensor: ..., numpy=array([2, 3, 4], ...)>\n",
            "        \n",
            "        _`tf.function` is polymorphic_\n",
            "        \n",
            "        Internally, `tf.function` can build more than one graph, to support arguments\n",
            "        with different data types or shapes, since TensorFlow can build more\n",
            "        efficient graphs that are specialized on shapes and dtypes. `tf.function`\n",
            "        also treats any pure Python value as opaque objects, and builds a separate\n",
            "        graph for each set of Python arguments that it encounters.\n",
            "        \n",
            "        To obtain an individual graph, use the `get_concrete_function` method of\n",
            "        the callable created by `tf.function`. It can be called with the same\n",
            "        arguments as `func` and returns a special `tf.Graph` object:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   return x + 1\n",
            "        >>> isinstance(f.get_concrete_function(1).graph, tf.Graph)\n",
            "        True\n",
            "        \n",
            "        Caution: Passing python scalars or lists as arguments to `tf.function` will\n",
            "        always build a new graph. To avoid this, pass numeric arguments as Tensors\n",
            "        whenever possible:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   return tf.abs(x)\n",
            "        >>> f1 = f.get_concrete_function(1)\n",
            "        >>> f2 = f.get_concrete_function(2)  # Slow - builds new graph\n",
            "        >>> f1 is f2\n",
            "        False\n",
            "        >>> f1 = f.get_concrete_function(tf.constant(1))\n",
            "        >>> f2 = f.get_concrete_function(tf.constant(2))  # Fast - reuses f1\n",
            "        >>> f1 is f2\n",
            "        True\n",
            "        \n",
            "        Python numerical arguments should only be used when they take few distinct\n",
            "        values, such as hyperparameters like the number of layers in a neural network.\n",
            "        \n",
            "        _Input signatures_\n",
            "        \n",
            "        For Tensor arguments, `tf.function` instantiates a separate graph for every\n",
            "        unique set of input shapes and datatypes. The example below creates two\n",
            "        separate graphs, each specialized to a different shape:\n",
            "        \n",
            "        >>> @tf.function\n",
            "        ... def f(x):\n",
            "        ...   return x + 1\n",
            "        >>> vector = tf.constant([1.0, 1.0])\n",
            "        >>> matrix = tf.constant([[3.0]])\n",
            "        >>> f.get_concrete_function(vector) is f.get_concrete_function(matrix)\n",
            "        False\n",
            "        \n",
            "        An \"input signature\" can be optionally provided to `tf.function` to control\n",
            "        the graphs traced. The input signature specifies the shape and type of each\n",
            "        Tensor argument to the function using a `tf.TensorSpec` object. More general\n",
            "        shapes can be used. This is useful to avoid creating multiple graphs when\n",
            "        Tensors have dynamic shapes. It also restricts the shape and datatype of\n",
            "        Tensors that can be used:\n",
            "        \n",
            "        >>> @tf.function(\n",
            "        ...     input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
            "        ... def f(x):\n",
            "        ...   return x + 1\n",
            "        >>> vector = tf.constant([1.0, 1.0])\n",
            "        >>> matrix = tf.constant([[3.0]])\n",
            "        >>> f.get_concrete_function(vector) is f.get_concrete_function(matrix)\n",
            "        True\n",
            "        \n",
            "        _Variables may only be created once_\n",
            "        \n",
            "        `tf.function` only allows creating new `tf.Variable` objects when it is called\n",
            "        for the first time:\n",
            "        \n",
            "        >>> class MyModule(tf.Module):\n",
            "        ...   def __init__(self):\n",
            "        ...     self.v = None\n",
            "        ...\n",
            "        ...   @tf.function\n",
            "        ...   def call(self, x):\n",
            "        ...     if self.v is None:\n",
            "        ...       self.v = tf.Variable(tf.ones_like(x))\n",
            "        ...     return self.v * x\n",
            "        \n",
            "        In general, it is recommended to create stateful objects like `tf.Variable`\n",
            "        outside of `tf.function` and passing them as arguments.\n",
            "        \n",
            "        Args:\n",
            "          func: the function to be compiled. If `func` is None, `tf.function` returns\n",
            "            a decorator that can be invoked with a single argument - `func`. In other\n",
            "            words, `tf.function(input_signature=...)(func)` is equivalent to\n",
            "            `tf.function(func, input_signature=...)`. The former can be used as\n",
            "            decorator.\n",
            "          input_signature: A possibly nested sequence of `tf.TensorSpec` objects\n",
            "            specifying the shapes and dtypes of the Tensors that will be supplied to\n",
            "            this function. If `None`, a separate function is instantiated for each\n",
            "            inferred input signature.  If input_signature is specified, every input to\n",
            "            `func` must be a `Tensor`, and `func` cannot accept `**kwargs`.\n",
            "          autograph: Whether autograph should be applied on `func` before tracing a\n",
            "            graph. Data-dependent control flow requires `autograph=True`. For more\n",
            "            information, see the [tf.function and AutoGraph guide](\n",
            "            https://www.tensorflow.org/guide/function).\n",
            "          experimental_implements: If provided, contains a name of a \"known\" function\n",
            "            this implements. For example \"mycompany.my_recurrent_cell\".\n",
            "            This is stored as an attribute in inference function,\n",
            "            which can then be detected when processing serialized function.\n",
            "            See [standardizing composite ops](https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md)  # pylint: disable=line-too-long\n",
            "            for details.  For an example of utilizing this attribute see this\n",
            "            [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc)\n",
            "            The code above automatically detects and substitutes function that\n",
            "            implements \"embedded_matmul\" and allows TFLite to substitute its own\n",
            "            implementations. For instance, a tensorflow user can use this\n",
            "             attribute to mark that their function also implements\n",
            "            `embedded_matmul` (perhaps more efficiently!)\n",
            "            by specifying it using this parameter:\n",
            "            `@tf.function(experimental_implements=\"embedded_matmul\")`\n",
            "          experimental_autograph_options: Optional tuple of\n",
            "            `tf.autograph.experimental.Feature` values.\n",
            "          experimental_relax_shapes: When True, `tf.function` may generate fewer,\n",
            "            graphs that are less specialized on input shapes.\n",
            "          experimental_compile: If True, the function is always compiled by\n",
            "            [XLA](https://www.tensorflow.org/xla). XLA may be more efficient in some\n",
            "            cases (e.g. TPU, XLA_GPU, dense tensor computations).\n",
            "        \n",
            "        Returns:\n",
            "           If `func` is not None, returns a callable that will execute the compiled\n",
            "           function (and return zero or more `tf.Tensor` objects).\n",
            "           If `func` is None, returns a decorator that, when invoked with a single\n",
            "           `func` argument, returns a callable equivalent to the case above.\n",
            "        \n",
            "        Raises:\n",
            "           ValueError when attempting to use experimental_compile, but XLA support is\n",
            "           not enabled.\n",
            "    \n",
            "    gather = gather_v2(params, indices, validate_indices=None, axis=None, batch_dims=0, name=None)\n",
            "        Gather slices from params axis `axis` according to indices.\n",
            "        \n",
            "        Gather slices from params axis `axis` according to `indices`.  `indices` must\n",
            "        be an integer tensor of any dimension (usually 0-D or 1-D).\n",
            "        \n",
            "        For 0-D (scalar) `indices`:\n",
            "        \n",
            "        $$\\begin{align*}\n",
            "        output[p_0, ..., p_{axis-1}, &&          &&& p_{axis + 1}, ..., p_{N-1}] = \\\\\n",
            "        params[p_0, ..., p_{axis-1}, && indices, &&& p_{axis + 1}, ..., p_{N-1}]\n",
            "        \\end{align*}$$\n",
            "        \n",
            "        Where *N* = `ndims(params)`.\n",
            "        \n",
            "        For 1-D (vector) `indices` with `batch_dims=0`:\n",
            "        \n",
            "        $$\\begin{align*}\n",
            "        output[p_0, ..., p_{axis-1}, &&         &i,  &&p_{axis + 1}, ..., p_{N-1}] =\\\\\n",
            "        params[p_0, ..., p_{axis-1}, && indices[&i], &&p_{axis + 1}, ..., p_{N-1}]\n",
            "        \\end{align*}$$\n",
            "        \n",
            "        In the general case, produces an output tensor where:\n",
            "        \n",
            "        $$\\begin{align*}\n",
            "        output[p_0,             &..., p_{axis-1},                       &\n",
            "               &i_{B},           ..., i_{M-1},                          &\n",
            "               p_{axis + 1},    &..., p_{N-1}]                          = \\\\\n",
            "        params[p_0,             &..., p_{axis-1},                       &\n",
            "               indices[p_0, ..., p_{B-1}, &i_{B}, ..., i_{M-1}],        &\n",
            "               p_{axis + 1},    &..., p_{N-1}]\n",
            "        \\end{align*}$$\n",
            "        \n",
            "        Where *N* = `ndims(params)`, *M* = `ndims(indices)`, and *B* = `batch_dims`.\n",
            "        Note that `params.shape[:batch_dims]` must be identical to\n",
            "        `indices.shape[:batch_dims]`.\n",
            "        \n",
            "        The shape of the output tensor is:\n",
            "        \n",
            "        > `output.shape = params.shape[:axis] + indices.shape[batch_dims:] +\n",
            "        > params.shape[axis + 1:]`.\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, a 0 is stored in the corresponding\n",
            "        output value.\n",
            "        \n",
            "        See also `tf.gather_nd`.\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\"\n",
            "        alt>\n",
            "        </div>\n",
            "        \n",
            "        Args:\n",
            "          params: The `Tensor` from which to gather values. Must be at least rank\n",
            "            `axis + 1`.\n",
            "          indices: The index `Tensor`.  Must be one of the following types: `int32`,\n",
            "            `int64`. Must be in range `[0, params.shape[axis])`.\n",
            "          validate_indices: Deprecated, does nothing.\n",
            "          axis: A `Tensor`. Must be one of the following types: `int32`, `int64`. The\n",
            "            `axis` in `params` to gather `indices` from. Must be greater than or equal\n",
            "            to `batch_dims`.  Defaults to the first non-batch dimension. Supports\n",
            "            negative indexes.\n",
            "          batch_dims: An `integer`.  The number of batch dimensions.  Must be less\n",
            "            than or equal to `rank(indices)`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `params`.\n",
            "    \n",
            "    gather_nd = gather_nd_v2(params, indices, batch_dims=0, name=None)\n",
            "        Gather slices from `params` into a Tensor with shape specified by `indices`.\n",
            "        \n",
            "        `indices` is an K-dimensional integer tensor, best thought of as a\n",
            "        (K-1)-dimensional tensor of indices into `params`, where each element defines\n",
            "        a slice of `params`:\n",
            "        \n",
            "            output[\\\\(i_0, ..., i_{K-2}\\\\)] = params[indices[\\\\(i_0, ..., i_{K-2}\\\\)]]\n",
            "        \n",
            "        Whereas in `tf.gather` `indices` defines slices into the first\n",
            "        dimension of `params`, in `tf.gather_nd`, `indices` defines slices into the\n",
            "        first `N` dimensions of `params`, where `N = indices.shape[-1]`.\n",
            "        \n",
            "        The last dimension of `indices` can be at most the rank of\n",
            "        `params`:\n",
            "        \n",
            "            indices.shape[-1] <= params.rank\n",
            "        \n",
            "        The last dimension of `indices` corresponds to elements\n",
            "        (if `indices.shape[-1] == params.rank`) or slices\n",
            "        (if `indices.shape[-1] < params.rank`) along dimension `indices.shape[-1]`\n",
            "        of `params`.  The output tensor has shape\n",
            "        \n",
            "            indices.shape[:-1] + params.shape[indices.shape[-1]:]\n",
            "        \n",
            "        Additionally both 'params' and 'indices' can have M leading batch\n",
            "        dimensions that exactly match. In this case 'batch_dims' must be M.\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, a 0 is stored in the\n",
            "        corresponding output value.\n",
            "        \n",
            "        Some examples below.\n",
            "        \n",
            "        Simple indexing into a matrix:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[0, 0], [1, 1]]\n",
            "            params = [['a', 'b'], ['c', 'd']]\n",
            "            output = ['a', 'd']\n",
            "        ```\n",
            "        \n",
            "        Slice indexing into a matrix:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[1], [0]]\n",
            "            params = [['a', 'b'], ['c', 'd']]\n",
            "            output = [['c', 'd'], ['a', 'b']]\n",
            "        ```\n",
            "        \n",
            "        Indexing into a 3-tensor:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[1]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [[['a1', 'b1'], ['c1', 'd1']]]\n",
            "        \n",
            "        \n",
            "            indices = [[0, 1], [1, 0]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [['c0', 'd0'], ['a1', 'b1']]\n",
            "        \n",
            "        \n",
            "            indices = [[0, 0, 1], [1, 0, 1]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = ['b0', 'b1']\n",
            "        ```\n",
            "        \n",
            "        The examples below are for the case when only indices have leading extra\n",
            "        dimensions. If both 'params' and 'indices' have leading batch dimensions, use\n",
            "        the 'batch_dims' parameter to run gather_nd in batch mode.\n",
            "        \n",
            "        Batched indexing into a matrix:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[[0, 0]], [[0, 1]]]\n",
            "            params = [['a', 'b'], ['c', 'd']]\n",
            "            output = [['a'], ['b']]\n",
            "        ```\n",
            "        \n",
            "        Batched slice indexing into a matrix:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[[1]], [[0]]]\n",
            "            params = [['a', 'b'], ['c', 'd']]\n",
            "            output = [[['c', 'd']], [['a', 'b']]]\n",
            "        ```\n",
            "        \n",
            "        Batched indexing into a 3-tensor:\n",
            "        \n",
            "        ```python\n",
            "            indices = [[[1]], [[0]]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [[[['a1', 'b1'], ['c1', 'd1']]],\n",
            "                      [[['a0', 'b0'], ['c0', 'd0']]]]\n",
            "        \n",
            "            indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [[['c0', 'd0'], ['a1', 'b1']],\n",
            "                      [['a0', 'b0'], ['c1', 'd1']]]\n",
            "        \n",
            "        \n",
            "            indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [['b0', 'b1'], ['d0', 'c1']]\n",
            "        ```\n",
            "        \n",
            "        Examples with batched 'params' and 'indices':\n",
            "        \n",
            "        ```python\n",
            "            batch_dims = 1\n",
            "            indices = [[1], [0]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [['c0', 'd0'], ['a1', 'b1']]\n",
            "        \n",
            "            batch_dims = 1\n",
            "            indices = [[[1]], [[0]]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [[['c0', 'd0']], [['a1', 'b1']]]\n",
            "        \n",
            "            batch_dims = 1\n",
            "            indices = [[[1, 0]], [[0, 1]]]\n",
            "            params = [[['a0', 'b0'], ['c0', 'd0']],\n",
            "                      [['a1', 'b1'], ['c1', 'd1']]]\n",
            "            output = [['c0'], ['b1']]\n",
            "        ```\n",
            "        \n",
            "        See also `tf.gather`.\n",
            "        \n",
            "        Args:\n",
            "          params: A `Tensor`. The tensor from which to gather values.\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Index tensor.\n",
            "          name: A name for the operation (optional).\n",
            "          batch_dims: An integer or a scalar 'Tensor'. The number of batch dimensions.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `params`.\n",
            "    \n",
            "    get_logger()\n",
            "        Return TF logger instance.\n",
            "    \n",
            "    get_static_value = constant_value(tensor, partial=False)\n",
            "        Returns the constant value of the given tensor, if efficiently calculable.\n",
            "        \n",
            "        This function attempts to partially evaluate the given tensor, and\n",
            "        returns its value as a numpy ndarray if this succeeds.\n",
            "        \n",
            "        Compatibility(V1): If `constant_value(tensor)` returns a non-`None` result, it\n",
            "        will no longer be possible to feed a different value for `tensor`. This allows\n",
            "        the result of this function to influence the graph that is constructed, and\n",
            "        permits static shape optimizations.\n",
            "        \n",
            "        Args:\n",
            "          tensor: The Tensor to be evaluated.\n",
            "          partial: If True, the returned numpy array is allowed to have partially\n",
            "            evaluated values. Values that can't be evaluated will be None.\n",
            "        \n",
            "        Returns:\n",
            "          A numpy ndarray containing the constant value of the given `tensor`,\n",
            "          or None if it cannot be calculated.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if tensor is not an ops.Tensor.\n",
            "    \n",
            "    grad_pass_through(f)\n",
            "        Creates a grad-pass-through op with the forward behavior provided in f.\n",
            "        \n",
            "        Use this function to wrap any op, maintaining its behavior in the forward\n",
            "        pass, but replacing the original op in the backward graph with an identity.\n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.Variable(1.0, name=\"x\")\n",
            "        z = tf.Variable(3.0, name=\"z\")\n",
            "        \n",
            "        with tf.GradientTape() as tape:\n",
            "          # y will evaluate to 9.0\n",
            "          y = tf.grad_pass_through(x.assign)(z**2)\n",
            "        # grads will evaluate to 6.0\n",
            "        grads = tape.gradient(y, z)\n",
            "        ```\n",
            "        \n",
            "        Another example is a 'differentiable' moving average approximation, where\n",
            "        gradients are allowed to flow into the last value fed to the moving average,\n",
            "        but the moving average is still used for the forward pass:\n",
            "        \n",
            "        ```python\n",
            "        x = ... # Some scalar value\n",
            "        # A moving average object, we don't need to know how this is implemented\n",
            "        moving_average = MovingAverage()\n",
            "        with backprop.GradientTape() as tape:\n",
            "          # mavg_x will evaluate to the current running average value\n",
            "          mavg_x = tf.grad_pass_through(moving_average)(x)\n",
            "        grads = tape.gradient(mavg_x, x) # grads will evaluate to 1.0\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          f: function `f(*x)` that returns a `Tensor` or nested structure of `Tensor`\n",
            "            outputs.\n",
            "        \n",
            "        Returns:\n",
            "         A function `h(x)` which returns the same values as `f(x)` and whose\n",
            "         gradients are the same as those of an identity function.\n",
            "    \n",
            "    gradients = gradients_v2(ys, xs, grad_ys=None, name='gradients', gate_gradients=False, aggregation_method=None, stop_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n",
            "        Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n",
            "        \n",
            "        `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
            "        is a list of `Tensor`, holding the gradients received by the\n",
            "        `ys`. The list must be the same length as `ys`.\n",
            "        \n",
            "        `gradients()` adds ops to the graph to output the derivatives of `ys` with\n",
            "        respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n",
            "        each tensor is the `sum(dy/dx)` for y in `ys` and for x in `xs`.\n",
            "        \n",
            "        `grad_ys` is a list of tensors of the same length as `ys` that holds\n",
            "        the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
            "        we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
            "        user can provide their own initial `grad_ys` to compute the\n",
            "        derivatives using a different initial gradient for each y (e.g., if\n",
            "        one wanted to weight the gradient differently for each value in\n",
            "        each y).\n",
            "        \n",
            "        `stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n",
            "        with respect to all `xs`. These tensors will not be backpropagated through,\n",
            "        as though they had been explicitly disconnected using `stop_gradient`.  Among\n",
            "        other things, this allows computation of partial derivatives as opposed to\n",
            "        total derivatives. For example:\n",
            "        \n",
            "        ```python\n",
            "        a = tf.constant(0.)\n",
            "        b = 2 * a\n",
            "        g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
            "        ```\n",
            "        \n",
            "        Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n",
            "        total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n",
            "        influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n",
            "        equivalent to:\n",
            "        \n",
            "        ```python\n",
            "        a = tf.stop_gradient(tf.constant(0.))\n",
            "        b = tf.stop_gradient(2 * a)\n",
            "        g = tf.gradients(a + b, [a, b])\n",
            "        ```\n",
            "        \n",
            "        `stop_gradients` provides a way of stopping gradient after the graph has\n",
            "        already been constructed, as compared to `tf.stop_gradient` which is used\n",
            "        during graph construction.  When the two approaches are combined,\n",
            "        backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n",
            "        `stop_gradients`, whichever is encountered first.\n",
            "        \n",
            "        All integer tensors are considered constant with respect to all `xs`, as if\n",
            "        they were included in `stop_gradients`.\n",
            "        \n",
            "        `unconnected_gradients` determines the value returned for each x in xs if it\n",
            "        is unconnected in the graph to ys. By default this is None to safeguard\n",
            "        against errors. Mathematically these gradients are zero which can be requested\n",
            "        using the `'zero'` option. `tf.UnconnectedGradients` provides the\n",
            "        following options and behaviors:\n",
            "        \n",
            "        ```python\n",
            "        a = tf.ones([1, 2])\n",
            "        b = tf.ones([3, 1])\n",
            "        g1 = tf.gradients([b], [a], unconnected_gradients='none')\n",
            "        sess.run(g1)  # [None]\n",
            "        \n",
            "        g2 = tf.gradients([b], [a], unconnected_gradients='zero')\n",
            "        sess.run(g2)  # [array([[0., 0.]], dtype=float32)]\n",
            "        ```\n",
            "        \n",
            "        Let us take one practical example which comes during the back propogation\n",
            "        phase. This function is used to evaluate the derivatives of the cost function\n",
            "        with respect to Weights `Ws` and Biases `bs`. Below sample implementation\n",
            "        provides the exaplantion of what it is actually used for :\n",
            "        \n",
            "        ```python\n",
            "        Ws = tf.constant(0.)\n",
            "        bs = 2 * Ws\n",
            "        cost = Ws + bs  # This is just an example. So, please ignore the formulas.\n",
            "        g = tf.gradients(cost, [Ws, bs])\n",
            "        dCost_dW, dCost_db = g\n",
            "        ```\n",
            "        \n",
            "        \n",
            "        Args:\n",
            "          ys: A `Tensor` or list of tensors to be differentiated.\n",
            "          xs: A `Tensor` or list of tensors to be used for differentiation.\n",
            "          grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
            "            `ys` and holding the gradients computed for each y in `ys`.\n",
            "          name: Optional name to use for grouping all the gradient ops together.\n",
            "            defaults to 'gradients'.\n",
            "          gate_gradients: If True, add a tuple around the gradients returned\n",
            "            for an operations.  This avoids some race conditions.\n",
            "          aggregation_method: Specifies the method used to combine gradient terms.\n",
            "            Accepted values are constants defined in the class `AggregationMethod`.\n",
            "          stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n",
            "            through.\n",
            "          unconnected_gradients: Optional. Specifies the gradient value returned when\n",
            "            the given input tensors are unconnected. Accepted values are constants\n",
            "            defined in the class `tf.UnconnectedGradients` and the default value is\n",
            "            `none`.\n",
            "        \n",
            "        Returns:\n",
            "          A list of `Tensor` of length `len(xs)` where each tensor is the `sum(dy/dx)`\n",
            "          for y in `ys` and for x in `xs`.\n",
            "        \n",
            "        Raises:\n",
            "          LookupError: if one of the operations between `x` and `y` does not\n",
            "            have a registered gradient function.\n",
            "          ValueError: if the arguments are invalid.\n",
            "          RuntimeError: if called in Eager mode.\n",
            "    \n",
            "    greater(x, y, name=None)\n",
            "        Returns the truth value of (x > y) element-wise.\n",
            "        \n",
            "        *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5, 2, 5])\n",
            "        tf.math.greater(x, y) ==> [False, True, True]\n",
            "        \n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5])\n",
            "        tf.math.greater(x, y) ==> [False, False, True]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    greater_equal(x, y, name=None)\n",
            "        Returns the truth value of (x >= y) element-wise.\n",
            "        \n",
            "        *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([5, 4, 6, 7])\n",
            "        y = tf.constant([5, 2, 5, 10])\n",
            "        tf.math.greater_equal(x, y) ==> [True, True, True, False]\n",
            "        \n",
            "        x = tf.constant([5, 4, 6, 7])\n",
            "        y = tf.constant([5])\n",
            "        tf.math.greater_equal(x, y) ==> [True, False, True, True]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    group(*inputs, **kwargs)\n",
            "        Create an op that groups multiple operations.\n",
            "        \n",
            "        When this op finishes, all ops in `inputs` have finished. This op has no\n",
            "        output.\n",
            "        \n",
            "        See also `tf.tuple` and\n",
            "        `tf.control_dependencies`.\n",
            "        \n",
            "        Args:\n",
            "          *inputs: Zero or more tensors to group.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          An Operation that executes all its inputs.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If an unknown keyword argument is provided.\n",
            "    \n",
            "    guarantee_const(input, name=None)\n",
            "        Gives a guarantee to the TF runtime that the input tensor is a constant.\n",
            "        \n",
            "        The runtime is then free to make optimizations based on this.\n",
            "        \n",
            "        Only accepts value typed tensors as inputs and rejects resource variable handles\n",
            "        as input.\n",
            "        \n",
            "        Returns the input tensor without modification.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    hessians = HessiansV2(ys, xs, gate_gradients=False, aggregation_method=None, name='hessians')\n",
            "        Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.\n",
            "        \n",
            "        `hessians()` adds ops to the graph to output the Hessian matrix of `ys`\n",
            "        with respect to `xs`.  It returns a list of `Tensor` of length `len(xs)`\n",
            "        where each tensor is the Hessian of `sum(ys)`.\n",
            "        \n",
            "        The Hessian is a matrix of second-order partial derivatives of a scalar\n",
            "        tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n",
            "        \n",
            "        Args:\n",
            "          ys: A `Tensor` or list of tensors to be differentiated.\n",
            "          xs: A `Tensor` or list of tensors to be used for differentiation.\n",
            "          name: Optional name to use for grouping all the gradient ops together.\n",
            "            defaults to 'hessians'.\n",
            "          colocate_gradients_with_ops: See `gradients()` documentation for details.\n",
            "          gate_gradients: See `gradients()` documentation for details.\n",
            "          aggregation_method: See `gradients()` documentation for details.\n",
            "        \n",
            "        Returns:\n",
            "          A list of Hessian matrices of `sum(ys)` for each `x` in `xs`.\n",
            "        \n",
            "        Raises:\n",
            "          LookupError: if one of the operations between `xs` and `ys` does not\n",
            "            have a registered gradient function.\n",
            "    \n",
            "    histogram_fixed_width(values, value_range, nbins=100, dtype=tf.int32, name=None)\n",
            "        Return histogram of values.\n",
            "        \n",
            "        Given the tensor `values`, this operation returns a rank 1 histogram counting\n",
            "        the number of entries in `values` that fell into every bin.  The bins are\n",
            "        equal width and determined by the arguments `value_range` and `nbins`.\n",
            "        \n",
            "        Args:\n",
            "          values:  Numeric `Tensor`.\n",
            "          value_range:  Shape [2] `Tensor` of same `dtype` as `values`.\n",
            "            values <= value_range[0] will be mapped to hist[0],\n",
            "            values >= value_range[1] will be mapped to hist[-1].\n",
            "          nbins:  Scalar `int32 Tensor`.  Number of histogram bins.\n",
            "          dtype:  dtype for returned histogram.\n",
            "          name:  A name for this operation (defaults to 'histogram_fixed_width').\n",
            "        \n",
            "        Returns:\n",
            "          A 1-D `Tensor` holding histogram of values.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If any unsupported dtype is provided.\n",
            "          tf.errors.InvalidArgumentError: If value_range does not\n",
            "              satisfy value_range[0] < value_range[1].\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        ```python\n",
            "        # Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)\n",
            "        nbins = 5\n",
            "        value_range = [0.0, 5.0]\n",
            "        new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\n",
            "        \n",
            "        with tf.compat.v1.get_default_session() as sess:\n",
            "          hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)\n",
            "          variables.global_variables_initializer().run()\n",
            "          sess.run(hist) => [2, 1, 1, 0, 2]\n",
            "        ```\n",
            "    \n",
            "    histogram_fixed_width_bins(values, value_range, nbins=100, dtype=tf.int32, name=None)\n",
            "        Bins the given values for use in a histogram.\n",
            "        \n",
            "        Given the tensor `values`, this operation returns a rank 1 `Tensor`\n",
            "        representing the indices of a histogram into which each element\n",
            "        of `values` would be binned. The bins are equal width and\n",
            "        determined by the arguments `value_range` and `nbins`.\n",
            "        \n",
            "        Args:\n",
            "          values:  Numeric `Tensor`.\n",
            "          value_range:  Shape [2] `Tensor` of same `dtype` as `values`.\n",
            "            values <= value_range[0] will be mapped to hist[0],\n",
            "            values >= value_range[1] will be mapped to hist[-1].\n",
            "          nbins:  Scalar `int32 Tensor`.  Number of histogram bins.\n",
            "          dtype:  dtype for returned histogram.\n",
            "          name:  A name for this operation (defaults to 'histogram_fixed_width').\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` holding the indices of the binned values whose shape matches\n",
            "          `values`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If any unsupported dtype is provided.\n",
            "          tf.errors.InvalidArgumentError: If value_range does not\n",
            "              satisfy value_range[0] < value_range[1].\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        ```python\n",
            "        # Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)\n",
            "        nbins = 5\n",
            "        value_range = [0.0, 5.0]\n",
            "        new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]\n",
            "        \n",
            "        with tf.compat.v1.get_default_session() as sess:\n",
            "          indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)\n",
            "          variables.global_variables_initializer().run()\n",
            "          sess.run(indices) # [0, 0, 1, 2, 4, 4]\n",
            "        ```\n",
            "    \n",
            "    identity(input, name=None)\n",
            "        Return a Tensor with the same shape and contents as input.\n",
            "        \n",
            "        The return value is not the same Tensor as the original, but contains the same\n",
            "        values.  This operation is fast when used on the same device.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> a = tf.constant([0.78])\n",
            "        >>> a_identity = tf.identity(a)\n",
            "        >>> a.numpy()\n",
            "        array([0.78], dtype=float32)\n",
            "        >>> a_identity.numpy()\n",
            "        array([0.78], dtype=float32)\n",
            "        \n",
            "        Calling `tf.identity` on a variable will make a Tensor that represents the\n",
            "        value of that variable at the time it is called. This is equivalent to calling\n",
            "        `<variable>.read_value()`.\n",
            "        \n",
            "        >>> a = tf.Variable(5)\n",
            "        >>> a_identity = tf.identity(a)\n",
            "        >>> a.assign_add(1)\n",
            "        <tf.Variable ... shape=() dtype=int32, numpy=6>\n",
            "        >>> a.numpy()\n",
            "        6\n",
            "        >>> a_identity.numpy()\n",
            "        5\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    identity_n(input, name=None)\n",
            "        Returns a list of tensors with the same shapes and contents as the input\n",
            "        \n",
            "        tensors.\n",
            "        \n",
            "        This op can be used to override the gradient for complicated functions. For\n",
            "        example, suppose y = f(x) and we wish to apply a custom function g for backprop\n",
            "        such that dx = g(dy). In Python,\n",
            "        \n",
            "        ```python\n",
            "        with tf.get_default_graph().gradient_override_map(\n",
            "            {'IdentityN': 'OverrideGradientWithG'}):\n",
            "          y, _ = identity_n([f(x), x])\n",
            "        \n",
            "        @tf.RegisterGradient('OverrideGradientWithG')\n",
            "        def ApplyG(op, dy, _):\n",
            "          return [None, g(dy)]  # Do not backprop to f(x).\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input: A list of `Tensor` objects.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A list of `Tensor` objects. Has the same type as `input`.\n",
            "    \n",
            "    import_graph_def(graph_def, input_map=None, return_elements=None, name=None, op_dict=None, producer_op_list=None)\n",
            "        Imports the graph from `graph_def` into the current default `Graph`. (deprecated arguments)\n",
            "        \n",
            "        Warning: SOME ARGUMENTS ARE DEPRECATED: `(op_dict)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        Please file an issue at https://github.com/tensorflow/tensorflow/issues if you depend on this feature.\n",
            "        \n",
            "        This function provides a way to import a serialized TensorFlow\n",
            "        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
            "        protocol buffer, and extract individual objects in the `GraphDef` as\n",
            "        `tf.Tensor` and `tf.Operation` objects. Once extracted,\n",
            "        these objects are placed into the current default `Graph`. See\n",
            "        `tf.Graph.as_graph_def` for a way to create a `GraphDef`\n",
            "        proto.\n",
            "        \n",
            "        Args:\n",
            "          graph_def: A `GraphDef` proto containing operations to be imported into\n",
            "            the default graph.\n",
            "          input_map: A dictionary mapping input names (as strings) in `graph_def`\n",
            "            to `Tensor` objects. The values of the named input tensors in the\n",
            "            imported graph will be re-mapped to the respective `Tensor` values.\n",
            "          return_elements: A list of strings containing operation names in\n",
            "            `graph_def` that will be returned as `Operation` objects; and/or\n",
            "            tensor names in `graph_def` that will be returned as `Tensor` objects.\n",
            "          name: (Optional.) A prefix that will be prepended to the names in\n",
            "            `graph_def`. Note that this does not apply to imported function names.\n",
            "            Defaults to `\"import\"`.\n",
            "          op_dict: (Optional.) Deprecated, do not use.\n",
            "          producer_op_list: (Optional.) An `OpList` proto with the (possibly stripped)\n",
            "            list of `OpDef`s used by the producer of the graph. If provided,\n",
            "            unrecognized attrs for ops in `graph_def` that have their default value\n",
            "            according to `producer_op_list` will be removed. This will allow some more\n",
            "            `GraphDef`s produced by later binaries to be accepted by earlier binaries.\n",
            "        \n",
            "        Returns:\n",
            "          A list of `Operation` and/or `Tensor` objects from the imported graph,\n",
            "          corresponding to the names in `return_elements`,\n",
            "          and None if `returns_elements` is None.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `graph_def` is not a `GraphDef` proto,\n",
            "            `input_map` is not a dictionary mapping strings to `Tensor` objects,\n",
            "            or `return_elements` is not a list of strings.\n",
            "          ValueError: If `input_map`, or `return_elements` contains names that\n",
            "            do not appear in `graph_def`, or `graph_def` is not well-formed (e.g.\n",
            "            it refers to an unknown tensor).\n",
            "    \n",
            "    init_scope()\n",
            "        A context manager that lifts ops out of control-flow scopes and function-building graphs.\n",
            "        \n",
            "        There is often a need to lift variable initialization ops out of control-flow\n",
            "        scopes, function-building graphs, and gradient tapes. Entering an\n",
            "        `init_scope` is a mechanism for satisfying these desiderata. In particular,\n",
            "        entering an `init_scope` has three effects:\n",
            "        \n",
            "          (1) All control dependencies are cleared the moment the scope is entered;\n",
            "              this is equivalent to entering the context manager returned from\n",
            "              `control_dependencies(None)`, which has the side-effect of exiting\n",
            "              control-flow scopes like `tf.cond` and `tf.while_loop`.\n",
            "        \n",
            "          (2) All operations that are created while the scope is active are lifted\n",
            "              into the lowest context on the `context_stack` that is not building a\n",
            "              graph function. Here, a context is defined as either a graph or an eager\n",
            "              context. Every context switch, i.e., every installation of a graph as\n",
            "              the default graph and every switch into eager mode, is logged in a\n",
            "              thread-local stack called `context_switches`; the log entry for a\n",
            "              context switch is popped from the stack when the context is exited.\n",
            "              Entering an `init_scope` is equivalent to crawling up\n",
            "              `context_switches`, finding the first context that is not building a\n",
            "              graph function, and entering it. A caveat is that if graph mode is\n",
            "              enabled but the default graph stack is empty, then entering an\n",
            "              `init_scope` will simply install a fresh graph as the default one.\n",
            "        \n",
            "          (3) The gradient tape is paused while the scope is active.\n",
            "        \n",
            "        When eager execution is enabled, code inside an init_scope block runs with\n",
            "        eager execution enabled even when tracing a `tf.function`. For example:\n",
            "        \n",
            "        ```python\n",
            "        tf.compat.v1.enable_eager_execution()\n",
            "        \n",
            "        @tf.function\n",
            "        def func():\n",
            "          # A function constructs TensorFlow graphs,\n",
            "          # it does not execute eagerly.\n",
            "          assert not tf.executing_eagerly()\n",
            "          with tf.init_scope():\n",
            "            # Initialization runs with eager execution enabled\n",
            "            assert tf.executing_eagerly()\n",
            "        ```\n",
            "        \n",
            "        Raises:\n",
            "          RuntimeError: if graph state is incompatible with this initialization.\n",
            "    \n",
            "    is_tensor(x)\n",
            "        Checks whether `x` is a tensor or \"tensor-like\".\n",
            "        \n",
            "        If `is_tensor(x)` returns `True`, it is safe to assume that `x` is a tensor or\n",
            "        can be converted to a tensor using `ops.convert_to_tensor(x)`.\n",
            "        \n",
            "        Usage example:\n",
            "        \n",
            "        >>> tf.is_tensor(tf.constant([[1,2,3],[4,5,6],[7,8,9]])) \n",
            "        True\n",
            "        >>> tf.is_tensor(\"Hello World\")\n",
            "        False\n",
            "          \n",
            "        Args:\n",
            "          x: A python object to check.\n",
            "        \n",
            "        Returns:\n",
            "          `True` if `x` is a tensor or \"tensor-like\", `False` if not.\n",
            "    \n",
            "    less(x, y, name=None)\n",
            "        Returns the truth value of (x < y) element-wise.\n",
            "        \n",
            "        *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5])\n",
            "        tf.math.less(x, y) ==> [False, True, False]\n",
            "        \n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5, 6, 7])\n",
            "        tf.math.less(x, y) ==> [False, True, True]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    less_equal(x, y, name=None)\n",
            "        Returns the truth value of (x <= y) element-wise.\n",
            "        \n",
            "        *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5])\n",
            "        tf.math.less_equal(x, y) ==> [True, True, False]\n",
            "        \n",
            "        x = tf.constant([5, 4, 6])\n",
            "        y = tf.constant([5, 6, 6])\n",
            "        tf.math.less_equal(x, y) ==> [True, True, True]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    linspace = lin_space(start, stop, num, name=None)\n",
            "        Generates values in an interval.\n",
            "        \n",
            "        A sequence of `num` evenly-spaced values are generated beginning at `start`.\n",
            "        If `num > 1`, the values in the sequence increase by `stop - start / num - 1`,\n",
            "        so that the last one is exactly `stop`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```\n",
            "        tf.linspace(10.0, 12.0, 3, name=\"linspace\") => [ 10.0  11.0  12.0]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          start: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`.\n",
            "            0-D tensor. First entry in the range.\n",
            "          stop: A `Tensor`. Must have the same type as `start`.\n",
            "            0-D tensor. Last entry in the range.\n",
            "          num: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            0-D tensor. Number of values to generate.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `start`.\n",
            "    \n",
            "    load_library(library_location)\n",
            "        Loads a TensorFlow plugin.\n",
            "        \n",
            "        \"library_location\" can be a path to a specific shared object, or a folder.\n",
            "        If it is a folder, all shared objects that are named \"libtfkernel*\" will be\n",
            "        loaded. When the library is loaded, kernels registered in the library via the\n",
            "        `REGISTER_*` macros are made available in the TensorFlow process.\n",
            "        \n",
            "        Args:\n",
            "          library_location: Path to the plugin or the folder of plugins.\n",
            "            Relative or absolute filesystem path to a dynamic library file or folder.\n",
            "        \n",
            "        Returns:\n",
            "          None\n",
            "        \n",
            "        Raises:\n",
            "          OSError: When the file to be loaded is not found.\n",
            "          RuntimeError: when unable to load the library.\n",
            "    \n",
            "    load_op_library(library_filename)\n",
            "        Loads a TensorFlow plugin, containing custom ops and kernels.\n",
            "        \n",
            "        Pass \"library_filename\" to a platform-specific mechanism for dynamically\n",
            "        loading a library. The rules for determining the exact location of the\n",
            "        library are platform-specific and are not documented here. When the\n",
            "        library is loaded, ops and kernels registered in the library via the\n",
            "        `REGISTER_*` macros are made available in the TensorFlow process. Note\n",
            "        that ops with the same name as an existing op are rejected and not\n",
            "        registered with the process.\n",
            "        \n",
            "        Args:\n",
            "          library_filename: Path to the plugin.\n",
            "            Relative or absolute filesystem path to a dynamic library file.\n",
            "        \n",
            "        Returns:\n",
            "          A python module containing the Python wrappers for Ops defined in\n",
            "          the plugin.\n",
            "        \n",
            "        Raises:\n",
            "          RuntimeError: when unable to load the library or get the python wrappers.\n",
            "    \n",
            "    logical_and(x, y, name=None)\n",
            "        Logical AND function.\n",
            "        \n",
            "        The operation works for the following input types:\n",
            "        \n",
            "        - Two single elements of type `bool`\n",
            "        - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n",
            "          be calculated by applying logical AND with the single element to each\n",
            "          element in the larger Tensor.\n",
            "        - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n",
            "          the result will be the element-wise logical AND of the two input tensors.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        >>> a = tf.constant([True])\n",
            "        >>> b = tf.constant([False])\n",
            "        >>> tf.math.logical_and(a, b)\n",
            "        <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n",
            "        \n",
            "        >>> c = tf.constant([True])\n",
            "        >>> x = tf.constant([False, True, True, False])\n",
            "        >>> tf.math.logical_and(c, x)\n",
            "        <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n",
            "        \n",
            "        >>> y = tf.constant([False, False, True, True])\n",
            "        >>> z = tf.constant([False, True, False, True])\n",
            "        >>> tf.math.logical_and(y, z)\n",
            "        <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])>\n",
            "        \n",
            "        Args:\n",
            "            x: A `tf.Tensor` type bool.\n",
            "            y: A `tf.Tensor` of type bool.\n",
            "            name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "    \n",
            "    logical_not(x, name=None)\n",
            "        Returns the truth value of `NOT x` element-wise.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        >>> tf.math.logical_not(tf.constant([True, False]))\n",
            "        <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` of type `bool`. A `Tensor` of type `bool`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    logical_or(x, y, name=None)\n",
            "        Returns the truth value of x OR y element-wise.\n",
            "        \n",
            "        *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` of type `bool`.\n",
            "          y: A `Tensor` of type `bool`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `bool`.\n",
            "    \n",
            "    make_ndarray = MakeNdarray(tensor)\n",
            "        Create a numpy ndarray from a tensor.\n",
            "        \n",
            "        Create a numpy ndarray with the same shape and data as the tensor.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        # Tensor a has shape (2,3)\n",
            "        a = tf.constant([[1,2,3],[4,5,6]])\n",
            "        proto_tensor = tf.make_tensor_proto(a)  # convert `tensor a` to a proto tensor\n",
            "        tf.make_ndarray(proto_tensor) # output: array([[1, 2, 3],\n",
            "        #                                              [4, 5, 6]], dtype=int32)\n",
            "        # output has shape (2,3)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          tensor: A TensorProto.\n",
            "        \n",
            "        Returns:\n",
            "          A numpy array with the tensor contents.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if tensor has unsupported type.\n",
            "    \n",
            "    make_tensor_proto(values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False)\n",
            "        Create a TensorProto.\n",
            "        \n",
            "        In TensorFlow 2.0, representing tensors as protos should no longer be a\n",
            "        common workflow. That said, this utility function is still useful for\n",
            "        generating TF Serving request protos:\n",
            "        \n",
            "        ```python\n",
            "          request = tensorflow_serving.apis.predict_pb2.PredictRequest()\n",
            "          request.model_spec.name = \"my_model\"\n",
            "          request.model_spec.signature_name = \"serving_default\"\n",
            "          request.inputs[\"images\"].CopyFrom(tf.make_tensor_proto(X_new))\n",
            "        ```\n",
            "        \n",
            "        `make_tensor_proto` accepts \"values\" of a python scalar, a python list, a\n",
            "        numpy ndarray, or a numpy scalar.\n",
            "        \n",
            "        If \"values\" is a python scalar or a python list, make_tensor_proto\n",
            "        first convert it to numpy ndarray. If dtype is None, the\n",
            "        conversion tries its best to infer the right numpy data\n",
            "        type. Otherwise, the resulting numpy array has a compatible data\n",
            "        type with the given dtype.\n",
            "        \n",
            "        In either case above, the numpy ndarray (either the caller provided\n",
            "        or the auto-converted) must have the compatible type with dtype.\n",
            "        \n",
            "        `make_tensor_proto` then converts the numpy array to a tensor proto.\n",
            "        \n",
            "        If \"shape\" is None, the resulting tensor proto represents the numpy\n",
            "        array precisely.\n",
            "        \n",
            "        Otherwise, \"shape\" specifies the tensor's shape and the numpy array\n",
            "        can not have more elements than what \"shape\" specifies.\n",
            "        \n",
            "        Args:\n",
            "          values:         Values to put in the TensorProto.\n",
            "          dtype:          Optional tensor_pb2 DataType value.\n",
            "          shape:          List of integers representing the dimensions of tensor.\n",
            "          verify_shape:   Boolean that enables verification of a shape of values.\n",
            "          allow_broadcast:  Boolean that enables allowing scalars and 1 length vector\n",
            "              broadcasting. Cannot be true when verify_shape is true.\n",
            "        \n",
            "        Returns:\n",
            "          A `TensorProto`. Depending on the type, it may contain data in the\n",
            "          \"tensor_content\" attribute, which is not directly useful to Python programs.\n",
            "          To access the values you should convert the proto back to a numpy ndarray\n",
            "          with `tf.make_ndarray(proto)`.\n",
            "        \n",
            "          If `values` is a `TensorProto`, it is immediately returned; `dtype` and\n",
            "          `shape` are ignored.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError:  if unsupported types are provided.\n",
            "          ValueError: if arguments have inappropriate values or if verify_shape is\n",
            "           True and shape of values is not equals to a shape from the argument.\n",
            "    \n",
            "    map_fn = map_fn_v2(fn, elems, dtype=None, parallel_iterations=None, back_prop=True, swap_memory=False, infer_shape=True, name=None)\n",
            "        map on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n",
            "        \n",
            "        Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(back_prop=False)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "        Instead of:\n",
            "        results = tf.map_fn(fn, elems, back_prop=False)\n",
            "        Use:\n",
            "        results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "        \n",
            "        The simplest version of `map_fn` repeatedly applies the callable `fn` to a\n",
            "        sequence of elements from first to last. The elements are made of the\n",
            "        tensors unpacked from `elems`. `dtype` is the data type of the return\n",
            "        value of `fn`. Users must provide `dtype` if it is different from\n",
            "        the data type of `elems`.\n",
            "        \n",
            "        Suppose that `elems` is unpacked into `values`, a list of tensors. The shape\n",
            "        of the result tensor is `[values.shape[0]] + fn(values[0]).shape`.\n",
            "        \n",
            "        This method also allows multi-arity `elems` and output of `fn`.  If `elems`\n",
            "        is a (possibly nested) list or tuple of tensors, then each of these tensors\n",
            "        must have a matching first (unpack) dimension.  The signature of `fn` may\n",
            "        match the structure of `elems`.  That is, if `elems` is\n",
            "        `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is:\n",
            "        `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n",
            "        \n",
            "        Furthermore, `fn` may emit a different structure than its input.  For example,\n",
            "        `fn` may look like: `fn = lambda t1: return (t1 + 1, t1 - 1)`.  In this case,\n",
            "        the `dtype` parameter is not optional: `dtype` must be a type or (possibly\n",
            "        nested) tuple of types matching the output of `fn`.\n",
            "        \n",
            "        To apply a functional operation to the nonzero elements of a SparseTensor\n",
            "        one of the following methods is recommended. First, if the function is\n",
            "        expressible as TensorFlow ops, use\n",
            "        \n",
            "        ```python\n",
            "          result = SparseTensor(input.indices, fn(input.values), input.dense_shape)\n",
            "        ```\n",
            "        \n",
            "        If, however, the function is not expressible as a TensorFlow op, then use\n",
            "        \n",
            "        ```python\n",
            "        result = SparseTensor(\n",
            "          input.indices, map_fn(fn, input.values), input.dense_shape)\n",
            "        ```\n",
            "        \n",
            "        instead.\n",
            "        \n",
            "        When executing eagerly, map_fn does not execute in parallel even if\n",
            "        `parallel_iterations` is set to a value > 1. You can still get the\n",
            "        performance benefits of running a function in parallel by using the\n",
            "        `tf.function` decorator,\n",
            "        \n",
            "        ```python\n",
            "        # Assume the function being used in map_fn is fn.\n",
            "        # To ensure map_fn calls fn in parallel, use the tf.function decorator.\n",
            "        @tf.function\n",
            "        def func(tensor):\n",
            "          return tf.map_fn(fn, tensor)\n",
            "        ```\n",
            "        \n",
            "        Note that if you use the `tf.function` decorator, any non-TensorFlow Python\n",
            "        code that you may have written in your function won't get executed. See\n",
            "        [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) for\n",
            "        more  details. The recommendation would be to debug without `tf.function` but\n",
            "        switch to it to get performance benefits of running `map_fn` in parallel.\n",
            "        \n",
            "        Args:\n",
            "          fn: The callable to be performed.  It accepts one argument, which will have\n",
            "            the same (possibly nested) structure as `elems`.  Its output must have the\n",
            "            same structure as `dtype` if one is provided, otherwise it must have the\n",
            "            same structure as `elems`.\n",
            "          elems: A tensor or (possibly nested) sequence of tensors, each of which will\n",
            "            be unpacked along their first dimension.  The nested sequence of the\n",
            "            resulting slices will be applied to `fn`.\n",
            "          dtype: (optional) The output type(s) of `fn`.  If `fn` returns a structure\n",
            "            of Tensors differing from the structure of `elems`, then `dtype` is not\n",
            "            optional and must have the same structure as the output of `fn`.\n",
            "          parallel_iterations: (optional) The number of iterations allowed to run in\n",
            "            parallel. When graph building, the default value is 10. While executing\n",
            "            eagerly, the default value is set to 1.\n",
            "          back_prop: (optional) Deprecated. False disables support for back\n",
            "            propagation. Prefer using `tf.stop_gradient` instead.\n",
            "          swap_memory: (optional) True enables GPU-CPU memory swapping.\n",
            "          infer_shape: (optional) False disables tests for consistent output shapes.\n",
            "          name: (optional) Name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor or (possibly nested) sequence of tensors.  Each tensor packs the\n",
            "          results of applying `fn` to tensors unpacked from `elems` along the first\n",
            "          dimension, from first to last.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `fn` is not callable or the structure of the output of\n",
            "            `fn` and `dtype` do not match, or if elems is a SparseTensor.\n",
            "          ValueError: if the lengths of the output of `fn` and `dtype` do not match.\n",
            "        \n",
            "        Examples:\n",
            "          ```python\n",
            "          elems = np.array([1, 2, 3, 4, 5, 6])\n",
            "          squares = map_fn(lambda x: x * x, elems)\n",
            "          # squares == [1, 4, 9, 16, 25, 36]\n",
            "          ```\n",
            "        \n",
            "          ```python\n",
            "          elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
            "          alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
            "          # alternate == [-1, 2, -3]\n",
            "          ```\n",
            "        \n",
            "          ```python\n",
            "          elems = np.array([1, 2, 3])\n",
            "          alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\n",
            "          # alternates[0] == [1, 2, 3]\n",
            "          # alternates[1] == [-1, -2, -3]\n",
            "          ```\n",
            "    \n",
            "    matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None)\n",
            "        Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
            "        \n",
            "        The inputs must, following any transpositions, be tensors of rank >= 2\n",
            "        where the inner 2 dimensions specify valid matrix multiplication dimensions,\n",
            "        and any further outer dimensions specify matching batch size.\n",
            "        \n",
            "        Both matrices must be of the same type. The supported types are:\n",
            "        `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
            "        \n",
            "        Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
            "        the fly by setting one of the corresponding flag to `True`. These are `False`\n",
            "        by default.\n",
            "        \n",
            "        If one or both of the matrices contain a lot of zeros, a more efficient\n",
            "        multiplication algorithm can be used by setting the corresponding\n",
            "        `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
            "        This optimization is only available for plain matrices (rank-2 tensors) with\n",
            "        datatypes `bfloat16` or `float32`.\n",
            "        \n",
            "        A simple 2-D tensor matrix multiplication:\n",
            "        \n",
            "        >>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
            "        >>> a  # 2-D tensor\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "        array([[1, 2, 3],\n",
            "               [4, 5, 6]], dtype=int32)>\n",
            "        >>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
            "        >>> b  # 2-D tensor\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "        array([[ 7,  8],\n",
            "               [ 9, 10],\n",
            "               [11, 12]], dtype=int32)>\n",
            "        >>> c = tf.matmul(a, b)\n",
            "        >>> c  # `a` * `b`\n",
            "        <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
            "        array([[ 58,  64],\n",
            "               [139, 154]], dtype=int32)>\n",
            "        \n",
            "        A batch matrix multiplication with batch shape [2]:\n",
            "        \n",
            "        >>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
            "        >>> a  # 3-D tensor\n",
            "        <tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
            "        array([[[ 1,  2,  3],\n",
            "                [ 4,  5,  6]],\n",
            "               [[ 7,  8,  9],\n",
            "                [10, 11, 12]]], dtype=int32)>\n",
            "        >>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
            "        >>> b  # 3-D tensor\n",
            "        <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "        array([[[13, 14],\n",
            "                [15, 16],\n",
            "                [17, 18]],\n",
            "               [[19, 20],\n",
            "                [21, 22],\n",
            "                [23, 24]]], dtype=int32)>\n",
            "        >>> c = tf.matmul(a, b)\n",
            "        >>> c  # `a` * `b`\n",
            "        <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
            "        array([[[ 94, 100],\n",
            "                [229, 244]],\n",
            "               [[508, 532],\n",
            "                [697, 730]]], dtype=int32)>\n",
            "        \n",
            "        Since python >= 3.5 the @ operator is supported\n",
            "        (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\n",
            "        it simply calls the `tf.matmul()` function, so the following lines are\n",
            "        equivalent:\n",
            "        \n",
            "        >>> d = a @ b @ [[10], [11]]\n",
            "        >>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
            "        \n",
            "        Args:\n",
            "          a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n",
            "            `complex64`, `complex128` and rank > 1.\n",
            "          b: `tf.Tensor` with same type and rank as `a`.\n",
            "          transpose_a: If `True`, `a` is transposed before multiplication.\n",
            "          transpose_b: If `True`, `b` is transposed before multiplication.\n",
            "          adjoint_a: If `True`, `a` is conjugated and transposed before\n",
            "            multiplication.\n",
            "          adjoint_b: If `True`, `b` is conjugated and transposed before\n",
            "            multiplication.\n",
            "          a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n",
            "            **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "            that assume most values in `a` are zero.\n",
            "            See `tf.sparse.sparse_dense_matmul`\n",
            "            for some support for `tf.SparseTensor` multiplication.\n",
            "          b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n",
            "            **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n",
            "            that assume most values in `a` are zero.\n",
            "            See `tf.sparse.sparse_dense_matmul`\n",
            "            for some support for `tf.SparseTensor` multiplication.\n",
            "          name: Name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n",
            "          is the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
            "          transpose or adjoint attributes are `False`:\n",
            "        \n",
            "          `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n",
            "          for all indices `i`, `j`.\n",
            "        \n",
            "          Note: This is matrix product, not element-wise product.\n",
            "        \n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n",
            "            `adjoint_b` are both set to `True`.\n",
            "    \n",
            "    matrix_square_root(input, name=None)\n",
            "        Computes the matrix square root of one or more square matrices:\n",
            "        \n",
            "        matmul(sqrtm(A), sqrtm(A)) = A\n",
            "        \n",
            "        The input matrix should be invertible. If the input matrix is real, it should\n",
            "        have no eigenvalues which are real and negative (pairs of complex conjugate\n",
            "        eigenvalues are allowed).\n",
            "        \n",
            "        The matrix square root is computed by first reducing the matrix to\n",
            "        quasi-triangular form with the real Schur decomposition. The square root\n",
            "        of the quasi-triangular matrix is then computed directly. Details of\n",
            "        the algorithm can be found in: Nicholas J. Higham, \"Computing real\n",
            "        square roots of a real matrix\", Linear Algebra Appl., 1987.\n",
            "        \n",
            "        The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\n",
            "        form square matrices. The output is a tensor of the same shape as the input\n",
            "        containing the matrix square root for all input submatrices `[..., :, :]`.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.\n",
            "            Shape is `[..., M, M]`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    maximum(x, y, name=None)\n",
            "        Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n",
            "        \n",
            "        Example:\n",
            "        >>> x = tf.constant([0., 0., 0., 0.])\n",
            "        >>> y = tf.constant([-2., 0., 2., 5.])\n",
            "        >>> tf.math.maximum(x, y)\n",
            "        <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    meshgrid(*args, **kwargs)\n",
            "        Broadcasts parameters for evaluation on an N-D grid.\n",
            "        \n",
            "        Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`\n",
            "        of N-D coordinate arrays for evaluating expressions on an N-D grid.\n",
            "        \n",
            "        Notes:\n",
            "        \n",
            "        `meshgrid` supports cartesian ('xy') and matrix ('ij') indexing conventions.\n",
            "        When the `indexing` argument is set to 'xy' (the default), the broadcasting\n",
            "        instructions for the first two dimensions are swapped.\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        Calling `X, Y = meshgrid(x, y)` with the tensors\n",
            "        \n",
            "        ```python\n",
            "        x = [1, 2, 3]\n",
            "        y = [4, 5, 6]\n",
            "        X, Y = tf.meshgrid(x, y)\n",
            "        # X = [[1, 2, 3],\n",
            "        #      [1, 2, 3],\n",
            "        #      [1, 2, 3]]\n",
            "        # Y = [[4, 4, 4],\n",
            "        #      [5, 5, 5],\n",
            "        #      [6, 6, 6]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          *args: `Tensor`s with rank 1.\n",
            "          **kwargs:\n",
            "            - indexing: Either 'xy' or 'ij' (optional, default: 'xy').\n",
            "            - name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          outputs: A list of N `Tensor`s with rank N.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: When no keyword arguments (kwargs) are passed.\n",
            "          ValueError: When indexing keyword argument is not one of `xy` or `ij`.\n",
            "    \n",
            "    minimum(x, y, name=None)\n",
            "        Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n",
            "        \n",
            "        Example:\n",
            "        >>> x = tf.constant([0., 0., 0., 0.])\n",
            "        >>> y = tf.constant([-5., -2., 0., 3.])\n",
            "        >>> tf.math.minimum(x, y)\n",
            "        <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    multiply(x, y, name=None)\n",
            "        Returns an element-wise x * y.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant(([1, 2, 3, 4]))\n",
            "        >>> tf.math.multiply(x, x)\n",
            "        <tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>\n",
            "        \n",
            "        Since `tf.math.multiply` will convert its arguments to `Tensor`s, you can also\n",
            "        pass in non-`Tensor` arguments:\n",
            "        \n",
            "        >>> tf.math.multiply(7,6)\n",
            "        <tf.Tensor: shape=(), dtype=int32, numpy=42>\n",
            "        \n",
            "        If `x.shape` is not thes same as `y.shape`, they will be broadcast to a\n",
            "        compatible shape. (More about broadcasting\n",
            "        [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).)\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.ones([1, 2]);\n",
            "        >>> y = tf.ones([2, 1]);\n",
            "        >>> x * y  # Taking advantage of operator overriding\n",
            "        <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
            "        array([[1., 1.],\n",
            "             [1., 1.]], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A Tensor. Must be one of the following types: `bfloat16`,\n",
            "            `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`,\n",
            "            `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "        \n",
            "        A `Tensor`.  Has the same type as `x`.\n",
            "        \n",
            "        Raises:\n",
            "        \n",
            "         * InvalidArgumentError: When `x` and `y` have incomptatible shapes or types.\n",
            "    \n",
            "    negative = neg(x, name=None)\n",
            "        Computes numerical negative value element-wise.\n",
            "        \n",
            "        I.e., \\\\(y = -x\\\\).\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    no_gradient(op_type)\n",
            "        Specifies that ops of type `op_type` is not differentiable.\n",
            "        \n",
            "        This function should *not* be used for operations that have a\n",
            "        well-defined gradient that is not yet implemented.\n",
            "        \n",
            "        This function is only used when defining a new op type. It may be\n",
            "        used for ops such as `tf.size()` that are not differentiable.  For\n",
            "        example:\n",
            "        \n",
            "        ```python\n",
            "        tf.no_gradient(\"Size\")\n",
            "        ```\n",
            "        \n",
            "        The gradient computed for 'op_type' will then propagate zeros.\n",
            "        \n",
            "        For ops that have a well-defined gradient but are not yet implemented,\n",
            "        no declaration should be made, and an error *must* be thrown if\n",
            "        an attempt to request its gradient is made.\n",
            "        \n",
            "        Args:\n",
            "          op_type: The string type of an operation. This corresponds to the\n",
            "            `OpDef.name` field for the proto that defines the operation.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `op_type` is not a string.\n",
            "    \n",
            "    no_op(name=None)\n",
            "        Does nothing. Only useful as a placeholder for control edges.\n",
            "        \n",
            "        Args:\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The created Operation.\n",
            "    \n",
            "    nondifferentiable_batch_function = batch_function(num_batch_threads, max_batch_size, batch_timeout_micros, allowed_batch_sizes=None, max_enqueued_batches=10, autograph=True)\n",
            "        Batches the computation done by the decorated function.\n",
            "        \n",
            "        So, for example, in the following code\n",
            "        \n",
            "        ```python\n",
            "        @batch_function(1, 2, 3)\n",
            "        def layer(a):\n",
            "          return tf.matmul(a, a)\n",
            "        \n",
            "        b = layer(w)\n",
            "        ```\n",
            "        \n",
            "        if more than one session.run call is simultaneously trying to compute `b`\n",
            "        the values of `w` will be gathered, non-deterministically concatenated\n",
            "        along the first axis, and only one thread will run the computation. See the\n",
            "        documentation of the `Batch` op for more details.\n",
            "        \n",
            "        Assumes that all arguments of the decorated function are Tensors which will\n",
            "        be batched along their first dimension.\n",
            "        \n",
            "        SparseTensor is not supported. The return value of the decorated function\n",
            "        must be a Tensor or a list/tuple of Tensors.\n",
            "        \n",
            "        Args:\n",
            "          num_batch_threads: Number of scheduling threads for processing batches\n",
            "           of work. Determines the number of batches processed in parallel.\n",
            "          max_batch_size: Batch sizes will never be bigger than this.\n",
            "          batch_timeout_micros: Maximum number of microseconds to wait before\n",
            "           outputting an incomplete batch.\n",
            "          allowed_batch_sizes: Optional list of allowed batch sizes. If left empty,\n",
            "           does nothing. Otherwise, supplies a list of batch sizes, causing the op\n",
            "           to pad batches up to one of those sizes. The entries must increase\n",
            "           monotonically, and the final entry must equal max_batch_size.\n",
            "          max_enqueued_batches: The maximum depth of the batch queue. Defaults to 10.\n",
            "          autograph: Whether to use autograph to compile python and eager style code\n",
            "           for efficient graph-mode execution.\n",
            "        \n",
            "        Returns:\n",
            "          The decorated function will return the unbatched computation output Tensors.\n",
            "    \n",
            "    norm = norm_v2(tensor, ord='euclidean', axis=None, keepdims=None, name=None)\n",
            "        Computes the norm of vectors, matrices, and tensors.\n",
            "        \n",
            "        This function can compute several different vector norms (the 1-norm, the\n",
            "        Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\n",
            "        matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n",
            "        \n",
            "        Args:\n",
            "          tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n",
            "          ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n",
            "            `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n",
            "            p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n",
            "            `tensor` is a matrix and equivalent to 2-norm for vectors.\n",
            "            Some restrictions apply:\n",
            "              a) The Frobenius norm `'fro'` is not defined for vectors,\n",
            "              b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n",
            "                 `2`, `np.inf` are supported.\n",
            "            See the description of `axis` on how to compute norms for a batch of\n",
            "            vectors or matrices stored in a tensor.\n",
            "          axis: If `axis` is `None` (the default), the input is considered a vector\n",
            "            and a single vector norm is computed over the entire set of values in the\n",
            "            tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n",
            "            `norm(reshape(tensor, [-1]), ord=ord)`.\n",
            "            If `axis` is a Python integer, the input is considered a batch of vectors,\n",
            "            and `axis` determines the axis in `tensor` over which to compute vector\n",
            "            norms.\n",
            "            If `axis` is a 2-tuple of Python integers it is considered a batch of\n",
            "            matrices and `axis` determines the axes in `tensor` over which to compute\n",
            "            a matrix norm.\n",
            "            Negative indices are supported. Example: If you are passing a tensor that\n",
            "            can be either a matrix or a batch of matrices at runtime, pass\n",
            "            `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n",
            "            computed.\n",
            "          keepdims: If True, the axis indicated in `axis` are kept with size 1.\n",
            "            Otherwise, the dimensions in `axis` are removed from the output shape.\n",
            "          name: The name of the op.\n",
            "        \n",
            "        Returns:\n",
            "          output: A `Tensor` of the same type as tensor, containing the vector or\n",
            "            matrix norms. If `keepdims` is True then the rank of output is equal to\n",
            "            the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n",
            "            if `axis` is an integer, the rank of `output` is one less than the rank\n",
            "            of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n",
            "            than the rank of `tensor`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `ord` or `axis` is invalid.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Mostly equivalent to numpy.linalg.norm.\n",
            "        Not supported: ord <= 0, 2-norm for matrices, nuclear norm.\n",
            "        Other differences:\n",
            "          a) If axis is `None`, treats the flattened `tensor` as a vector\n",
            "           regardless of rank.\n",
            "          b) Explicitly supports 'euclidean' norm as the default, including for\n",
            "           higher order tensors.\n",
            "        @end_compatibility\n",
            "    \n",
            "    not_equal(x, y, name=None)\n",
            "        Returns the truth value of (x != y) element-wise.\n",
            "        \n",
            "        Performs a [broadcast](\n",
            "        https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the\n",
            "        arguments and then an element-wise inequality comparison, returning a Tensor\n",
            "        of boolean values.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([2, 4])\n",
            "        >>> y = tf.constant(2)\n",
            "        >>> tf.math.not_equal(x, y)\n",
            "        <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
            "        \n",
            "        >>> x = tf.constant([2, 4])\n",
            "        >>> y = tf.constant([2, 4])\n",
            "        >>> tf.math.not_equal(x, y)\n",
            "        <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  False])>\n",
            "        \n",
            "        Args:\n",
            "          x: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.\n",
            "          y: A `tf.Tensor` or `tf.SparseTensor` or `tf.IndexedSlices`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of type bool with the same size as that of x or y.\n",
            "        \n",
            "        Raises:\n",
            "          `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible\n",
            "    \n",
            "    numpy_function(func, inp, Tout, name=None)\n",
            "        Wraps a python function and uses it as a TensorFlow op.\n",
            "        \n",
            "        Given a python function `func` wrap this function as an operation in a\n",
            "        TensorFlow function. `func` must take numpy arrays as its arguments and\n",
            "        return numpy arrays as its outputs.\n",
            "        \n",
            "        The following example creates a TensorFlow graph with `np.sinh()` as an\n",
            "        operation in the graph:\n",
            "        \n",
            "        >>> def my_numpy_func(x):\n",
            "        ...   # x will be a numpy array with the contents of the input to the\n",
            "        ...   # tf.function\n",
            "        ...   return np.sinh(x)\n",
            "        >>> @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n",
            "        ... def tf_function(input):\n",
            "        ...   y = tf.numpy_function(my_numpy_func, [input], tf.float32)\n",
            "        ...   return y * y\n",
            "        >>> tf_function(tf.constant(1.))\n",
            "        <tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>\n",
            "        \n",
            "        Comparison to `tf.py_function`:\n",
            "        `tf.py_function` and `tf.numpy_function` are very similar, except that\n",
            "        `tf.numpy_function` takes numpy arrays, and not `tf.Tensor`s. If you want the\n",
            "        function to contain `tf.Tensors`, and have any TensorFlow operations executed\n",
            "        in the function be differentiable, please use `tf.py_function`.\n",
            "        \n",
            "        Note: The `tf.numpy_function` operation has the following known\n",
            "        limitations:\n",
            "        \n",
            "        * The body of the function (i.e. `func`) will not be serialized in a\n",
            "          `tf.SavedModel`. Therefore, you should not use this function if you need to\n",
            "          serialize your model and restore it in a different environment.\n",
            "        \n",
            "        * The operation must run in the same address space as the Python program\n",
            "          that calls `tf.numpy_function()`. If you are using distributed\n",
            "          TensorFlow, you must run a `tf.distribute.Server` in the same process as the\n",
            "          program that calls `tf.numpy_function`  you must pin the created\n",
            "          operation to a device in that server (e.g. using `with tf.device():`).\n",
            "        \n",
            "        * Since the function takes numpy arrays, you cannot take gradients\n",
            "          through a numpy_function. If you require something that is differentiable,\n",
            "          please consider using tf.py_function.\n",
            "        \n",
            "        * The resulting function is assumed stateful and will never be optimized.\n",
            "        \n",
            "        Args:\n",
            "          func: A Python function, which accepts `numpy.ndarray` objects as arguments\n",
            "            and returns a list of `numpy.ndarray` objects (or a single\n",
            "            `numpy.ndarray`). This function must accept as many arguments as there are\n",
            "            tensors in `inp`, and these argument types will match the corresponding\n",
            "            `tf.Tensor` objects in `inp`. The returns `numpy.ndarray`s must match the\n",
            "            number and types defined `Tout`.\n",
            "            Important Note: Input and output `numpy.ndarray`s of `func` are not\n",
            "              guaranteed to be copies. In some cases their underlying memory will be\n",
            "              shared with the corresponding TensorFlow tensors. In-place modification\n",
            "              or storing `func` input or return values in python datastructures\n",
            "              without explicit (np.)copy can have non-deterministic consequences.\n",
            "          inp: A list of `tf.Tensor` objects.\n",
            "          Tout: A list or tuple of tensorflow data types or a single tensorflow data\n",
            "            type if there is only one, indicating what `func` returns.\n",
            "          name: (Optional) A name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          Single or list of `tf.Tensor` which `func` computes.\n",
            "    \n",
            "    one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)\n",
            "        Returns a one-hot tensor.\n",
            "        \n",
            "        The locations represented by indices in `indices` take value `on_value`,\n",
            "        while all other locations take value `off_value`.\n",
            "        \n",
            "        `on_value` and `off_value` must have matching data types. If `dtype` is also\n",
            "        provided, they must be the same data type as specified by `dtype`.\n",
            "        \n",
            "        If `on_value` is not provided, it will default to the value `1` with type\n",
            "        `dtype`\n",
            "        \n",
            "        If `off_value` is not provided, it will default to the value `0` with type\n",
            "        `dtype`\n",
            "        \n",
            "        If the input `indices` is rank `N`, the output will have rank `N+1`. The\n",
            "        new axis is created at dimension `axis` (default: the new axis is appended\n",
            "        at the end).\n",
            "        \n",
            "        If `indices` is a scalar the output shape will be a vector of length `depth`\n",
            "        \n",
            "        If `indices` is a vector of length `features`, the output shape will be:\n",
            "        \n",
            "        ```\n",
            "          features x depth if axis == -1\n",
            "          depth x features if axis == 0\n",
            "        ```\n",
            "        \n",
            "        If `indices` is a matrix (batch) with shape `[batch, features]`, the output\n",
            "        shape will be:\n",
            "        \n",
            "        ```\n",
            "          batch x features x depth if axis == -1\n",
            "          batch x depth x features if axis == 1\n",
            "          depth x batch x features if axis == 0\n",
            "        ```\n",
            "        \n",
            "        If `indices` is a RaggedTensor, the 'axis' argument must be positive and refer\n",
            "        to a non-ragged axis. The output will be equivalent to applying 'one_hot' on\n",
            "        the values of the RaggedTensor, and creating a new RaggedTensor from the\n",
            "        result.\n",
            "        \n",
            "        If `dtype` is not provided, it will attempt to assume the data type of\n",
            "        `on_value` or `off_value`, if one or both are passed in. If none of\n",
            "        `on_value`, `off_value`, or `dtype` are provided, `dtype` will default to the\n",
            "        value `tf.float32`.\n",
            "        \n",
            "        Note: If a non-numeric data type output is desired (`tf.string`, `tf.bool`,\n",
            "        etc.), both `on_value` and `off_value` _must_ be provided to `one_hot`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        indices = [0, 1, 2]\n",
            "        depth = 3\n",
            "        tf.one_hot(indices, depth)  # output: [3 x 3]\n",
            "        # [[1., 0., 0.],\n",
            "        #  [0., 1., 0.],\n",
            "        #  [0., 0., 1.]]\n",
            "        \n",
            "        indices = [0, 2, -1, 1]\n",
            "        depth = 3\n",
            "        tf.one_hot(indices, depth,\n",
            "                   on_value=5.0, off_value=0.0,\n",
            "                   axis=-1)  # output: [4 x 3]\n",
            "        # [[5.0, 0.0, 0.0],  # one_hot(0)\n",
            "        #  [0.0, 0.0, 5.0],  # one_hot(2)\n",
            "        #  [0.0, 0.0, 0.0],  # one_hot(-1)\n",
            "        #  [0.0, 5.0, 0.0]]  # one_hot(1)\n",
            "        \n",
            "        indices = [[0, 2], [1, -1]]\n",
            "        depth = 3\n",
            "        tf.one_hot(indices, depth,\n",
            "                   on_value=1.0, off_value=0.0,\n",
            "                   axis=-1)  # output: [2 x 2 x 3]\n",
            "        # [[[1.0, 0.0, 0.0],   # one_hot(0)\n",
            "        #   [0.0, 0.0, 1.0]],  # one_hot(2)\n",
            "        #  [[0.0, 1.0, 0.0],   # one_hot(1)\n",
            "        #   [0.0, 0.0, 0.0]]]  # one_hot(-1)\n",
            "        \n",
            "        indices = tf.ragged.constant([[0, 1], [2]])\n",
            "        depth = 3\n",
            "        tf.one_hot(indices, depth)  # output: [2 x None x 3]\n",
            "        # [[[1., 0., 0.],\n",
            "        #   [0., 1., 0.]],\n",
            "        #  [[0., 0., 1.]]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          indices: A `Tensor` of indices.\n",
            "          depth: A scalar defining the depth of the one hot dimension.\n",
            "          on_value: A scalar defining the value to fill in output when `indices[j]\n",
            "            = i`. (default: 1)\n",
            "          off_value: A scalar defining the value to fill in output when `indices[j]\n",
            "            != i`. (default: 0)\n",
            "          axis: The axis to fill (default: -1, a new inner-most axis).\n",
            "          dtype: The data type of the output tensor.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          output: The one-hot tensor.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If dtype of either `on_value` or `off_value` don't match `dtype`\n",
            "          TypeError: If dtype of `on_value` and `off_value` don't match one another\n",
            "    \n",
            "    ones(shape, dtype=tf.float32, name=None)\n",
            "        Creates a tensor with all elements set to one (1).\n",
            "        \n",
            "        See also `tf.ones_like`.\n",
            "        \n",
            "        This operation returns a tensor of type `dtype` with shape `shape` and\n",
            "        all elements set to one.\n",
            "        \n",
            "        >>> tf.ones([3, 4], tf.int32)\n",
            "        <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
            "        array([[1, 1, 1, 1],\n",
            "               [1, 1, 1, 1],\n",
            "               [1, 1, 1, 1]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          shape: A `list` of integers, a `tuple` of integers, or\n",
            "            a 1-D `Tensor` of type `int32`.\n",
            "          dtype: Optional DType of an element in the resulting `Tensor`. Default is\n",
            "            `tf.float32`.\n",
            "          name: Optional string. A name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with all elements set to one (1).\n",
            "    \n",
            "    ones_like = ones_like_v2(input, dtype=None, name=None)\n",
            "        Creates a tensor of all ones that has the same shape as the input.\n",
            "        \n",
            "        See also `tf.ones`.\n",
            "        \n",
            "        Given a single tensor (`tensor`), this operation returns a tensor of the\n",
            "        same type and shape as `tensor` with all elements set to 1. Optionally,\n",
            "        you can use `dtype` to specify a new type for the returned tensor.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
            "        >>> tf.ones_like(tensor)\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "          array([[1, 1, 1],\n",
            "                 [1, 1, 1]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,\n",
            "            `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,\n",
            "            `complex64`, `complex128`, `bool` or `string`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with all elements set to one.\n",
            "    \n",
            "    pad = pad_v2(tensor, paddings, mode='CONSTANT', constant_values=0, name=None)\n",
            "        Pads a tensor.\n",
            "        \n",
            "        This operation pads a `tensor` according to the `paddings` you specify.\n",
            "        `paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of\n",
            "        `tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how\n",
            "        many values to add before the contents of `tensor` in that dimension, and\n",
            "        `paddings[D, 1]` indicates how many values to add after the contents of\n",
            "        `tensor` in that dimension. If `mode` is \"REFLECT\" then both `paddings[D, 0]`\n",
            "        and `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If\n",
            "        `mode` is \"SYMMETRIC\" then both `paddings[D, 0]` and `paddings[D, 1]` must be\n",
            "        no greater than `tensor.dim_size(D)`.\n",
            "        \n",
            "        The padded size of each dimension D of the output is:\n",
            "        \n",
            "        `paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        t = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
            "        paddings = tf.constant([[1, 1,], [2, 2]])\n",
            "        # 'constant_values' is 0.\n",
            "        # rank of 't' is 2.\n",
            "        tf.pad(t, paddings, \"CONSTANT\")  # [[0, 0, 0, 0, 0, 0, 0],\n",
            "                                         #  [0, 0, 1, 2, 3, 0, 0],\n",
            "                                         #  [0, 0, 4, 5, 6, 0, 0],\n",
            "                                         #  [0, 0, 0, 0, 0, 0, 0]]\n",
            "        \n",
            "        tf.pad(t, paddings, \"REFLECT\")  # [[6, 5, 4, 5, 6, 5, 4],\n",
            "                                        #  [3, 2, 1, 2, 3, 2, 1],\n",
            "                                        #  [6, 5, 4, 5, 6, 5, 4],\n",
            "                                        #  [3, 2, 1, 2, 3, 2, 1]]\n",
            "        \n",
            "        tf.pad(t, paddings, \"SYMMETRIC\")  # [[2, 1, 1, 2, 3, 3, 2],\n",
            "                                          #  [2, 1, 1, 2, 3, 3, 2],\n",
            "                                          #  [5, 4, 4, 5, 6, 6, 5],\n",
            "                                          #  [5, 4, 4, 5, 6, 6, 5]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`.\n",
            "          paddings: A `Tensor` of type `int32`.\n",
            "          mode: One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n",
            "          constant_values: In \"CONSTANT\" mode, the scalar pad value to use. Must be\n",
            "            same type as `tensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".\n",
            "    \n",
            "    parallel_stack(values, name='parallel_stack')\n",
            "        Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor in parallel.\n",
            "        \n",
            "        Requires that the shape of inputs be known at graph construction time.\n",
            "        \n",
            "        Packs the list of tensors in `values` into a tensor with rank one higher than\n",
            "        each tensor in `values`, by packing them along the first dimension.\n",
            "        Given a list of length `N` of tensors of shape `(A, B, C)`; the `output`\n",
            "        tensor will have the shape `(N, A, B, C)`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([1, 4])\n",
            "        y = tf.constant([2, 5])\n",
            "        z = tf.constant([3, 6])\n",
            "        tf.parallel_stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]]\n",
            "        ```\n",
            "        \n",
            "        The difference between `stack` and `parallel_stack` is that `stack` requires\n",
            "        all the inputs be computed before the operation will begin but doesn't require\n",
            "        that the input shapes be known during graph construction.\n",
            "        \n",
            "        `parallel_stack` will copy pieces of the input into the output as they become\n",
            "        available, in some situations this can provide a performance benefit.\n",
            "        \n",
            "        Unlike `stack`, `parallel_stack` does NOT support backpropagation.\n",
            "        \n",
            "        This is the opposite of unstack.  The numpy equivalent is\n",
            "        \n",
            "            tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])\n",
            "        \n",
            "        Args:\n",
            "          values: A list of `Tensor` objects with the same shape and type.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          output: A stacked `Tensor` with the same type as `values`.\n",
            "    \n",
            "    pow(x, y, name=None)\n",
            "        Computes the power of one value to another.\n",
            "        \n",
            "        Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
            "        corresponding elements in `x` and `y`. For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([[2, 2], [3, 3]])\n",
            "        y = tf.constant([[8, 16], [2, 3]])\n",
            "        tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "            `complex64`, or `complex128`.\n",
            "          y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
            "            `complex64`, or `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`.\n",
            "    \n",
            "    print = print_v2(*inputs, **kwargs)\n",
            "        Print the specified inputs.\n",
            "        \n",
            "        A TensorFlow operator that prints the specified inputs to a desired\n",
            "        output stream or logging level. The inputs may be dense or sparse Tensors,\n",
            "        primitive python objects, data structures that contain tensors, and printable\n",
            "        Python objects. Printed tensors will recursively show the first and last\n",
            "        elements of each dimension to summarize.\n",
            "        \n",
            "        @compatibility(python2)\n",
            "        In python 2.7, make sure to import the following:\n",
            "        `from __future__ import print_function`\n",
            "        @end_compatibility\n",
            "        \n",
            "        Example:\n",
            "          Single-input usage:\n",
            "        \n",
            "          ```python\n",
            "          tensor = tf.range(10)\n",
            "          tf.print(tensor, output_stream=sys.stderr)\n",
            "          ```\n",
            "        \n",
            "          (This prints \"[0 1 2 ... 7 8 9]\" to sys.stderr)\n",
            "        \n",
            "          Multi-input usage:\n",
            "        \n",
            "          ```python\n",
            "          tensor = tf.range(10)\n",
            "          tf.print(\"tensors:\", tensor, {2: tensor * 2}, output_stream=sys.stdout)\n",
            "          ```\n",
            "        \n",
            "          (This prints \"tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}\" to\n",
            "          sys.stdout)\n",
            "        \n",
            "          Changing the input separator:\n",
            "          ```python\n",
            "          tensor_a = tf.range(2)\n",
            "          tensor_b = tensor_a * 2\n",
            "          tf.print(tensor_a, tensor_b, output_stream=sys.stderr, sep=',')\n",
            "          ```\n",
            "        \n",
            "          (This prints \"[0 1],[0 2]\" to sys.stderr)\n",
            "        \n",
            "          Usage in a `tf.function`:\n",
            "        \n",
            "          ```python\n",
            "          @tf.function\n",
            "          def f():\n",
            "              tensor = tf.range(10)\n",
            "              tf.print(tensor, output_stream=sys.stderr)\n",
            "              return tensor\n",
            "        \n",
            "          range_tensor = f()\n",
            "          ```\n",
            "        \n",
            "          (This prints \"[0 1 2 ... 7 8 9]\" to sys.stderr)\n",
            "        \n",
            "        @compatibility(TF 1.x Graphs and Sessions)\n",
            "        In graphs manually created outside of `tf.function`, this method returns\n",
            "        the created TF operator that prints the data. To make sure the\n",
            "        operator runs, users need to pass the produced op to\n",
            "        `tf.compat.v1.Session`'s run method, or to use the op as a control\n",
            "        dependency for executed ops by specifying\n",
            "        `with tf.compat.v1.control_dependencies([print_op])`.\n",
            "        @end_compatibility\n",
            "        \n",
            "          Compatibility usage in TF 1.x graphs:\n",
            "        \n",
            "          ```python\n",
            "          sess = tf.compat.v1.Session()\n",
            "          with sess.as_default():\n",
            "              tensor = tf.range(10)\n",
            "              print_op = tf.print(\"tensors:\", tensor, {2: tensor * 2},\n",
            "                                  output_stream=sys.stdout)\n",
            "              with tf.control_dependencies([print_op]):\n",
            "                tripled_tensor = tensor * 3\n",
            "              sess.run(tripled_tensor)\n",
            "          ```\n",
            "        \n",
            "          (This prints \"tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}\" to\n",
            "          sys.stdout)\n",
            "        \n",
            "        Note: In Jupyter notebooks and colabs, `tf.print` prints to the notebook\n",
            "          cell outputs. It will not write to the notebook kernel's console logs.\n",
            "        \n",
            "        Args:\n",
            "          *inputs: Positional arguments that are the inputs to print. Inputs in the\n",
            "            printed output will be separated by spaces. Inputs may be python\n",
            "            primitives, tensors, data structures such as dicts and lists that may\n",
            "            contain tensors (with the data structures possibly nested in arbitrary\n",
            "            ways), and printable python objects.\n",
            "          output_stream: The output stream, logging level, or file to print to.\n",
            "            Defaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info,\n",
            "            tf.compat.v1.logging.warning, tf.compat.v1.logging.error,\n",
            "            absl.logging.info, absl.logging.warning and absl.logging.error are also\n",
            "            supported. To print to a file, pass a string started with \"file://\"\n",
            "            followed by the file path, e.g., \"file:///tmp/foo.out\".\n",
            "          summarize: The first and last `summarize` elements within each dimension are\n",
            "            recursively printed per Tensor. If None, then the first 3 and last 3\n",
            "            elements of each dimension are printed for each tensor. If set to -1, it\n",
            "            will print all elements of every tensor.\n",
            "          sep: The string to use to separate the inputs. Defaults to \" \".\n",
            "          end: End character that is appended at the end the printed string.\n",
            "            Defaults to the newline character.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          None when executing eagerly. During graph tracing this returns\n",
            "          a TF operator that prints the specified inputs in the specified output\n",
            "          stream or logging level. This operator will be automatically executed\n",
            "          except inside of `tf.compat.v1` graphs and sessions.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If an unsupported output stream is specified.\n",
            "    \n",
            "    py_function = eager_py_func(func, inp, Tout, name=None)\n",
            "        Wraps a python function into a TensorFlow op that executes it eagerly.\n",
            "        \n",
            "        This function allows expressing computations in a TensorFlow graph as\n",
            "        Python functions. In particular, it wraps a Python function `func`\n",
            "        in a once-differentiable TensorFlow operation that executes it with eager\n",
            "        execution enabled. As a consequence, `tf.py_function` makes it\n",
            "        possible to express control flow using Python constructs (`if`, `while`,\n",
            "        `for`, etc.), instead of TensorFlow control flow constructs (`tf.cond`,\n",
            "        `tf.while_loop`). For example, you might use `tf.py_function` to\n",
            "        implement the log huber function:\n",
            "        \n",
            "        ```python\n",
            "        def log_huber(x, m):\n",
            "          if tf.abs(x) <= m:\n",
            "            return x**2\n",
            "          else:\n",
            "            return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))\n",
            "        \n",
            "        x = tf.compat.v1.placeholder(tf.float32)\n",
            "        m = tf.compat.v1.placeholder(tf.float32)\n",
            "        \n",
            "        y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)\n",
            "        dy_dx = tf.gradients(y, x)[0]\n",
            "        \n",
            "        with tf.compat.v1.Session() as sess:\n",
            "          # The session executes `log_huber` eagerly. Given the feed values below,\n",
            "          # it will take the first branch, so `y` evaluates to 1.0 and\n",
            "          # `dy_dx` evaluates to 2.0.\n",
            "          y, dy_dx = sess.run([y, dy_dx], feed_dict={x: 1.0, m: 2.0})\n",
            "        ```\n",
            "        \n",
            "        You can also use `tf.py_function` to debug your models at runtime\n",
            "        using Python tools, i.e., you can isolate portions of your code that\n",
            "        you want to debug, wrap them in Python functions and insert `pdb` tracepoints\n",
            "        or print statements as desired, and wrap those functions in\n",
            "        `tf.py_function`.\n",
            "        \n",
            "        For more information on eager execution, see the\n",
            "        [Eager guide](https://tensorflow.org/guide/eager).\n",
            "        \n",
            "        `tf.py_function` is similar in spirit to `tf.compat.v1.py_func`, but unlike\n",
            "        the latter, the former lets you use TensorFlow operations in the wrapped\n",
            "        Python function. In particular, while `tf.compat.v1.py_func` only runs on CPUs\n",
            "        and\n",
            "        wraps functions that take NumPy arrays as inputs and return NumPy arrays as\n",
            "        outputs, `tf.py_function` can be placed on GPUs and wraps functions\n",
            "        that take Tensors as inputs, execute TensorFlow operations in their bodies,\n",
            "        and return Tensors as outputs.\n",
            "        \n",
            "        Like `tf.compat.v1.py_func`, `tf.py_function` has the following limitations\n",
            "        with respect to serialization and distribution:\n",
            "        \n",
            "        * The body of the function (i.e. `func`) will not be serialized in a\n",
            "          `GraphDef`. Therefore, you should not use this function if you need to\n",
            "          serialize your model and restore it in a different environment.\n",
            "        \n",
            "        * The operation must run in the same address space as the Python program\n",
            "          that calls `tf.py_function()`. If you are using distributed\n",
            "          TensorFlow, you must run a `tf.distribute.Server` in the same process as the\n",
            "          program that calls `tf.py_function()` and you must pin the created\n",
            "          operation to a device in that server (e.g. using `with tf.device():`).\n",
            "        \n",
            "        \n",
            "        Args:\n",
            "          func: A Python function which accepts a list of `Tensor` objects having\n",
            "            element types that match the corresponding `tf.Tensor` objects in `inp`\n",
            "            and returns a list of `Tensor` objects (or a single `Tensor`, or `None`)\n",
            "            having element types that match the corresponding values in `Tout`.\n",
            "          inp: A list of `Tensor` objects.\n",
            "          Tout: A list or tuple of tensorflow data types or a single tensorflow data\n",
            "            type if there is only one, indicating what `func` returns; an empty list\n",
            "            if no value is returned (i.e., if the return value is `None`).\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A list of `Tensor` or a single `Tensor` which `func` computes; an empty list\n",
            "          if `func` returns None.\n",
            "    \n",
            "    range(start, limit=None, delta=1, dtype=None, name='range')\n",
            "        Creates a sequence of numbers.\n",
            "        \n",
            "        Creates a sequence of numbers that begins at `start` and extends by\n",
            "        increments of `delta` up to but not including `limit`.\n",
            "        \n",
            "        The dtype of the resulting tensor is inferred from the inputs unless\n",
            "        it is provided explicitly.\n",
            "        \n",
            "        Like the Python builtin `range`, `start` defaults to 0, so that\n",
            "        `range(n) = range(0, n)`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> start = 3\n",
            "        >>> limit = 18\n",
            "        >>> delta = 3\n",
            "        >>> tf.range(start, limit, delta)\n",
            "        <tf.Tensor: shape=(5,), dtype=int32,\n",
            "        numpy=array([ 3,  6,  9, 12, 15], dtype=int32)>\n",
            "        \n",
            "        >>> start = 3\n",
            "        >>> limit = 1\n",
            "        >>> delta = -0.5\n",
            "        >>> tf.range(start, limit, delta)\n",
            "        <tf.Tensor: shape=(4,), dtype=float32,\n",
            "        numpy=array([3. , 2.5, 2. , 1.5], dtype=float32)>\n",
            "        \n",
            "        >>> limit = 5\n",
            "        >>> tf.range(limit)\n",
            "        <tf.Tensor: shape=(5,), dtype=int32,\n",
            "        numpy=array([0, 1, 2, 3, 4], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          start: A 0-D `Tensor` (scalar). Acts as first entry in the range if `limit`\n",
            "            is not None; otherwise, acts as range limit and first entry defaults to 0.\n",
            "          limit: A 0-D `Tensor` (scalar). Upper limit of sequence, exclusive. If None,\n",
            "            defaults to the value of `start` while the first entry of the range\n",
            "            defaults to 0.\n",
            "          delta: A 0-D `Tensor` (scalar). Number that increments `start`. Defaults to\n",
            "            1.\n",
            "          dtype: The type of the elements of the resulting tensor.\n",
            "          name: A name for the operation. Defaults to \"range\".\n",
            "        \n",
            "        Returns:\n",
            "          An 1-D `Tensor` of type `dtype`.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.arange\n",
            "        @end_compatibility\n",
            "    \n",
            "    rank(input, name=None)\n",
            "        Returns the rank of a tensor.\n",
            "        \n",
            "        Returns a 0-D `int32` `Tensor` representing the rank of `input`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        # shape of tensor 't' is [2, 2, 3]\n",
            "        t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
            "        tf.rank(t)  # 3\n",
            "        ```\n",
            "        \n",
            "        **Note**: The rank of a tensor is not the same as the rank of a matrix. The\n",
            "        rank of a tensor is the number of indices required to uniquely select each\n",
            "        element of the tensor. Rank is also known as \"order\", \"degree\", or \"ndims.\"\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor` or `SparseTensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `int32`.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.ndim\n",
            "        @end_compatibility\n",
            "    \n",
            "    realdiv = real_div(x, y, name=None)\n",
            "        Returns x / y element-wise for real types.\n",
            "        \n",
            "        If `x` and `y` are reals, this will return the floating-point division.\n",
            "        \n",
            "        *NOTE*: `Div` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    recompute_grad(f)\n",
            "        An eager-compatible version of recompute_grad.\n",
            "        \n",
            "        For f(*args, **kwargs), this supports gradients with respect to args or\n",
            "        kwargs, but kwargs are currently only supported in eager-mode.\n",
            "        Note that for keras layer and model objects, this is handled automatically.\n",
            "        \n",
            "        Warning: If `f` was originally a tf.keras Model or Layer object, `g` will not\n",
            "        be able to access the member variables of that object, because `g` returns\n",
            "        through the wrapper function `inner`.  When recomputing gradients through\n",
            "        objects that inherit from keras, we suggest keeping a reference to the\n",
            "        underlying object around for the purpose of accessing these variables.\n",
            "        \n",
            "        Args:\n",
            "          f: function `f(*x)` that returns a `Tensor` or sequence of `Tensor` outputs.\n",
            "        \n",
            "        Returns:\n",
            "         A function `g` that wraps `f`, but which recomputes `f` on the backwards\n",
            "         pass of a gradient call.\n",
            "    \n",
            "    reduce_all(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the \"logical and\" of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([[True,  True], [False, False]])\n",
            "        tf.reduce_all(x)  # False\n",
            "        tf.reduce_all(x, 0)  # [False, False]\n",
            "        tf.reduce_all(x, 1)  # [True, False]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The boolean tensor to reduce.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.all\n",
            "        @end_compatibility\n",
            "    \n",
            "    reduce_any(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the \"logical or\" of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([[True,  True], [False, False]])\n",
            "        tf.reduce_any(x)  # True\n",
            "        tf.reduce_any(x, 0)  # [True, True]\n",
            "        tf.reduce_any(x, 1)  # [True, False]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The boolean tensor to reduce.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.any\n",
            "        @end_compatibility\n",
            "    \n",
            "    reduce_logsumexp(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes log(sum(exp(elements across dimensions of a tensor))).\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` has no entries, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        This function is more numerically stable than log(sum(exp(input))). It avoids\n",
            "        overflows caused by taking the exp of large inputs and underflows caused by\n",
            "        taking the log of small inputs.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([[0., 0., 0.], [0., 0., 0.]])\n",
            "        tf.reduce_logsumexp(x)  # log(6)\n",
            "        tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]\n",
            "        tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]\n",
            "        tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]\n",
            "        tf.reduce_logsumexp(x, [0, 1])  # log(6)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "    \n",
            "    reduce_max(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the maximum of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        Usage example:\n",
            "        \n",
            "        >>> x = tf.constant([5, 1, 2, 4])\n",
            "        >>> print(tf.reduce_max(x))\n",
            "        tf.Tensor(5, shape=(), dtype=int32)\n",
            "        >>> x = tf.constant([-5, -1, -2, -4])\n",
            "        >>> print(tf.reduce_max(x))\n",
            "        tf.Tensor(-1, shape=(), dtype=int32)\n",
            "        >>> x = tf.constant([4, float('nan')])\n",
            "        >>> print(tf.reduce_max(x))\n",
            "        tf.Tensor(4.0, shape=(), dtype=float32)\n",
            "        >>> x = tf.constant([float('nan'), float('nan')])\n",
            "        >>> print(tf.reduce_max(x))\n",
            "        tf.Tensor(-inf, shape=(), dtype=float32)\n",
            "        >>> x = tf.constant([float('-inf'), float('inf')])\n",
            "        >>> print(tf.reduce_max(x))\n",
            "        tf.Tensor(inf, shape=(), dtype=float32)\n",
            "        \n",
            "        See the numpy docs for `np.amax` and `np.nanmax` behavior.\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have real numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "    \n",
            "    reduce_mean(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the mean of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis` by computing the\n",
            "        mean of elements across the dimensions in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions are retained\n",
            "        with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a tensor with a single\n",
            "        element is returned.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([[1., 1.], [2., 2.]])\n",
            "        >>> tf.reduce_mean(x)\n",
            "        <tf.Tensor: shape=(), dtype=float32, numpy=1.5>\n",
            "        >>> tf.reduce_mean(x, 0)\n",
            "        <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>\n",
            "        >>> tf.reduce_mean(x, 1)\n",
            "        <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.mean\n",
            "        \n",
            "        Please note that `np.mean` has a `dtype` parameter that could be used to\n",
            "        specify the output type. By default this is `dtype=float64`. On the other\n",
            "        hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,\n",
            "        for example:\n",
            "        \n",
            "        >>> x = tf.constant([1, 0, 1, 0])\n",
            "        >>> tf.reduce_mean(x)\n",
            "        <tf.Tensor: shape=(), dtype=int32, numpy=0>\n",
            "        >>> y = tf.constant([1., 0., 1., 0.])\n",
            "        >>> tf.reduce_mean(y)\n",
            "        <tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n",
            "        \n",
            "        @end_compatibility\n",
            "    \n",
            "    reduce_min(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the minimum of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have real numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "        \n",
            "        For example:\n",
            "          >>> a = tf.constant([[1, 2], [3, 4]])\n",
            "          >>> tf.reduce_min(a)\n",
            "          <tf.Tensor: shape=(), dtype=int32, numpy=1>\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.min\n",
            "        @end_compatibility\n",
            "    \n",
            "    reduce_prod(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the product of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.prod\n",
            "        @end_compatibility\n",
            "    \n",
            "    reduce_sum(input_tensor, axis=None, keepdims=False, name=None)\n",
            "        Computes the sum of elements across dimensions of a tensor.\n",
            "        \n",
            "        Reduces `input_tensor` along the dimensions given in `axis`.\n",
            "        Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
            "        entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
            "        are retained with length 1.\n",
            "        \n",
            "        If `axis` is None, all dimensions are reduced, and a\n",
            "        tensor with a single element is returned.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
            "        tf.reduce_sum(x)  # 6\n",
            "        tf.reduce_sum(x, 0)  # [2, 2, 2]\n",
            "        tf.reduce_sum(x, 1)  # [3, 3]\n",
            "        tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]\n",
            "        tf.reduce_sum(x, [0, 1])  # 6\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_tensor: The tensor to reduce. Should have numeric type.\n",
            "          axis: The dimensions to reduce. If `None` (the default), reduces all\n",
            "            dimensions. Must be in the range `[-rank(input_tensor),\n",
            "            rank(input_tensor))`.\n",
            "          keepdims: If true, retains reduced dimensions with length 1.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The reduced tensor, of the same dtype as the input_tensor.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\n",
            "        int64 while tensorflow returns the same dtype as the input.\n",
            "        @end_compatibility\n",
            "    \n",
            "    register_tensor_conversion_function(base_type, conversion_func, priority=100)\n",
            "        Registers a function for converting objects of `base_type` to `Tensor`.\n",
            "        \n",
            "        The conversion function must have the following signature:\n",
            "        \n",
            "        ```python\n",
            "            def conversion_func(value, dtype=None, name=None, as_ref=False):\n",
            "              # ...\n",
            "        ```\n",
            "        \n",
            "        It must return a `Tensor` with the given `dtype` if specified. If the\n",
            "        conversion function creates a new `Tensor`, it should use the given\n",
            "        `name` if specified. All exceptions will be propagated to the caller.\n",
            "        \n",
            "        The conversion function may return `NotImplemented` for some\n",
            "        inputs. In this case, the conversion process will continue to try\n",
            "        subsequent conversion functions.\n",
            "        \n",
            "        If `as_ref` is true, the function must return a `Tensor` reference,\n",
            "        such as a `Variable`.\n",
            "        \n",
            "        NOTE: The conversion functions will execute in order of priority,\n",
            "        followed by order of registration. To ensure that a conversion function\n",
            "        `F` runs before another conversion function `G`, ensure that `F` is\n",
            "        registered with a smaller priority than `G`.\n",
            "        \n",
            "        Args:\n",
            "          base_type: The base type or tuple of base types for all objects that\n",
            "            `conversion_func` accepts.\n",
            "          conversion_func: A function that converts instances of `base_type` to\n",
            "            `Tensor`.\n",
            "          priority: Optional integer that indicates the priority for applying this\n",
            "            conversion function. Conversion functions with smaller priority values run\n",
            "            earlier than conversion functions with larger priority values. Defaults to\n",
            "            100.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If the arguments do not have the appropriate type.\n",
            "    \n",
            "    repeat(input, repeats, axis=None, name=None)\n",
            "        Repeat elements of `input`.\n",
            "        \n",
            "        See also `tf.concat`, `tf.stack`, `tf.tile`.\n",
            "        \n",
            "        Args:\n",
            "          input: An `N`-dimensional Tensor.\n",
            "          repeats: An 1-D `int` Tensor. The number of repetitions for each element.\n",
            "            repeats is broadcasted to fit the shape of the given axis. `len(repeats)`\n",
            "            must equal `input.shape[axis]` if axis is not None.\n",
            "          axis: An int. The axis along which to repeat values. By default (axis=None),\n",
            "            use the flattened input array, and return a flat output array.\n",
            "          name: A name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A Tensor which has the same shape as `input`, except along the given axis.\n",
            "            If axis is None then the output array is flattened to match the flattened\n",
            "            input array.\n",
            "        \n",
            "        Example usage:\n",
            "        \n",
            "        >>> repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)\n",
            "        <tf.Tensor: shape=(5,), dtype=string,\n",
            "        numpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>\n",
            "        \n",
            "        >>> repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=0)\n",
            "        <tf.Tensor: shape=(5, 2), dtype=int32, numpy=\n",
            "        array([[1, 2],\n",
            "               [1, 2],\n",
            "               [3, 4],\n",
            "               [3, 4],\n",
            "               [3, 4]], dtype=int32)>\n",
            "        \n",
            "        >>> repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=1)\n",
            "        <tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
            "        array([[1, 1, 2, 2, 2],\n",
            "               [3, 3, 4, 4, 4]], dtype=int32)>\n",
            "        \n",
            "        >>> repeat(3, repeats=4)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 3, 3, 3], dtype=int32)>\n",
            "        \n",
            "        >>> repeat([[1,2], [3,4]], repeats=2)\n",
            "        <tf.Tensor: shape=(8,), dtype=int32,\n",
            "        numpy=array([1, 1, 2, 2, 3, 3, 4, 4], dtype=int32)>\n",
            "    \n",
            "    required_space_to_batch_paddings(input_shape, block_shape, base_paddings=None, name=None)\n",
            "        Calculate padding required to make block_shape divide input_shape.\n",
            "        \n",
            "        This function can be used to calculate a suitable paddings argument for use\n",
            "        with space_to_batch_nd and batch_to_space_nd.\n",
            "        \n",
            "        Args:\n",
            "          input_shape: int32 Tensor of shape [N].\n",
            "          block_shape: int32 Tensor of shape [N].\n",
            "          base_paddings: Optional int32 Tensor of shape [N, 2].  Specifies the minimum\n",
            "            amount of padding to use.  All elements must be >= 0.  If not specified,\n",
            "            defaults to 0.\n",
            "          name: string.  Optional name prefix.\n",
            "        \n",
            "        Returns:\n",
            "          (paddings, crops), where:\n",
            "        \n",
            "          `paddings` and `crops` are int32 Tensors of rank 2 and shape [N, 2]\n",
            "          satisfying:\n",
            "        \n",
            "              paddings[i, 0] = base_paddings[i, 0].\n",
            "              0 <= paddings[i, 1] - base_paddings[i, 1] < block_shape[i]\n",
            "              (input_shape[i] + paddings[i, 0] + paddings[i, 1]) % block_shape[i] == 0\n",
            "        \n",
            "              crops[i, 0] = 0\n",
            "              crops[i, 1] = paddings[i, 1] - base_paddings[i, 1]\n",
            "        \n",
            "        Raises: ValueError if called with incompatible shapes.\n",
            "    \n",
            "    reshape(tensor, shape, name=None)\n",
            "        Reshapes a tensor.\n",
            "        \n",
            "        Given `tensor`, this operation returns a new `tf.Tensor` that has the same\n",
            "        values as `tensor` in the same order, except with a new shape given by\n",
            "        `shape`.\n",
            "        \n",
            "        >>> t1 = [[1, 2, 3],\n",
            "        ...       [4, 5, 6]]\n",
            "        >>> print(tf.shape(t1).numpy())\n",
            "        [2 3]\n",
            "        >>> t2 = tf.reshape(t1, [6])\n",
            "        >>> t2\n",
            "        <tf.Tensor: shape=(6,), dtype=int32,\n",
            "          numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
            "        >>> tf.reshape(t2, [3, 2])\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "          array([[1, 2],\n",
            "                 [3, 4],\n",
            "                 [5, 6]], dtype=int32)>\n",
            "        \n",
            "        The `tf.reshape` does not change the order of or the total number of elements\n",
            "        in the tensor, and so it can reuse the underlying data buffer. This makes it\n",
            "        a fast operation independent of how big of a tensor it is operating on.\n",
            "        \n",
            "        >>> tf.reshape([1, 2, 3], [2, 2])\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        InvalidArgumentError: Input to reshape is a tensor with 3 values, but the\n",
            "        requested shape has 4\n",
            "        \n",
            "        To instead reorder the data to rearrange the dimensions of a tensor, see\n",
            "        `tf.transpose`.\n",
            "        \n",
            "        >>> t = [[1, 2, 3],\n",
            "        ...      [4, 5, 6]]\n",
            "        >>> tf.reshape(t, [3, 2]).numpy()\n",
            "        array([[1, 2],\n",
            "               [3, 4],\n",
            "               [5, 6]], dtype=int32)\n",
            "        >>> tf.transpose(t, perm=[1, 0]).numpy()\n",
            "        array([[1, 4],\n",
            "               [2, 5],\n",
            "               [3, 6]], dtype=int32)\n",
            "        \n",
            "        If one component of `shape` is the special value -1, the size of that\n",
            "        dimension is computed so that the total size remains constant.  In particular,\n",
            "        a `shape` of `[-1]` flattens into 1-D.  At most one component of `shape` can\n",
            "        be -1.\n",
            "        \n",
            "        >>> t = [[1, 2, 3],\n",
            "        ...      [4, 5, 6]]\n",
            "        >>> tf.reshape(t, [-1])\n",
            "        <tf.Tensor: shape=(6,), dtype=int32,\n",
            "          numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
            "        >>> tf.reshape(t, [3, -1])\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "          array([[1, 2],\n",
            "                 [3, 4],\n",
            "                 [5, 6]], dtype=int32)>\n",
            "        >>> tf.reshape(t, [-1, 2])\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "          array([[1, 2],\n",
            "                 [3, 4],\n",
            "                 [5, 6]], dtype=int32)>\n",
            "        \n",
            "        `tf.reshape(t, [])` reshapes a tensor `t` with one element to a scalar.\n",
            "        \n",
            "        >>> tf.reshape([7], []).numpy()\n",
            "        7\n",
            "        \n",
            "        More examples:\n",
            "        \n",
            "        >>> t = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "        >>> print(tf.shape(t).numpy())\n",
            "        [9]\n",
            "        >>> tf.reshape(t, [3, 3])\n",
            "        <tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
            "          array([[1, 2, 3],\n",
            "                 [4, 5, 6],\n",
            "                 [7, 8, 9]], dtype=int32)>\n",
            "        \n",
            "        >>> t = [[[1, 1], [2, 2]],\n",
            "        ...      [[3, 3], [4, 4]]]\n",
            "        >>> print(tf.shape(t).numpy())\n",
            "        [2 2 2]\n",
            "        >>> tf.reshape(t, [2, 4])\n",
            "        <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
            "          array([[1, 1, 2, 2],\n",
            "                 [3, 3, 4, 4]], dtype=int32)>\n",
            "        \n",
            "        >>> t = [[[1, 1, 1],\n",
            "        ...       [2, 2, 2]],\n",
            "        ...      [[3, 3, 3],\n",
            "        ...       [4, 4, 4]],\n",
            "        ...      [[5, 5, 5],\n",
            "        ...       [6, 6, 6]]]\n",
            "        >>> print(tf.shape(t).numpy())\n",
            "        [3 2 3]\n",
            "        >>> # Pass '[-1]' to flatten 't'.\n",
            "        >>> tf.reshape(t, [-1])\n",
            "        <tf.Tensor: shape=(18,), dtype=int32,\n",
            "          numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],\n",
            "          dtype=int32)>\n",
            "        >>> # -- Using -1 to infer the shape --\n",
            "        >>> # Here -1 is inferred to be 9:\n",
            "        >>> tf.reshape(t, [2, -1])\n",
            "        <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
            "          array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
            "                 [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
            "        >>> # -1 is inferred to be 2:\n",
            "        >>> tf.reshape(t, [-1, 9])\n",
            "        <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
            "          array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
            "                 [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
            "        >>> # -1 is inferred to be 3:\n",
            "        >>> tf.reshape(t, [ 2, -1, 3])\n",
            "        <tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=\n",
            "          array([[[1, 1, 1],\n",
            "                  [2, 2, 2],\n",
            "                  [3, 3, 3]],\n",
            "                 [[4, 4, 4],\n",
            "                  [5, 5, 5],\n",
            "                  [6, 6, 6]]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`.\n",
            "          shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Defines the shape of the output tensor.\n",
            "          name: Optional string. A name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "    \n",
            "    reverse = reverse_v2(tensor, axis, name=None)\n",
            "        Reverses specific dimensions of a tensor.\n",
            "        \n",
            "        NOTE `tf.reverse` has now changed behavior in preparation for 1.0.\n",
            "        `tf.reverse_v2` is currently an alias that will be deprecated before TF 1.0.\n",
            "        \n",
            "        Given a `tensor`, and a `int32` tensor `axis` representing the set of\n",
            "        dimensions of `tensor` to reverse. This operation reverses each dimension\n",
            "        `i` for which there exists `j` s.t. `axis[j] == i`.\n",
            "        \n",
            "        `tensor` can have up to 8 dimensions. The number of dimensions specified\n",
            "        in `axis` may be 0 or more entries. If an index is specified more than\n",
            "        once, a InvalidArgument error is raised.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```\n",
            "        # tensor 't' is [[[[ 0,  1,  2,  3],\n",
            "        #                  [ 4,  5,  6,  7],\n",
            "        #                  [ 8,  9, 10, 11]],\n",
            "        #                 [[12, 13, 14, 15],\n",
            "        #                  [16, 17, 18, 19],\n",
            "        #                  [20, 21, 22, 23]]]]\n",
            "        # tensor 't' shape is [1, 2, 3, 4]\n",
            "        \n",
            "        # 'dims' is [3] or 'dims' is [-1]\n",
            "        reverse(t, dims) ==> [[[[ 3,  2,  1,  0],\n",
            "                                [ 7,  6,  5,  4],\n",
            "                                [ 11, 10, 9, 8]],\n",
            "                               [[15, 14, 13, 12],\n",
            "                                [19, 18, 17, 16],\n",
            "                                [23, 22, 21, 20]]]]\n",
            "        \n",
            "        # 'dims' is '[1]' (or 'dims' is '[-3]')\n",
            "        reverse(t, dims) ==> [[[[12, 13, 14, 15],\n",
            "                                [16, 17, 18, 19],\n",
            "                                [20, 21, 22, 23]\n",
            "                               [[ 0,  1,  2,  3],\n",
            "                                [ 4,  5,  6,  7],\n",
            "                                [ 8,  9, 10, 11]]]]\n",
            "        \n",
            "        # 'dims' is '[2]' (or 'dims' is '[-2]')\n",
            "        reverse(t, dims) ==> [[[[8, 9, 10, 11],\n",
            "                                [4, 5, 6, 7],\n",
            "                                [0, 1, 2, 3]]\n",
            "                               [[20, 21, 22, 23],\n",
            "                                [16, 17, 18, 19],\n",
            "                                [12, 13, 14, 15]]]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`. Must be one of the following types: `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `bool`, `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`, `string`.\n",
            "            Up to 8-D.\n",
            "          axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            1-D. The indices of the dimensions to reverse. Must be in the range\n",
            "            `[-rank(tensor), rank(tensor))`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "    \n",
            "    reverse_sequence = reverse_sequence_v2(input, seq_lengths, seq_axis=None, batch_axis=None, name=None)\n",
            "        Reverses variable length slices. (deprecated arguments) (deprecated arguments)\n",
            "        \n",
            "        Warning: SOME ARGUMENTS ARE DEPRECATED: `(seq_dim)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        seq_dim is deprecated, use seq_axis instead\n",
            "        \n",
            "        Warning: SOME ARGUMENTS ARE DEPRECATED: `(batch_dim)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        batch_dim is deprecated, use batch_axis instead\n",
            "        \n",
            "        This op first slices `input` along the dimension `batch_axis`, and for\n",
            "        each slice `i`, reverses the first `seq_lengths[i]` elements along the\n",
            "        dimension `seq_axis`.\n",
            "        \n",
            "        The elements of `seq_lengths` must obey `seq_lengths[i] <=\n",
            "        input.dims[seq_dim]`, and `seq_lengths` must be a vector of length\n",
            "        `input.dims[batch_dim]`.\n",
            "        \n",
            "        The output slice `i` along dimension `batch_axis` is then given by\n",
            "        input slice `i`, with the first `seq_lengths[i]` slices along\n",
            "        dimension `seq_axis` reversed.\n",
            "        \n",
            "        Example usage:\n",
            "        \n",
            "        >>> seq_lengths = [7, 2, 3, 5]\n",
            "        >>> input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],\n",
            "        ...          [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]\n",
            "        >>> output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)\n",
            "        >>> output\n",
            "        <tf.Tensor: shape=(4, 8), dtype=int32, numpy=\n",
            "        array([[0, 0, 5, 4, 3, 2, 1, 0],\n",
            "               [2, 1, 0, 0, 0, 0, 0, 0],\n",
            "               [3, 2, 1, 4, 0, 0, 0, 0],\n",
            "               [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          `input`: A `Tensor`. The input to reverse.\n",
            "          `seq_lengths`: A `Tensor`. Must be one of the following types: `int32`,\n",
            "            `int64`. 1-D with length `input.dims(batch_dim)` and `max(seq_lengths) <=\n",
            "            input.dims(seq_dim)`\n",
            "          `seq_axis`: An `int`. The dimension which is partially reversed.\n",
            "          `batch_axis`: An optional `int`. Defaults to `0`. The dimension along which\n",
            "            reversal is performed.\n",
            "          `name`: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A Tensor. Has the same type as input.\n",
            "    \n",
            "    roll(input, shift, axis, name=None)\n",
            "        Rolls the elements of a tensor along an axis.\n",
            "        \n",
            "        The elements are shifted positively (towards larger indices) by the offset of\n",
            "        `shift` along the dimension of `axis`. Negative `shift` values will shift\n",
            "        elements in the opposite direction. Elements that roll passed the last position\n",
            "        will wrap around to the first and vice versa. Multiple shifts along multiple\n",
            "        axes may be specified.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```\n",
            "        # 't' is [0, 1, 2, 3, 4]\n",
            "        roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\n",
            "        \n",
            "        # shifting along multiple dimensions\n",
            "        # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n",
            "        roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\n",
            "        \n",
            "        # shifting along the same axis multiple times\n",
            "        # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n",
            "        roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          shift: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Dimension must be 0-D or 1-D. `shift[i]` specifies the number of places by which\n",
            "            elements are shifted positively (towards larger indices) along the dimension\n",
            "            specified by `axis[i]`. Negative shifts will roll the elements in the opposite\n",
            "            direction.\n",
            "          axis: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Dimension must be 0-D or 1-D. `axis[i]` specifies the dimension that the shift\n",
            "            `shift[i]` should occur. If the same axis is referenced more than once, the\n",
            "            total shift for that axis will be the sum of all the shifts that belong to that\n",
            "            axis.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    round(x, name=None)\n",
            "        Rounds the values of a tensor to the nearest integer, element-wise.\n",
            "        \n",
            "        Rounds half to even.  Also known as bankers rounding. If you want to round\n",
            "        according to the current system rounding mode use tf::cint.\n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])\n",
            "        tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, or `int64`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of same shape and type as `x`.\n",
            "    \n",
            "    saturate_cast(value, dtype, name=None)\n",
            "        Performs a safe saturating cast of `value` to `dtype`.\n",
            "        \n",
            "        This function casts the input to `dtype` without applying any scaling.  If\n",
            "        there is a danger that values would over or underflow in the cast, this op\n",
            "        applies the appropriate clamping before the cast.\n",
            "        \n",
            "        Args:\n",
            "          value: A `Tensor`.\n",
            "          dtype: The desired output `DType`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          `value` safely cast to `dtype`.\n",
            "    \n",
            "    scalar_mul = scalar_mul_v2(scalar, x, name=None)\n",
            "        Multiplies a scalar times a `Tensor` or `IndexedSlices` object.\n",
            "        \n",
            "        Intended for use in gradient code which might deal with `IndexedSlices`\n",
            "        objects, which are easy to multiply by a scalar but more expensive to\n",
            "        multiply with arbitrary tensors.\n",
            "        \n",
            "        Args:\n",
            "          scalar: A 0-D scalar `Tensor`. Must have known shape.\n",
            "          x: A `Tensor` or `IndexedSlices` to be scaled.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          `scalar * x` of the same type (`Tensor` or `IndexedSlices`) as `x`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: if scalar is not a 0-D `scalar`.\n",
            "    \n",
            "    scan = scan_v2(fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, reverse=False, name=None)\n",
            "        scan on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n",
            "        \n",
            "        Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(back_prop=False)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "        Instead of:\n",
            "        results = tf.scan(fn, elems, back_prop=False)\n",
            "        Use:\n",
            "        results = tf.nest.map_structure(tf.stop_gradient, tf.scan(fn, elems))\n",
            "        \n",
            "        The simplest version of `scan` repeatedly applies the callable `fn` to a\n",
            "        sequence of elements from first to last. The elements are made of the tensors\n",
            "        unpacked from `elems` on dimension 0. The callable fn takes two tensors as\n",
            "        arguments. The first argument is the accumulated value computed from the\n",
            "        preceding invocation of fn, and the second is the value at the current\n",
            "        position of `elems`. If `initializer` is None, `elems` must contain at least\n",
            "        one element, and its first element is used as the initializer.\n",
            "        \n",
            "        Suppose that `elems` is unpacked into `values`, a list of tensors. The shape\n",
            "        of the result tensor is `[len(values)] + fn(initializer, values[0]).shape`.\n",
            "        If reverse=True, it's fn(initializer, values[-1]).shape.\n",
            "        \n",
            "        This method also allows multi-arity `elems` and accumulator.  If `elems`\n",
            "        is a (possibly nested) list or tuple of tensors, then each of these tensors\n",
            "        must have a matching first (unpack) dimension.  The second argument of\n",
            "        `fn` must match the structure of `elems`.\n",
            "        \n",
            "        If no `initializer` is provided, the output structure and dtypes of `fn`\n",
            "        are assumed to be the same as its input; and in this case, the first\n",
            "        argument of `fn` must match the structure of `elems`.\n",
            "        \n",
            "        If an `initializer` is provided, then the output of `fn` must have the same\n",
            "        structure as `initializer`; and the first argument of `fn` must match\n",
            "        this structure.\n",
            "        \n",
            "        For example, if `elems` is `(t1, [t2, t3])` and `initializer` is\n",
            "        `[i1, i2]` then an appropriate signature for `fn` in `python2` is:\n",
            "        `fn = lambda (acc_p1, acc_p2), (t1, [t2, t3]):` and `fn` must return a list,\n",
            "        `[acc_n1, acc_n2]`.  An alternative correct signature for `fn`, and the\n",
            "         one that works in `python3`, is:\n",
            "        `fn = lambda a, t:`, where `a` and `t` correspond to the input tuples.\n",
            "        \n",
            "        Args:\n",
            "          fn: The callable to be performed.  It accepts two arguments.  The first will\n",
            "            have the same structure as `initializer` if one is provided, otherwise it\n",
            "            will have the same structure as `elems`.  The second will have the same\n",
            "            (possibly nested) structure as `elems`.  Its output must have the same\n",
            "            structure as `initializer` if one is provided, otherwise it must have the\n",
            "            same structure as `elems`.\n",
            "          elems: A tensor or (possibly nested) sequence of tensors, each of which will\n",
            "            be unpacked along their first dimension.  The nested sequence of the\n",
            "            resulting slices will be the first argument to `fn`.\n",
            "          initializer: (optional) A tensor or (possibly nested) sequence of tensors,\n",
            "            initial value for the accumulator, and the expected output type of `fn`.\n",
            "          parallel_iterations: (optional) The number of iterations allowed to run in\n",
            "            parallel.\n",
            "          back_prop: (optional) Deprecated. False disables support for back\n",
            "            propagation. Prefer using `tf.stop_gradient` instead.\n",
            "          swap_memory: (optional) True enables GPU-CPU memory swapping.\n",
            "          infer_shape: (optional) False disables tests for consistent output shapes.\n",
            "          reverse: (optional) True scans the tensor last to first (instead of first to\n",
            "            last).\n",
            "          name: (optional) Name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor or (possibly nested) sequence of tensors.  Each tensor packs the\n",
            "          results of applying `fn` to tensors unpacked from `elems` along the first\n",
            "          dimension, and the previous accumulator value(s), from first to last (or\n",
            "          last to first, if `reverse=True`).\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `fn` is not callable or the structure of the output of\n",
            "            `fn` and `initializer` do not match.\n",
            "          ValueError: if the lengths of the output of `fn` and `initializer`\n",
            "            do not match.\n",
            "        \n",
            "        Examples:\n",
            "          ```python\n",
            "          elems = np.array([1, 2, 3, 4, 5, 6])\n",
            "          sum = scan(lambda a, x: a + x, elems)\n",
            "          # sum == [1, 3, 6, 10, 15, 21]\n",
            "          sum = scan(lambda a, x: a + x, elems, reverse=True)\n",
            "          # sum == [21, 20, 18, 15, 11, 6]\n",
            "          ```\n",
            "        \n",
            "          ```python\n",
            "          elems = np.array([1, 2, 3, 4, 5, 6])\n",
            "          initializer = np.array(0)\n",
            "          sum_one = scan(\n",
            "              lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)\n",
            "          # sum_one == [1, 2, 3, 4, 5, 6]\n",
            "          ```\n",
            "        \n",
            "          ```python\n",
            "          elems = np.array([1, 0, 0, 0, 0, 0])\n",
            "          initializer = (np.array(0), np.array(1))\n",
            "          fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)\n",
            "          # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])\n",
            "          ```\n",
            "    \n",
            "    scatter_nd(indices, updates, shape, name=None)\n",
            "        Scatter `updates` into a new tensor according to `indices`.\n",
            "        \n",
            "        Creates a new tensor by applying sparse `updates` to individual values or\n",
            "        slices within a tensor (initially zero for numeric, empty for string) of\n",
            "        the given `shape` according to indices.  This operator is the inverse of the\n",
            "        `tf.gather_nd` operator which extracts values or slices from a given tensor.\n",
            "        \n",
            "        This operation is similar to tensor_scatter_add, except that the tensor is\n",
            "        zero-initialized. Calling `tf.scatter_nd(indices, values, shape)` is identical\n",
            "        to `tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)`\n",
            "        \n",
            "        If `indices` contains duplicates, then their updates are accumulated (summed).\n",
            "        \n",
            "        **WARNING**: The order in which updates are applied is nondeterministic, so the\n",
            "        output will be nondeterministic if `indices` contains duplicates -- because\n",
            "        of some numerical approximation issues, numbers summed in different order\n",
            "        may yield different results.\n",
            "        \n",
            "        `indices` is an integer tensor containing indices into a new tensor of shape\n",
            "        `shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n",
            "        \n",
            "            indices.shape[-1] <= shape.rank\n",
            "        \n",
            "        The last dimension of `indices` corresponds to indices into elements\n",
            "        (if `indices.shape[-1] = shape.rank`) or slices\n",
            "        (if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n",
            "        `shape`.  `updates` is a tensor with shape\n",
            "        \n",
            "            indices.shape[:-1] + shape[indices.shape[-1]:]\n",
            "        \n",
            "        The simplest form of scatter is to insert individual elements in a tensor by\n",
            "        index. For example, say we want to insert 4 scattered elements in a rank-1\n",
            "        tensor with 8 elements.\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd1.png\" alt>\n",
            "        </div>\n",
            "        \n",
            "        In Python, this scatter operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[4], [3], [1], [7]])\n",
            "            updates = tf.constant([9, 10, 11, 12])\n",
            "            shape = tf.constant([8])\n",
            "            scatter = tf.scatter_nd(indices, updates, shape)\n",
            "            print(scatter)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [0, 11, 0, 10, 9, 0, 0, 12]\n",
            "        \n",
            "        We can also, insert entire slices of a higher rank tensor all at once. For\n",
            "        example, if we wanted to insert two slices in the first dimension of a\n",
            "        rank-3 tensor with two matrices of new values.\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd2.png\" alt>\n",
            "        </div>\n",
            "        \n",
            "        In Python, this scatter operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[0], [2]])\n",
            "            updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "                                   [[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]]])\n",
            "            shape = tf.constant([4, 4, 4])\n",
            "            scatter = tf.scatter_nd(indices, updates, shape)\n",
            "            print(scatter)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "             [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
            "             [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "             [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, the index is ignored.\n",
            "        \n",
            "        Args:\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Index tensor.\n",
            "          updates: A `Tensor`. Updates to scatter into output.\n",
            "          shape: A `Tensor`. Must have the same type as `indices`.\n",
            "            1-D. The shape of the resulting tensor.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `updates`.\n",
            "    \n",
            "    searchsorted(sorted_sequence, values, side='left', out_type=tf.int32, name=None)\n",
            "        Searches input tensor for values on the innermost dimension.\n",
            "        \n",
            "        A 2-D example:\n",
            "        \n",
            "        ```\n",
            "          sorted_sequence = [[0, 3, 9, 9, 10],\n",
            "                             [1, 2, 3, 4, 5]]\n",
            "          values = [[2, 4, 9],\n",
            "                    [0, 2, 6]]\n",
            "        \n",
            "          result = searchsorted(sorted_sequence, values, side=\"left\")\n",
            "        \n",
            "          result == [[1, 2, 2],\n",
            "                     [0, 1, 5]]\n",
            "        \n",
            "          result = searchsorted(sorted_sequence, values, side=\"right\")\n",
            "        \n",
            "          result == [[1, 2, 4],\n",
            "                     [0, 2, 5]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          sorted_sequence: N-D `Tensor` containing a sorted sequence.\n",
            "          values: N-D `Tensor` containing the search values.\n",
            "          side: 'left' or 'right'; 'left' corresponds to lower_bound and 'right' to\n",
            "            upper_bound.\n",
            "          out_type: The output type (`int32` or `int64`).  Default is `tf.int32`.\n",
            "          name: Optional name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          An N-D `Tensor` the size of values containing the result of applying either\n",
            "          lower_bound or upper_bound (depending on side) to each value.  The result\n",
            "          is not a global index to the entire `Tensor`, but the index in the last\n",
            "          dimension.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If the last dimension of `sorted_sequence >= 2^31-1` elements.\n",
            "                      If the total size of values exceeds `2^31 - 1` elements.\n",
            "                      If the first `N-1` dimensions of the two tensors don't match.\n",
            "    \n",
            "    sequence_mask(lengths, maxlen=None, dtype=tf.bool, name=None)\n",
            "        Returns a mask tensor representing the first N positions of each cell.\n",
            "        \n",
            "        If `lengths` has shape `[d_1, d_2, ..., d_n]` the resulting tensor `mask` has\n",
            "        dtype `dtype` and shape `[d_1, d_2, ..., d_n, maxlen]`, with\n",
            "        \n",
            "        ```\n",
            "        mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])\n",
            "        ```\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        ```python\n",
            "        tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
            "                                        #  [True, True, True, False, False],\n",
            "                                        #  [True, True, False, False, False]]\n",
            "        \n",
            "        tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],\n",
            "                                          #   [True, True, True]],\n",
            "                                          #  [[True, True, False],\n",
            "                                          #   [False, False, False]]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          lengths: integer tensor, all its values <= maxlen.\n",
            "          maxlen: scalar integer tensor, size of last dimension of returned tensor.\n",
            "            Default is the maximum value in `lengths`.\n",
            "          dtype: output type of the resulting tensor.\n",
            "          name: name of the op.\n",
            "        \n",
            "        Returns:\n",
            "          A mask tensor of shape `lengths.shape + (maxlen,)`, cast to specified dtype.\n",
            "        Raises:\n",
            "          ValueError: if `maxlen` is not a scalar.\n",
            "    \n",
            "    shape = shape_v2(input, out_type=tf.int32, name=None)\n",
            "        Returns the shape of a tensor.\n",
            "        \n",
            "        See also `tf.size`.\n",
            "        \n",
            "        This operation returns a 1-D integer tensor representing the shape of `input`.\n",
            "        This represents the minimal set of known information at definition time.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
            "        >>> tf.shape(t)\n",
            "        <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)>\n",
            "        >>> tf.shape(t).numpy()\n",
            "        array([2, 2, 3], dtype=int32)\n",
            "        \n",
            "        Note: When using symbolic tensors, such as when using the Keras functional\n",
            "        API, tf.shape() will return the shape of the symbolic tensor.\n",
            "        \n",
            "        >>> a = tf.keras.layers.Input((None, 10))\n",
            "        >>> tf.shape(a)\n",
            "        <tf.Tensor ... shape=(3,) dtype=int32>\n",
            "        \n",
            "        In these cases, using `tf.Tensor.shape` will return more informative results.\n",
            "        \n",
            "        >>> a.shape\n",
            "        TensorShape([None, None, 10])\n",
            "        \n",
            "        `tf.shape` and `Tensor.shape` should be identical in eager mode.  Within\n",
            "        `tf.function` or within a `compat.v1` context, not all dimensions may be\n",
            "        known until execution time.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor` or `SparseTensor`.\n",
            "          out_type: (Optional) The specified output type of the operation (`int32` or\n",
            "            `int64`). Defaults to `tf.int32`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `out_type`.\n",
            "    \n",
            "    shape_n(input, out_type=tf.int32, name=None)\n",
            "        Returns shape of tensors.\n",
            "        \n",
            "        Args:\n",
            "          input: A list of at least 1 `Tensor` object with the same type.\n",
            "          out_type: The specified output type of the operation (`int32` or `int64`).\n",
            "            Defaults to `tf.int32`(optional).\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A list with the same length as `input` of `Tensor` objects with\n",
            "            type `out_type`.\n",
            "    \n",
            "    sigmoid(x, name=None)\n",
            "        Computes sigmoid of `x` element-wise.\n",
            "        \n",
            "        Formula for calculating sigmoid(x): `y = 1 / (1 + exp(-x))`.\n",
            "        \n",
            "        For x \\in (-inf, inf) => sigmoid(x) \\in (0, 1)\n",
            "        \n",
            "        Example Usage:\n",
            "        \n",
            "        If a positive number is large, then its sigmoid will approach to 1 since the\n",
            "        formula will be `y = <large_num> / (1 + <large_num>)`\n",
            "        \n",
            "        >>> x = tf.constant([0.0, 1.0, 50.0, 100.0])\n",
            "        >>> tf.math.sigmoid(x)\n",
            "        <tf.Tensor: shape=(4,), dtype=float32,\n",
            "        numpy=array([0.5      , 0.7310586, 1.       , 1.       ], dtype=float32)>\n",
            "        \n",
            "        If a negative number is large, its sigmoid will approach to 0 since the\n",
            "        formula will be `y = 1 / (1 + <large_num>)`\n",
            "        \n",
            "        >>> x = tf.constant([-100.0, -50.0, -1.0, 0.0])\n",
            "        >>> tf.math.sigmoid(x)\n",
            "        <tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
            "        array([0.0000000e+00, 1.9287499e-22, 2.6894143e-01, 0.5],\n",
            "              dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A Tensor with type `float16`, `float32`, `float64`, `complex64`, or\n",
            "            `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A Tensor with the same type as `x`.\n",
            "        \n",
            "        Usage Example:\n",
            "        \n",
            "        >>> x = tf.constant([-128.0, 0.0, 128.0], dtype=tf.float32)\n",
            "        >>> tf.sigmoid(x)\n",
            "        <tf.Tensor: shape=(3,), dtype=float32,\n",
            "        numpy=array([0. , 0.5, 1. ], dtype=float32)>\n",
            "        \n",
            "        @compatibility(scipy)\n",
            "        Equivalent to scipy.special.expit\n",
            "        @end_compatibility\n",
            "    \n",
            "    sign(x, name=None)\n",
            "        Returns an element-wise indication of the sign of a number.\n",
            "        \n",
            "        y = sign(x) = -1 if x < 0; 0 if x == 0; 1 if x > 0.\n",
            "        \n",
            "        For complex numbers, y = sign(x) = x / |x| if x != 0, otherwise y = 0.\n",
            "        \n",
            "        Example usage:\n",
            "        \n",
            "        >>> tf.math.sign([0., 2., -3.])\n",
            "        <tf.Tensor: ... numpy=array([ 0.,  1., -1.], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "         x: A Tensor. Must be one of the following types: bfloat16, half, float32,\n",
            "            float64, int32, int64, complex64, complex128.\n",
            "         name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "         A Tensor. Has the same type as x.\n",
            "        \n",
            "         If x is a SparseTensor, returns SparseTensor(x.indices,\n",
            "           tf.math.sign(x.values, ...), x.dense_shape).\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.sign(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    sin(x, name=None)\n",
            "        Computes sine of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes sine of every\n",
            "          element in the tensor. Input range is `(-inf, inf)` and\n",
            "          output range is `[-1,1]`.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10, float(\"inf\")])\n",
            "          tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    sinh(x, name=None)\n",
            "        Computes hyperbolic sine of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes hyperbolic sine of every\n",
            "          element in the tensor. Input range is `[-inf,inf]` and output range\n",
            "          is `[-inf,inf]`.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 2, 10, float(\"inf\")])\n",
            "          tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    size = size_v2(input, out_type=tf.int32, name=None)\n",
            "        Returns the size of a tensor.\n",
            "        \n",
            "        See also `tf.shape`.\n",
            "        \n",
            "        Returns a 0-D `Tensor` representing the number of elements in `input`\n",
            "        of type `out_type`. Defaults to tf.int32.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n",
            "        >>> tf.size(t)\n",
            "        <tf.Tensor: shape=(), dtype=int32, numpy=12>\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor` or `SparseTensor`.\n",
            "          name: A name for the operation (optional).\n",
            "          out_type: (Optional) The specified non-quantized numeric output type of the\n",
            "            operation. Defaults to `tf.int32`.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `out_type`. Defaults to `tf.int32`.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.size()\n",
            "        @end_compatibility\n",
            "    \n",
            "    slice(input_, begin, size, name=None)\n",
            "        Extracts a slice from a tensor.\n",
            "        \n",
            "        This operation extracts a slice of size `size` from a tensor `input_` starting\n",
            "        at the location specified by `begin`. The slice `size` is represented as a\n",
            "        tensor shape, where `size[i]` is the number of elements of the 'i'th dimension\n",
            "        of `input_` that you want to slice. The starting location (`begin`) for the\n",
            "        slice is represented as an offset in each dimension of `input_`. In other\n",
            "        words, `begin[i]` is the offset into the i'th dimension of `input_` that you\n",
            "        want to slice from.\n",
            "        \n",
            "        Note that `tf.Tensor.__getitem__` is typically a more pythonic way to\n",
            "        perform slices, as it allows you to write `foo[3:7, :-2]` instead of\n",
            "        `tf.slice(foo, [3, 0], [4, foo.get_shape()[1]-2])`.\n",
            "        \n",
            "        `begin` is zero-based; `size` is one-based. If `size[i]` is -1,\n",
            "        all remaining elements in dimension i are included in the\n",
            "        slice. In other words, this is equivalent to setting:\n",
            "        \n",
            "        `size[i] = input_.dim_size(i) - begin[i]`\n",
            "        \n",
            "        This operation requires that:\n",
            "        \n",
            "        `0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n]`\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
            "                         [[3, 3, 3], [4, 4, 4]],\n",
            "                         [[5, 5, 5], [6, 6, 6]]])\n",
            "        tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]\n",
            "        tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],\n",
            "                                           #   [4, 4, 4]]]\n",
            "        tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],\n",
            "                                           #  [[5, 5, 5]]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_: A `Tensor`.\n",
            "          begin: An `int32` or `int64` `Tensor`.\n",
            "          size: An `int32` or `int64` `Tensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` the same type as `input_`.\n",
            "    \n",
            "    sort(values, axis=-1, direction='ASCENDING', name=None)\n",
            "        Sorts a tensor.\n",
            "        \n",
            "        Usage:\n",
            "        \n",
            "        ```python\n",
            "        import tensorflow as tf\n",
            "        a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n",
            "        b = tf.sort(a,axis=-1,direction='ASCENDING',name=None)\n",
            "        c = tf.keras.backend.eval(b)\n",
            "        # Here, c = [  1.     2.8   10.    26.9   62.3  166.32]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          values: 1-D or higher numeric `Tensor`.\n",
            "          axis: The axis along which to sort. The default is -1, which sorts the last\n",
            "            axis.\n",
            "          direction: The direction in which to sort the values (`'ASCENDING'` or\n",
            "            `'DESCENDING'`).\n",
            "          name: Optional name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with the same dtype and shape as `values`, with the elements\n",
            "              sorted along the given `axis`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If axis is not a constant scalar, or the direction is invalid.\n",
            "    \n",
            "    space_to_batch = space_to_batch_v2(input, block_shape, paddings, name=None)\n",
            "        SpaceToBatch for N-D tensors of type T.\n",
            "        \n",
            "        This operation divides \"spatial\" dimensions `[1, ..., M]` of the input into a\n",
            "        grid of blocks of shape `block_shape`, and interleaves these blocks with the\n",
            "        \"batch\" dimension (0) such that in the output, the spatial dimensions\n",
            "        `[1, ..., M]` correspond to the position within the grid, and the batch\n",
            "        dimension combines both the position within a spatial block and the original\n",
            "        batch position.  Prior to division into blocks, the spatial dimensions of the\n",
            "        input are optionally zero padded according to `paddings`.  See below for a\n",
            "        precise description.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "            N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,\n",
            "            where spatial_shape has `M` dimensions.\n",
            "          block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            1-D with shape `[M]`, all values must be >= 1.\n",
            "          paddings: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            2-D with shape `[M, 2]`, all values must be >= 0.\n",
            "              `paddings[i] = [pad_start, pad_end]` specifies the padding for input dimension\n",
            "              `i + 1`, which corresponds to spatial dimension `i`.  It is required that\n",
            "              `block_shape[i]` divides `input_shape[i + 1] + pad_start + pad_end`.\n",
            "        \n",
            "            This operation is equivalent to the following steps:\n",
            "        \n",
            "            1. Zero-pad the start and end of dimensions `[1, ..., M]` of the\n",
            "               input according to `paddings` to produce `padded` of shape `padded_shape`.\n",
            "        \n",
            "            2. Reshape `padded` to `reshaped_padded` of shape:\n",
            "        \n",
            "                 [batch] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                   block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1],\n",
            "                  block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            3. Permute dimensions of `reshaped_padded` to produce\n",
            "               `permuted_reshaped_padded` of shape:\n",
            "        \n",
            "                 block_shape +\n",
            "                 [batch] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            4. Reshape `permuted_reshaped_padded` to flatten `block_shape` into the batch\n",
            "               dimension, producing an output tensor of shape:\n",
            "        \n",
            "                 [batch * prod(block_shape)] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            Some examples:\n",
            "        \n",
            "            (1) For the following input of shape `[1, 2, 2, 1]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1], [2]], [[3], [4]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 1, 1, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n",
            "            ```\n",
            "        \n",
            "            (2) For the following input of shape `[1, 2, 2, 3]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1, 2, 3], [4, 5, 6]],\n",
            "                  [[7, 8, 9], [10, 11, 12]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 1, 1, 3]` and value:\n",
            "        \n",
            "            ```\n",
            "            [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n",
            "            ```\n",
            "        \n",
            "            (3) For the following input of shape `[1, 4, 4, 1]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1],   [2],  [3],  [4]],\n",
            "                  [[5],   [6],  [7],  [8]],\n",
            "                  [[9],  [10], [11],  [12]],\n",
            "                  [[13], [14], [15],  [16]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 2, 2, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1], [3]], [[9], [11]]],\n",
            "                 [[[2], [4]], [[10], [12]]],\n",
            "                 [[[5], [7]], [[13], [15]]],\n",
            "                 [[[6], [8]], [[14], [16]]]]\n",
            "            ```\n",
            "        \n",
            "            (4) For the following input of shape `[2, 2, 4, 1]`, block_shape = `[2, 2]`, and\n",
            "                paddings = `[[0, 0], [2, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1],   [2],  [3],  [4]],\n",
            "                  [[5],   [6],  [7],  [8]]],\n",
            "                 [[[9],  [10], [11],  [12]],\n",
            "                  [[13], [14], [15],  [16]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[8, 1, 3, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[0], [1], [3]]], [[[0], [9], [11]]],\n",
            "                 [[[0], [2], [4]]], [[[0], [10], [12]]],\n",
            "                 [[[0], [5], [7]]], [[[0], [13], [15]]],\n",
            "                 [[[0], [6], [8]]], [[[0], [14], [16]]]]\n",
            "            ```\n",
            "        \n",
            "            Among others, this operation is useful for reducing atrous convolution into\n",
            "            regular convolution.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    space_to_batch_nd(input, block_shape, paddings, name=None)\n",
            "        SpaceToBatch for N-D tensors of type T.\n",
            "        \n",
            "        This operation divides \"spatial\" dimensions `[1, ..., M]` of the input into a\n",
            "        grid of blocks of shape `block_shape`, and interleaves these blocks with the\n",
            "        \"batch\" dimension (0) such that in the output, the spatial dimensions\n",
            "        `[1, ..., M]` correspond to the position within the grid, and the batch\n",
            "        dimension combines both the position within a spatial block and the original\n",
            "        batch position.  Prior to division into blocks, the spatial dimensions of the\n",
            "        input are optionally zero padded according to `paddings`.  See below for a\n",
            "        precise description.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "            N-D with shape `input_shape = [batch] + spatial_shape + remaining_shape`,\n",
            "            where spatial_shape has `M` dimensions.\n",
            "          block_shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            1-D with shape `[M]`, all values must be >= 1.\n",
            "          paddings: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            2-D with shape `[M, 2]`, all values must be >= 0.\n",
            "              `paddings[i] = [pad_start, pad_end]` specifies the padding for input dimension\n",
            "              `i + 1`, which corresponds to spatial dimension `i`.  It is required that\n",
            "              `block_shape[i]` divides `input_shape[i + 1] + pad_start + pad_end`.\n",
            "        \n",
            "            This operation is equivalent to the following steps:\n",
            "        \n",
            "            1. Zero-pad the start and end of dimensions `[1, ..., M]` of the\n",
            "               input according to `paddings` to produce `padded` of shape `padded_shape`.\n",
            "        \n",
            "            2. Reshape `padded` to `reshaped_padded` of shape:\n",
            "        \n",
            "                 [batch] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                   block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1],\n",
            "                  block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            3. Permute dimensions of `reshaped_padded` to produce\n",
            "               `permuted_reshaped_padded` of shape:\n",
            "        \n",
            "                 block_shape +\n",
            "                 [batch] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            4. Reshape `permuted_reshaped_padded` to flatten `block_shape` into the batch\n",
            "               dimension, producing an output tensor of shape:\n",
            "        \n",
            "                 [batch * prod(block_shape)] +\n",
            "                 [padded_shape[1] / block_shape[0],\n",
            "                  ...,\n",
            "                  padded_shape[M] / block_shape[M-1]] +\n",
            "                 remaining_shape\n",
            "        \n",
            "            Some examples:\n",
            "        \n",
            "            (1) For the following input of shape `[1, 2, 2, 1]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1], [2]], [[3], [4]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 1, 1, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            [[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\n",
            "            ```\n",
            "        \n",
            "            (2) For the following input of shape `[1, 2, 2, 3]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1, 2, 3], [4, 5, 6]],\n",
            "                  [[7, 8, 9], [10, 11, 12]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 1, 1, 3]` and value:\n",
            "        \n",
            "            ```\n",
            "            [[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\n",
            "            ```\n",
            "        \n",
            "            (3) For the following input of shape `[1, 4, 4, 1]`, `block_shape = [2, 2]`, and\n",
            "                `paddings = [[0, 0], [0, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1],   [2],  [3],  [4]],\n",
            "                  [[5],   [6],  [7],  [8]],\n",
            "                  [[9],  [10], [11],  [12]],\n",
            "                  [[13], [14], [15],  [16]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[4, 2, 2, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1], [3]], [[9], [11]]],\n",
            "                 [[[2], [4]], [[10], [12]]],\n",
            "                 [[[5], [7]], [[13], [15]]],\n",
            "                 [[[6], [8]], [[14], [16]]]]\n",
            "            ```\n",
            "        \n",
            "            (4) For the following input of shape `[2, 2, 4, 1]`, block_shape = `[2, 2]`, and\n",
            "                paddings = `[[0, 0], [2, 0]]`:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[1],   [2],  [3],  [4]],\n",
            "                  [[5],   [6],  [7],  [8]]],\n",
            "                 [[[9],  [10], [11],  [12]],\n",
            "                  [[13], [14], [15],  [16]]]]\n",
            "            ```\n",
            "        \n",
            "            The output tensor has shape `[8, 1, 3, 1]` and value:\n",
            "        \n",
            "            ```\n",
            "            x = [[[[0], [1], [3]]], [[[0], [9], [11]]],\n",
            "                 [[[0], [2], [4]]], [[[0], [10], [12]]],\n",
            "                 [[[0], [5], [7]]], [[[0], [13], [15]]],\n",
            "                 [[[0], [6], [8]]], [[[0], [14], [16]]]]\n",
            "            ```\n",
            "        \n",
            "            Among others, this operation is useful for reducing atrous convolution into\n",
            "            regular convolution.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    split(value, num_or_size_splits, axis=0, num=None, name='split')\n",
            "        Splits a tensor `value` into a list of sub tensors.\n",
            "        \n",
            "        See also `tf.unstack`.\n",
            "        \n",
            "        If `num_or_size_splits` is an integer, then `value` is split along the\n",
            "        dimension `axis` into `num_split` smaller tensors. This requires that\n",
            "        `value.shape[axis]` is divisible by `num_split`.\n",
            "        \n",
            "        If `num_or_size_splits` is a 1-D Tensor (or list), we call it `size_splits`\n",
            "        and `value` is split into `len(size_splits)` elements. The shape of the `i`-th\n",
            "        element has the same size as the `value` except along dimension `axis` where\n",
            "        the size is `size_splits[i]`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.Variable(tf.random.uniform([5, 30], -1, 1))\n",
            "        \n",
            "        Split `x` into 3 tensors along dimension 1\n",
            "        >>> s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)\n",
            "        >>> tf.shape(s0).numpy()\n",
            "        array([ 5, 10], dtype=int32)\n",
            "        \n",
            "        Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1\n",
            "        >>> split0, split1, split2 = tf.split(x, [4, 15, 11], 1)\n",
            "        >>> tf.shape(split0).numpy()\n",
            "        array([5, 4], dtype=int32)\n",
            "        >>> tf.shape(split1).numpy()\n",
            "        array([ 5, 15], dtype=int32)\n",
            "        >>> tf.shape(split2).numpy()\n",
            "        array([ 5, 11], dtype=int32)\n",
            "        \n",
            "        Args:\n",
            "          value: The `Tensor` to split.\n",
            "          num_or_size_splits: Either an integer indicating the number of splits along\n",
            "            `axis` or a 1-D integer `Tensor` or Python list containing the sizes of\n",
            "            each output tensor along `axis`. If a scalar, then it must evenly divide\n",
            "            `value.shape[axis]`; otherwise the sum of sizes along the split axis\n",
            "            must match that of the `value`.\n",
            "          axis: An integer or scalar `int32` `Tensor`. The dimension along which to\n",
            "            split. Must be in the range `[-rank(value), rank(value))`. Defaults to 0.\n",
            "          num: Optional, used to specify the number of outputs when it cannot be\n",
            "            inferred from the shape of `size_splits`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          if `num_or_size_splits` is a scalar returns a list of `num_or_size_splits`\n",
            "          `Tensor` objects; if `num_or_size_splits` is a 1-D Tensor returns\n",
            "          `num_or_size_splits.get_shape[0]` `Tensor` objects resulting from splitting\n",
            "          `value`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `num` is unspecified and cannot be inferred.\n",
            "    \n",
            "    sqrt(x, name=None)\n",
            "        Computes element-wise square root of the input tensor.\n",
            "        \n",
            "        Note: This operation does not support integer types.\n",
            "        \n",
            "        >>> x = tf.constant([[4.0], [16.0]])\n",
            "        >>> tf.sqrt(x)\n",
            "        <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "          array([[2.],\n",
            "                 [4.]], dtype=float32)>\n",
            "        >>> y = tf.constant([[-4.0], [16.0]])\n",
            "        >>> tf.sqrt(y)\n",
            "        <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "          array([[nan],\n",
            "                 [ 4.]], dtype=float32)>\n",
            "        >>> z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)\n",
            "        >>> tf.sqrt(z)\n",
            "        <tf.Tensor: shape=(2, 1), dtype=complex128, numpy=\n",
            "          array([[0.0+1.j],\n",
            "                 [4.0+0.j]])>\n",
            "        \n",
            "        Note: In order to support complex complex, please provide an input tensor\n",
            "        of `complex64` or `complex128`.\n",
            "        \n",
            "        Args:\n",
            "          x: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,\n",
            "            `complex64`, `complex128`\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `tf.Tensor` of same size, type and sparsity as `x`.\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    square(x, name=None)\n",
            "        Computes square of x element-wise.\n",
            "        \n",
            "        I.e., \\\\(y = x * x = x^2\\\\).\n",
            "        \n",
            "        >>> tf.math.square([-2., 0., 3.])\n",
            "        <tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.square(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    squeeze = squeeze_v2(input, axis=None, name=None)\n",
            "        Removes dimensions of size 1 from the shape of a tensor.\n",
            "        \n",
            "        Given a tensor `input`, this operation returns a tensor of the same type with\n",
            "        all dimensions of size 1 removed. If you don't want to remove all size 1\n",
            "        dimensions, you can remove specific size 1 dimensions by specifying\n",
            "        `axis`.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```python\n",
            "        # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
            "        tf.shape(tf.squeeze(t))  # [2, 3]\n",
            "        ```\n",
            "        \n",
            "        Or, to remove specific size 1 dimensions:\n",
            "        \n",
            "        ```python\n",
            "        # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
            "        tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n",
            "        ```\n",
            "        \n",
            "        Unlike the older op `tf.compat.v1.squeeze`, this op does not accept a\n",
            "        deprecated `squeeze_dims` argument.\n",
            "        \n",
            "        Note: if `input` is a `tf.RaggedTensor`, then this operation takes `O(N)`\n",
            "        time, where `N` is the number of elements in the squeezed dimensions.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. The `input` to squeeze.\n",
            "          axis: An optional list of `ints`. Defaults to `[]`. If specified, only\n",
            "            squeezes the dimensions listed. The dimension index starts at 0. It is an\n",
            "            error to squeeze a dimension that is not 1. Must be in the range\n",
            "            `[-rank(input), rank(input))`. Must be specified if `input` is a\n",
            "            `RaggedTensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "          Contains the same data as `input`, but has one or more dimensions of\n",
            "          size 1 removed.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: The input cannot be converted to a tensor, or the specified\n",
            "            axis cannot be squeezed.\n",
            "    \n",
            "    stack(values, axis=0, name='stack')\n",
            "        Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n",
            "        \n",
            "        See also `tf.concat`, `tf.tile`, `tf.repeat`.\n",
            "        \n",
            "        Packs the list of tensors in `values` into a tensor with rank one higher than\n",
            "        each tensor in `values`, by packing them along the `axis` dimension.\n",
            "        Given a list of length `N` of tensors of shape `(A, B, C)`;\n",
            "        \n",
            "        if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n",
            "        if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n",
            "        Etc.\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([1, 4])\n",
            "        >>> y = tf.constant([2, 5])\n",
            "        >>> z = tf.constant([3, 6])\n",
            "        >>> tf.stack([x, y, z])\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "        array([[1, 4],\n",
            "               [2, 5],\n",
            "               [3, 6]], dtype=int32)>\n",
            "        >>> tf.stack([x, y, z], axis=1)\n",
            "        <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "        array([[1, 2, 3],\n",
            "               [4, 5, 6]], dtype=int32)>\n",
            "        \n",
            "        This is the opposite of unstack.  The numpy equivalent is `np.stack`\n",
            "        \n",
            "        >>> np.array_equal(np.stack([x, y, z]), tf.stack([x, y, z]))\n",
            "        True\n",
            "        \n",
            "        Args:\n",
            "          values: A list of `Tensor` objects with the same shape and type.\n",
            "          axis: An `int`. The axis to stack along. Defaults to the first dimension.\n",
            "            Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          output: A stacked `Tensor` with the same type as `values`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `axis` is out of the range [-(R+1), R+1).\n",
            "    \n",
            "    stop_gradient(input, name=None)\n",
            "        Stops gradient computation.\n",
            "        \n",
            "        When executed in a graph, this op outputs its input tensor as-is.\n",
            "        \n",
            "        When building ops to compute gradients, this op prevents the contribution of\n",
            "        its inputs to be taken into account.  Normally, the gradient generator adds ops\n",
            "        to a graph to compute the derivatives of a specified 'loss' by recursively\n",
            "        finding out inputs that contributed to its computation.  If you insert this op\n",
            "        in the graph it inputs are masked from the gradient generator.  They are not\n",
            "        taken into account for computing gradients.\n",
            "        \n",
            "        This is useful any time you want to compute a value with TensorFlow but need\n",
            "        to pretend that the value was a constant. Some examples include:\n",
            "        \n",
            "        *  The *EM* algorithm where the *M-step* should not involve backpropagation\n",
            "           through the output of the *E-step*.\n",
            "        *  Contrastive divergence training of Boltzmann machines where, when\n",
            "           differentiating the energy function, the training must not backpropagate\n",
            "           through the graph that generated the samples from the model.\n",
            "        *  Adversarial training, where no backprop should happen through the adversarial\n",
            "           example generation process.\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    strided_slice(input_, begin, end, strides=None, begin_mask=0, end_mask=0, ellipsis_mask=0, new_axis_mask=0, shrink_axis_mask=0, var=None, name=None)\n",
            "        Extracts a strided slice of a tensor (generalized python array indexing).\n",
            "        \n",
            "        **Instead of calling this op directly most users will want to use the\n",
            "        NumPy-style slicing syntax (e.g. `tensor[..., 3:4:-1, tf.newaxis, 3]`), which\n",
            "        is supported via `tf.Tensor.__getitem__` and `tf.Variable.__getitem__`.**\n",
            "        The interface of this op is a low-level encoding of the slicing syntax.\n",
            "        \n",
            "        Roughly speaking, this op extracts a slice of size `(end-begin)/stride`\n",
            "        from the given `input_` tensor. Starting at the location specified by `begin`\n",
            "        the slice continues by adding `stride` to the index until all dimensions are\n",
            "        not less than `end`.\n",
            "        Note that a stride can be negative, which causes a reverse slice.\n",
            "        \n",
            "        Given a Python slice `input[spec0, spec1, ..., specn]`,\n",
            "        this function will be called as follows.\n",
            "        \n",
            "        `begin`, `end`, and `strides` will be vectors of length n.\n",
            "        n in general is not equal to the rank of the `input_` tensor.\n",
            "        \n",
            "        In each mask field (`begin_mask`, `end_mask`, `ellipsis_mask`,\n",
            "        `new_axis_mask`, `shrink_axis_mask`) the ith bit will correspond to\n",
            "        the ith spec.\n",
            "        \n",
            "        If the ith bit of `begin_mask` is set, `begin[i]` is ignored and\n",
            "        the fullest possible range in that dimension is used instead.\n",
            "        `end_mask` works analogously, except with the end range.\n",
            "        \n",
            "        `foo[5:,:,:3]` on a 7x8x9 tensor is equivalent to `foo[5:7,0:8,0:3]`.\n",
            "        `foo[::-1]` reverses a tensor with shape 8.\n",
            "        \n",
            "        If the ith bit of `ellipsis_mask` is set, as many unspecified dimensions\n",
            "        as needed will be inserted between other dimensions. Only one\n",
            "        non-zero bit is allowed in `ellipsis_mask`.\n",
            "        \n",
            "        For example `foo[3:5,...,4:5]` on a shape 10x3x3x10 tensor is\n",
            "        equivalent to `foo[3:5,:,:,4:5]` and\n",
            "        `foo[3:5,...]` is equivalent to `foo[3:5,:,:,:]`.\n",
            "        \n",
            "        If the ith bit of `new_axis_mask` is set, then `begin`,\n",
            "        `end`, and `stride` are ignored and a new length 1 dimension is\n",
            "        added at this point in the output tensor.\n",
            "        \n",
            "        For example,\n",
            "        `foo[:4, tf.newaxis, :2]` would produce a shape `(4, 1, 2)` tensor.\n",
            "        \n",
            "        If the ith bit of `shrink_axis_mask` is set, it implies that the ith\n",
            "        specification shrinks the dimensionality by 1, taking on the value at index\n",
            "        `begin[i]`. `end[i]` and `strides[i]` are ignored in this case. For example in\n",
            "        Python one might do `foo[:, 3, :]` which would result in `shrink_axis_mask`\n",
            "        equal to 2.\n",
            "        \n",
            "        \n",
            "        NOTE: `begin` and `end` are zero-indexed.\n",
            "        `strides` entries must be non-zero.\n",
            "        \n",
            "        \n",
            "        ```python\n",
            "        t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
            "                         [[3, 3, 3], [4, 4, 4]],\n",
            "                         [[5, 5, 5], [6, 6, 6]]])\n",
            "        tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]\n",
            "        tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],\n",
            "                                                              #   [4, 4, 4]]]\n",
            "        tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],\n",
            "                                                                 #   [3, 3, 3]]]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          input_: A `Tensor`.\n",
            "          begin: An `int32` or `int64` `Tensor`.\n",
            "          end: An `int32` or `int64` `Tensor`.\n",
            "          strides: An `int32` or `int64` `Tensor`.\n",
            "          begin_mask: An `int32` mask.\n",
            "          end_mask: An `int32` mask.\n",
            "          ellipsis_mask: An `int32` mask.\n",
            "          new_axis_mask: An `int32` mask.\n",
            "          shrink_axis_mask: An `int32` mask.\n",
            "          var: The variable corresponding to `input_` or None\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` the same type as `input`.\n",
            "    \n",
            "    subtract(x, y, name=None)\n",
            "        Returns x - y element-wise.\n",
            "        \n",
            "        *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    switch_case(branch_index, branch_fns, default=None, name='switch_case')\n",
            "        Create a switch/case operation, i.e. an integer-indexed conditional.\n",
            "        \n",
            "        See also `tf.case`.\n",
            "        \n",
            "        This op can be substantially more efficient than `tf.case` when exactly one\n",
            "        branch will be selected. `tf.switch_case` is more like a C++ switch/case\n",
            "        statement than `tf.case`, which is more like an if/elif/elif/else chain.\n",
            "        \n",
            "        The `branch_fns` parameter is either a dict from `int` to callables, or list\n",
            "        of (`int`, callable) pairs, or simply a list of callables (in which case the\n",
            "        index is implicitly the key). The `branch_index` `Tensor` is used to select an\n",
            "        element in `branch_fns` with matching `int` key, falling back to `default`\n",
            "        if none match, or `max(keys)` if no `default` is provided. The keys must form\n",
            "        a contiguous set from `0` to `len(branch_fns) - 1`.\n",
            "        \n",
            "        `tf.switch_case` supports nested structures as implemented in `tf.nest`. All\n",
            "        callables must return the same (possibly nested) value structure of lists,\n",
            "        tuples, and/or named tuples.\n",
            "        \n",
            "        **Example:**\n",
            "        \n",
            "        Pseudocode:\n",
            "        \n",
            "        ```c++\n",
            "        switch (branch_index) {  // c-style switch\n",
            "          case 0: return 17;\n",
            "          case 1: return 31;\n",
            "          default: return -1;\n",
            "        }\n",
            "        ```\n",
            "        or\n",
            "        ```python\n",
            "        branches = {0: lambda: 17, 1: lambda: 31}\n",
            "        branches.get(branch_index, lambda: -1)()\n",
            "        ```\n",
            "        \n",
            "        Expressions:\n",
            "        \n",
            "        ```python\n",
            "        def f1(): return tf.constant(17)\n",
            "        def f2(): return tf.constant(31)\n",
            "        def f3(): return tf.constant(-1)\n",
            "        r = tf.switch_case(branch_index, branch_fns={0: f1, 1: f2}, default=f3)\n",
            "        # Equivalent: tf.switch_case(branch_index, branch_fns={0: f1, 1: f2, 2: f3})\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          branch_index: An int Tensor specifying which of `branch_fns` should be\n",
            "            executed.\n",
            "          branch_fns: A `dict` mapping `int`s to callables, or a `list` of\n",
            "            (`int`, callable) pairs, or simply a list of callables (in which case the\n",
            "            index serves as the key). Each callable must return a matching structure\n",
            "            of tensors.\n",
            "          default: Optional callable that returns a structure of tensors.\n",
            "          name: A name for this operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The tensors returned by the callable identified by `branch_index`, or those\n",
            "          returned by `default` if no key matches and `default` was provided, or those\n",
            "          returned by the max-keyed `branch_fn` if no `default` is provided.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `branch_fns` is not a list/dictionary.\n",
            "          TypeError: If `branch_fns` is a list but does not contain 2-tuples or\n",
            "                     callables.\n",
            "          TypeError: If `fns[i]` is not callable for any i, or `default` is not\n",
            "                     callable.\n",
            "    \n",
            "    tan(x, name=None)\n",
            "        Computes tan of x element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes tangent of every\n",
            "          element in the tensor. Input range is `(-inf, inf)` and\n",
            "          output range is `(-inf, inf)`. If input lies outside the boundary, `nan`\n",
            "          is returned.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n",
            "          tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    tanh(x, name=None)\n",
            "        Computes hyperbolic tangent of `x` element-wise.\n",
            "        \n",
            "          Given an input tensor, this function computes hyperbolic tangent of every\n",
            "          element in the tensor. Input range is `[-inf, inf]` and\n",
            "          output range is `[-1,1]`.\n",
            "        \n",
            "          ```python\n",
            "          x = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\n",
            "          tf.math.tanh(x) ==> [-1. -0.99990916 -0.46211717 0.7615942 0.8336547 0.9640276 0.9950547 1.]\n",
            "          ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "        \n",
            "          If `x` is a `SparseTensor`, returns\n",
            "          `SparseTensor(x.indices, tf.math.tanh(x.values, ...), x.dense_shape)`\n",
            "    \n",
            "    tensor_scatter_nd_add = tensor_scatter_add(tensor, indices, updates, name=None)\n",
            "        Adds sparse `updates` to an existing tensor according to `indices`.\n",
            "        \n",
            "        This operation creates a new tensor by adding sparse `updates` to the passed\n",
            "        in `tensor`.\n",
            "        This operation is very similar to `tf.scatter_nd_add`, except that the updates\n",
            "        are added onto an existing tensor (as opposed to a variable). If the memory\n",
            "        for the existing tensor cannot be re-used, a copy is made and updated.\n",
            "        \n",
            "        `indices` is an integer tensor containing indices into a new tensor of shape\n",
            "        `shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n",
            "        \n",
            "            indices.shape[-1] <= shape.rank\n",
            "        \n",
            "        The last dimension of `indices` corresponds to indices into elements\n",
            "        (if `indices.shape[-1] = shape.rank`) or slices\n",
            "        (if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n",
            "        `shape`.  `updates` is a tensor with shape\n",
            "        \n",
            "            indices.shape[:-1] + shape[indices.shape[-1]:]\n",
            "        \n",
            "        The simplest form of tensor_scatter_add is to add individual elements to a\n",
            "        tensor by index. For example, say we want to add 4 elements in a rank-1\n",
            "        tensor with 8 elements.\n",
            "        \n",
            "        In Python, this scatter add operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[4], [3], [1], [7]])\n",
            "            updates = tf.constant([9, 10, 11, 12])\n",
            "            tensor = tf.ones([8], dtype=tf.int32)\n",
            "            updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n",
            "            print(updated)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [1, 12, 1, 11, 10, 1, 1, 13]\n",
            "        \n",
            "        We can also, insert entire slices of a higher rank tensor all at once. For\n",
            "        example, if we wanted to insert two slices in the first dimension of a\n",
            "        rank-3 tensor with two matrices of new values.\n",
            "        \n",
            "        In Python, this scatter add operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[0], [2]])\n",
            "            updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "                                   [[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]]])\n",
            "            tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n",
            "            updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n",
            "            print(updated)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n",
            "             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n",
            "             [[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n",
            "             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, the index is ignored.\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`. Tensor to copy/update.\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Index tensor.\n",
            "          updates: A `Tensor`. Must have the same type as `tensor`.\n",
            "            Updates to scatter into output.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "    \n",
            "    tensor_scatter_nd_sub = tensor_scatter_sub(tensor, indices, updates, name=None)\n",
            "        Subtracts sparse `updates` from an existing tensor according to `indices`.\n",
            "        \n",
            "        This operation creates a new tensor by subtracting sparse `updates` from the\n",
            "        passed in `tensor`.\n",
            "        This operation is very similar to `tf.scatter_nd_sub`, except that the updates\n",
            "        are subtracted from an existing tensor (as opposed to a variable). If the memory\n",
            "        for the existing tensor cannot be re-used, a copy is made and updated.\n",
            "        \n",
            "        `indices` is an integer tensor containing indices into a new tensor of shape\n",
            "        `shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n",
            "        \n",
            "            indices.shape[-1] <= shape.rank\n",
            "        \n",
            "        The last dimension of `indices` corresponds to indices into elements\n",
            "        (if `indices.shape[-1] = shape.rank`) or slices\n",
            "        (if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n",
            "        `shape`.  `updates` is a tensor with shape\n",
            "        \n",
            "            indices.shape[:-1] + shape[indices.shape[-1]:]\n",
            "        \n",
            "        The simplest form of tensor_scatter_sub is to subtract individual elements\n",
            "        from a tensor by index. For example, say we want to insert 4 scattered elements\n",
            "        in a rank-1 tensor with 8 elements.\n",
            "        \n",
            "        In Python, this scatter subtract operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[4], [3], [1], [7]])\n",
            "            updates = tf.constant([9, 10, 11, 12])\n",
            "            tensor = tf.ones([8], dtype=tf.int32)\n",
            "            updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n",
            "            print(updated)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [1, -10, 1, -9, -8, 1, 1, -11]\n",
            "        \n",
            "        We can also, insert entire slices of a higher rank tensor all at once. For\n",
            "        example, if we wanted to insert two slices in the first dimension of a\n",
            "        rank-3 tensor with two matrices of new values.\n",
            "        \n",
            "        In Python, this scatter add operation would look like this:\n",
            "        \n",
            "        ```python\n",
            "            indices = tf.constant([[0], [2]])\n",
            "            updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "                                   [[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "                                    [7, 7, 7, 7], [8, 8, 8, 8]]])\n",
            "            tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n",
            "            updated = tf.tensor_scatter_nd_sub(tensor, indices, updates)\n",
            "            print(updated)\n",
            "        ```\n",
            "        \n",
            "        The resulting tensor would look like this:\n",
            "        \n",
            "            [[[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n",
            "             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n",
            "             [[-4, -4, -4, -4], [-5, -5, -5, -5], [-6, -6, -6, -6], [-7, -7, -7, -7]],\n",
            "             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, the index is ignored.\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`. Tensor to copy/update.\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Index tensor.\n",
            "          updates: A `Tensor`. Must have the same type as `tensor`.\n",
            "            Updates to scatter into output.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "    \n",
            "    tensor_scatter_nd_update = tensor_scatter_update(tensor, indices, updates, name=None)\n",
            "        Scatter `updates` into an existing tensor according to `indices`.\n",
            "        \n",
            "        This operation creates a new tensor by applying sparse `updates` to the passed\n",
            "        in `tensor`.\n",
            "        This operation is very similar to `tf.scatter_nd`, except that the updates are\n",
            "        scattered onto an existing tensor (as opposed to a zero-tensor). If the memory\n",
            "        for the existing tensor cannot be re-used, a copy is made and updated.\n",
            "        \n",
            "        If `indices` contains duplicates, then their updates are accumulated (summed).\n",
            "        \n",
            "        **WARNING**: The order in which updates are applied is nondeterministic, so the\n",
            "        output will be nondeterministic if `indices` contains duplicates -- because\n",
            "        of some numerical approximation issues, numbers summed in different order\n",
            "        may yield different results.\n",
            "        \n",
            "        `indices` is an integer tensor containing indices into a new tensor of shape\n",
            "        `shape`.  The last dimension of `indices` can be at most the rank of `shape`:\n",
            "        \n",
            "            indices.shape[-1] <= shape.rank\n",
            "        \n",
            "        The last dimension of `indices` corresponds to indices into elements\n",
            "        (if `indices.shape[-1] = shape.rank`) or slices\n",
            "        (if `indices.shape[-1] < shape.rank`) along dimension `indices.shape[-1]` of\n",
            "        `shape`.  `updates` is a tensor with shape\n",
            "        \n",
            "            indices.shape[:-1] + shape[indices.shape[-1]:]\n",
            "        \n",
            "        The simplest form of scatter is to insert individual elements in a tensor by\n",
            "        index. For example, say we want to insert 4 scattered elements in a rank-1\n",
            "        tensor with 8 elements.\n",
            "        \n",
            "        <div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n",
            "        <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/ScatterNd1.png\" alt>\n",
            "        </div>\n",
            "        \n",
            "        In Python, this scatter operation would look like this:\n",
            "        \n",
            "            >>> indices = tf.constant([[4], [3], [1], [7]])\n",
            "            >>> updates = tf.constant([9, 10, 11, 12])\n",
            "            >>> tensor = tf.ones([8], dtype=tf.int32)\n",
            "            >>> print(tf.tensor_scatter_nd_update(tensor, indices, updates))\n",
            "            tf.Tensor([ 1 11  1 10  9  1  1 12], shape=(8,), dtype=int32)\n",
            "        \n",
            "        We can also, insert entire slices of a higher rank tensor all at once. For\n",
            "        example, if we wanted to insert two slices in the first dimension of a\n",
            "        rank-3 tensor with two matrices of new values.\n",
            "        \n",
            "        In Python, this scatter operation would look like this:\n",
            "        \n",
            "            >>> indices = tf.constant([[0], [2]])\n",
            "            >>> updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "            ...                         [7, 7, 7, 7], [8, 8, 8, 8]],\n",
            "            ...                        [[5, 5, 5, 5], [6, 6, 6, 6],\n",
            "            ...                         [7, 7, 7, 7], [8, 8, 8, 8]]])\n",
            "            >>> tensor = tf.ones([4, 4, 4], dtype=tf.int32)\n",
            "            >>> print(tf.tensor_scatter_nd_update(tensor, indices, updates).numpy())\n",
            "            [[[5 5 5 5]\n",
            "              [6 6 6 6]\n",
            "              [7 7 7 7]\n",
            "              [8 8 8 8]]\n",
            "             [[1 1 1 1]\n",
            "              [1 1 1 1]\n",
            "              [1 1 1 1]\n",
            "              [1 1 1 1]]\n",
            "             [[5 5 5 5]\n",
            "              [6 6 6 6]\n",
            "              [7 7 7 7]\n",
            "              [8 8 8 8]]\n",
            "             [[1 1 1 1]\n",
            "              [1 1 1 1]\n",
            "              [1 1 1 1]\n",
            "              [1 1 1 1]]]\n",
            "        \n",
            "        Note that on CPU, if an out of bound index is found, an error is returned.\n",
            "        On GPU, if an out of bound index is found, the index is ignored.\n",
            "        \n",
            "        Args:\n",
            "          tensor: A `Tensor`. Tensor to copy/update.\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            Index tensor.\n",
            "          updates: A `Tensor`. Must have the same type as `tensor`.\n",
            "            Updates to scatter into output.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `tensor`.\n",
            "    \n",
            "    tensordot(a, b, axes, name=None)\n",
            "        Tensor contraction of a and b along specified axes and outer product.\n",
            "        \n",
            "        Tensordot (also known as tensor contraction) sums the product of elements\n",
            "        from `a` and `b` over the indices specified by `a_axes` and `b_axes`.\n",
            "        The lists `a_axes` and `b_axes` specify those pairs of axes along which to\n",
            "        contract the tensors. The axis `a_axes[i]` of `a` must have the same dimension\n",
            "        as axis `b_axes[i]` of `b` for all `i` in `range(0, len(a_axes))`. The lists\n",
            "        `a_axes` and `b_axes` must have identical length and consist of unique\n",
            "        integers that specify valid axes for each of the tensors. Additionally\n",
            "        outer product is supported by passing `axes=0`.\n",
            "        \n",
            "        This operation corresponds to `numpy.tensordot(a, b, axes)`.\n",
            "        \n",
            "        Example 1: When `a` and `b` are matrices (order 2), the case `axes = 1`\n",
            "        is equivalent to matrix multiplication.\n",
            "        \n",
            "        Example 2: When `a` and `b` are matrices (order 2), the case\n",
            "        `axes = [[1], [0]]` is equivalent to matrix multiplication.\n",
            "        \n",
            "        Example 3: When `a` and `b` are matrices (order 2), the case `axes=0` gives\n",
            "        the outer product, a tensor of order 4.\n",
            "        \n",
            "        Example 4: Suppose that \\\\(a_{ijk}\\\\) and \\\\(b_{lmn}\\\\) represent two\n",
            "        tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor\n",
            "        \\\\(c_{jklm}\\\\) whose entry\n",
            "        corresponding to the indices \\\\((j,k,l,m)\\\\) is given by:\n",
            "        \n",
            "        \\\\( c_{jklm} = \\sum_i a_{ijk} b_{lmi} \\\\).\n",
            "        \n",
            "        In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.\n",
            "        \n",
            "        Args:\n",
            "          a: `Tensor` of type `float32` or `float64`.\n",
            "          b: `Tensor` with the same type as `a`.\n",
            "          axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].\n",
            "            If axes is a scalar, sum over the last N axes of a and the first N axes of\n",
            "            b in order. If axes is a list or `Tensor` the first and second row contain\n",
            "            the set of unique integers specifying axes along which the contraction is\n",
            "            computed, for `a` and `b`, respectively. The number of axes for `a` and\n",
            "            `b` must be equal. If `axes=0`, computes the outer product between `a` and\n",
            "            `b`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with the same type as `a`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.\n",
            "          IndexError: If the values in axes exceed the rank of the corresponding\n",
            "            tensor.\n",
            "    \n",
            "    tile(input, multiples, name=None)\n",
            "        Constructs a tensor by tiling a given tensor.\n",
            "        \n",
            "        This operation creates a new tensor by replicating `input` `multiples` times.\n",
            "        The output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\n",
            "        and the values of `input` are replicated `multiples[i]` times along the 'i'th\n",
            "        dimension. For example, tiling `[a b c d]` by `[2]` produces\n",
            "        `[a b c d a b c d]`.\n",
            "        \n",
            "        >>> a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n",
            "        >>> b = tf.constant([1,2], tf.int32)\n",
            "        >>> tf.tile(a, b)\n",
            "        <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
            "        array([[1, 2, 3, 1, 2, 3],\n",
            "               [4, 5, 6, 4, 5, 6]], dtype=int32)>\n",
            "        >>> c = tf.constant([2,1], tf.int32)\n",
            "        >>> tf.tile(a, c)\n",
            "        <tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
            "        array([[1, 2, 3],\n",
            "               [4, 5, 6],\n",
            "               [1, 2, 3],\n",
            "               [4, 5, 6]], dtype=int32)>\n",
            "        >>> d = tf.constant([2,2], tf.int32)\n",
            "        >>> tf.tile(a, d)\n",
            "        <tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
            "        array([[1, 2, 3, 1, 2, 3],\n",
            "               [4, 5, 6, 4, 5, 6],\n",
            "               [1, 2, 3, 1, 2, 3],\n",
            "               [4, 5, 6, 4, 5, 6]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor`. 1-D or higher.\n",
            "          multiples: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            1-D. Length must be the same as the number of dimensions in `input`\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `input`.\n",
            "    \n",
            "    timestamp(name=None)\n",
            "        Provides the time since epoch in seconds.\n",
            "        \n",
            "        Returns the timestamp as a `float64` for seconds since the Unix epoch.\n",
            "        \n",
            "        Note: the timestamp is computed when the op is executed, not when it is added\n",
            "        to the graph.\n",
            "        \n",
            "        Args:\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` of type `float64`.\n",
            "    \n",
            "    transpose = transpose_v2(a, perm=None, conjugate=False, name='transpose')\n",
            "        Transposes `a`, where `a` is a Tensor.\n",
            "        \n",
            "        Permutes the dimensions according to the value of `perm`.\n",
            "        \n",
            "        The returned tensor's dimension `i` will correspond to the input dimension\n",
            "        `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\n",
            "        of the input tensor. Hence by default, this operation performs a regular\n",
            "        matrix transpose on 2-D input Tensors.\n",
            "        \n",
            "        If conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\n",
            "        then the values of `a` are conjugated and transposed.\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        In `numpy` transposes are memory-efficient constant time operations as they\n",
            "        simply return a new view of the same data with adjusted `strides`.\n",
            "        \n",
            "        TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
            "        the items permuted.\n",
            "        @end_compatibility\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
            "        >>> tf.transpose(x)\n",
            "        <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
            "        array([[1, 4],\n",
            "               [2, 5],\n",
            "               [3, 6]], dtype=int32)>\n",
            "        \n",
            "        Equivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n",
            "        \n",
            "        If `x` is complex, setting conjugate=True gives the conjugate transpose:\n",
            "        \n",
            "        >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
            "        ...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n",
            "        >>> tf.transpose(x, conjugate=True)\n",
            "        <tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\n",
            "        array([[1.-1.j, 4.-4.j],\n",
            "               [2.-2.j, 5.-5.j],\n",
            "               [3.-3.j, 6.-6.j]])>\n",
            "        \n",
            "        'perm' is more useful for n-dimensional tensors where n > 2:\n",
            "        \n",
            "        >>> x = tf.constant([[[ 1,  2,  3],\n",
            "        ...                   [ 4,  5,  6]],\n",
            "        ...                  [[ 7,  8,  9],\n",
            "        ...                   [10, 11, 12]]])\n",
            "        \n",
            "        As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n",
            "        \n",
            "        To take the transpose of the matrices in dimension-0 (such as when you are\n",
            "        transposing matrices where 0 is the batch dimesnion), you would set\n",
            "        `perm=[0,2,1]`.\n",
            "        \n",
            "        >>> tf.transpose(x, perm=[0, 2, 1])\n",
            "        <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
            "        array([[[ 1,  4],\n",
            "                [ 2,  5],\n",
            "                [ 3,  6]],\n",
            "                [[ 7, 10],\n",
            "                [ 8, 11],\n",
            "                [ 9, 12]]], dtype=int32)>\n",
            "        \n",
            "        Note: This has a shorthand `linalg.matrix_transpose`):\n",
            "        \n",
            "        Args:\n",
            "          a: A `Tensor`.\n",
            "          perm: A permutation of the dimensions of `a`.  This should be a vector.\n",
            "          conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
            "            to tf.math.conj(tf.transpose(input)).\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A transposed `Tensor`.\n",
            "    \n",
            "    truediv(x, y, name=None)\n",
            "        Divides x / y elementwise (using Python 3 division operator semantics).\n",
            "        \n",
            "        NOTE: Prefer using the Tensor operator or tf.divide which obey Python\n",
            "        division operator semantics.\n",
            "        \n",
            "        This function forces Python 3 division operator semantics where all integer\n",
            "        arguments are cast to floating types first.   This op is generated by normal\n",
            "        `x / y` division in Python 3 and in Python 2.7 with\n",
            "        `from __future__ import division`.  If you want integer division that rounds\n",
            "        down, use `x // y` or `tf.math.floordiv`.\n",
            "        \n",
            "        `x` and `y` must have the same numeric type.  If the inputs are floating\n",
            "        point, the output will have the same type.  If the inputs are integral, the\n",
            "        inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`\n",
            "        and `int64` (matching the behavior of Numpy).\n",
            "        \n",
            "        Args:\n",
            "          x: `Tensor` numerator of numeric type.\n",
            "          y: `Tensor` denominator of numeric type.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          `x / y` evaluated in floating point.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: If `x` and `y` have different dtypes.\n",
            "    \n",
            "    truncatediv = truncate_div(x, y, name=None)\n",
            "        Returns x / y element-wise for integer types.\n",
            "        \n",
            "        Truncation designates that negative numbers will round fractional quantities\n",
            "        toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different\n",
            "        than Python semantics. See `FloorDiv` for a division function that matches\n",
            "        Python Semantics.\n",
            "        \n",
            "        *NOTE*: `truncatediv` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    truncatemod = truncate_mod(x, y, name=None)\n",
            "        Returns element-wise remainder of division. This emulates C semantics in that\n",
            "        \n",
            "        the result here is consistent with a truncating divide. E.g. `truncate(x / y) *\n",
            "        y + truncate_mod(x, y) = x`.\n",
            "        \n",
            "        *NOTE*: `truncatemod` supports broadcasting. More about broadcasting\n",
            "        [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
            "          y: A `Tensor`. Must have the same type as `x`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `x`.\n",
            "    \n",
            "    tuple = tuple_v2(tensors, control_inputs=None, name=None)\n",
            "        Group tensors together.\n",
            "        \n",
            "        This creates a tuple of tensors with the same values as the `tensors`\n",
            "        argument, except that the value of each tensor is only returned after the\n",
            "        values of all tensors have been computed.\n",
            "        \n",
            "        `control_inputs` contains additional ops that have to finish before this op\n",
            "        finishes, but whose outputs are not returned.\n",
            "        \n",
            "        This can be used as a \"join\" mechanism for parallel computations: all the\n",
            "        argument tensors can be computed in parallel, but the values of any tensor\n",
            "        returned by `tuple` are only available after all the parallel computations\n",
            "        are done.\n",
            "        \n",
            "        See also `tf.group` and\n",
            "        `tf.control_dependencies`.\n",
            "        \n",
            "        Args:\n",
            "          tensors: A list of `Tensor`s or `IndexedSlices`, some entries can be `None`.\n",
            "          control_inputs: List of additional ops to finish before returning.\n",
            "          name: (optional) A name to use as a `name_scope` for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          Same as `tensors`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `tensors` does not contain any `Tensor` or `IndexedSlices`.\n",
            "          TypeError: If `control_inputs` is not a list of `Operation` or `Tensor`\n",
            "            objects.\n",
            "    \n",
            "    unique(x, out_idx=tf.int32, name=None)\n",
            "        Finds unique elements in a 1-D tensor.\n",
            "        \n",
            "        This operation returns a tensor `y` containing all of the unique elements of `x`\n",
            "        sorted in the same order that they occur in `x`; `x` does not need to be sorted.\n",
            "        This operation also returns a tensor `idx` the same size as `x` that contains\n",
            "        the index of each value of `x` in the unique output `y`. In other words:\n",
            "        \n",
            "        `y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "        ```\n",
            "        # tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\n",
            "        y, idx = unique(x)\n",
            "        y ==> [1, 2, 4, 7, 8]\n",
            "        idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n",
            "        ```\n",
            "        \n",
            "        ```\n",
            "        # tensor 'x' is [4, 5, 1, 2, 3, 3, 4, 5]\n",
            "        y, idx = unique(x)\n",
            "        y ==> [4, 5, 1, 2, 3]\n",
            "        idx ==> [0, 1, 2, 3, 4, 4, 0, 1]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. 1-D.\n",
            "          out_idx: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A tuple of `Tensor` objects (y, idx).\n",
            "        \n",
            "          y: A `Tensor`. Has the same type as `x`.\n",
            "          idx: A `Tensor` of type `out_idx`.\n",
            "    \n",
            "    unique_with_counts(x, out_idx=tf.int32, name=None)\n",
            "        Finds unique elements in a 1-D tensor.\n",
            "        \n",
            "        This operation returns a tensor `y` containing all of the unique elements of `x`\n",
            "        sorted in the same order that they occur in `x`. This operation also returns a\n",
            "        tensor `idx` the same size as `x` that contains the index of each value of `x`\n",
            "        in the unique output `y`. Finally, it returns a third tensor `count` that\n",
            "        contains the count of each element of `y` in `x`. In other words:\n",
            "        \n",
            "        `y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n",
            "        \n",
            "        For example:\n",
            "        \n",
            "        ```\n",
            "        # tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\n",
            "        y, idx, count = unique_with_counts(x)\n",
            "        y ==> [1, 2, 4, 7, 8]\n",
            "        idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n",
            "        count ==> [2, 1, 3, 1, 2]\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          x: A `Tensor`. 1-D.\n",
            "          out_idx: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A tuple of `Tensor` objects (y, idx, count).\n",
            "        \n",
            "          y: A `Tensor`. Has the same type as `x`.\n",
            "          idx: A `Tensor` of type `out_idx`.\n",
            "          count: A `Tensor` of type `out_idx`.\n",
            "    \n",
            "    unravel_index(indices, dims, name=None)\n",
            "        Converts an array of flat indices into a tuple of coordinate arrays.\n",
            "        \n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```\n",
            "        y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])\n",
            "        # 'dims' represent a hypothetical (3, 3) tensor of indices:\n",
            "        # [[0, 1, *2*],\n",
            "        #  [3, 4, *5*],\n",
            "        #  [6, *7*, 8]]\n",
            "        # For each entry from 'indices', this operation returns\n",
            "        # its coordinates (marked with '*'), such as\n",
            "        # 2 ==> (0, 2)\n",
            "        # 5 ==> (1, 2)\n",
            "        # 7 ==> (2, 1)\n",
            "        y ==> [[0, 1, 2], [2, 2, 1]]\n",
            "        ```\n",
            "        \n",
            "        @compatibility(numpy)\n",
            "        Equivalent to np.unravel_index\n",
            "        @end_compatibility\n",
            "        \n",
            "        Args:\n",
            "          indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
            "            An 0-D or 1-D `int` Tensor whose elements are indices into the\n",
            "            flattened version of an array of dimensions dims.\n",
            "          dims: A `Tensor`. Must have the same type as `indices`.\n",
            "            An 1-D `int` Tensor. The shape of the array to use for unraveling\n",
            "            indices.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor`. Has the same type as `indices`.\n",
            "    \n",
            "    unstack(value, num=None, axis=0, name='unstack')\n",
            "        Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n",
            "        \n",
            "        Unpacks `num` tensors from `value` by chipping it along the `axis` dimension.\n",
            "        If `num` is not specified (the default), it is inferred from `value`'s shape.\n",
            "        If `value.shape[axis]` is not known, `ValueError` is raised.\n",
            "        \n",
            "        For example, given a tensor of shape `(A, B, C, D)`;\n",
            "        \n",
            "        If `axis == 0` then the i'th tensor in `output` is the slice\n",
            "          `value[i, :, :, :]` and each tensor in `output` will have shape `(B, C, D)`.\n",
            "          (Note that the dimension unpacked along is gone, unlike `split`).\n",
            "        \n",
            "        If `axis == 1` then the i'th tensor in `output` is the slice\n",
            "          `value[:, i, :, :]` and each tensor in `output` will have shape `(A, C, D)`.\n",
            "        Etc.\n",
            "        \n",
            "        This is the opposite of stack.\n",
            "        \n",
            "        Args:\n",
            "          value: A rank `R > 0` `Tensor` to be unstacked.\n",
            "          num: An `int`. The length of the dimension `axis`. Automatically inferred if\n",
            "            `None` (the default).\n",
            "          axis: An `int`. The axis to unstack along. Defaults to the first dimension.\n",
            "            Negative values wrap around, so the valid range is `[-R, R)`.\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          The list of `Tensor` objects unstacked from `value`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: If `num` is unspecified and cannot be inferred.\n",
            "          ValueError: If `axis` is out of the range [-R, R).\n",
            "    \n",
            "    variable_creator_scope(variable_creator)\n",
            "        Scope which defines a variable creation function to be used by variable().\n",
            "        \n",
            "        variable_creator is expected to be a function with the following signature:\n",
            "        \n",
            "        ```\n",
            "          def variable_creator(next_creator, **kwargs)\n",
            "        ```\n",
            "        \n",
            "        The creator is supposed to eventually call the next_creator to create a\n",
            "        variable if it does want to create a variable and not call Variable or\n",
            "        ResourceVariable directly. This helps make creators composable. A creator may\n",
            "        choose to create multiple variables, return already existing variables, or\n",
            "        simply register that a variable was created and defer to the next creators in\n",
            "        line. Creators can also modify the keyword arguments seen by the next\n",
            "        creators.\n",
            "        \n",
            "        Custom getters in the variable scope will eventually resolve down to these\n",
            "        custom creators when they do create variables.\n",
            "        \n",
            "        The valid keyword arguments in kwds are:\n",
            "        \n",
            "         * initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
            "              which is the initial value for the Variable. The initial value must have\n",
            "              a shape specified unless `validate_shape` is set to False. Can also be a\n",
            "              callable with no argument that returns the initial value when called. In\n",
            "              that case, `dtype` must be specified. (Note that initializer functions\n",
            "              from init_ops.py must first be bound to a shape before being used here.)\n",
            "         * trainable: If `True`, the default, GradientTapes automatically watch\n",
            "              uses of this Variable.\n",
            "         * validate_shape: If `False`, allows the variable to be initialized with a\n",
            "              value of unknown shape. If `True`, the default, the shape of\n",
            "              `initial_value` must be known.\n",
            "         * caching_device: Optional device string describing where the Variable\n",
            "              should be cached for reading.  Defaults to the Variable's device.\n",
            "              If not `None`, caches on another device.  Typical use is to cache\n",
            "              on the device where the Ops using the Variable reside, to deduplicate\n",
            "              copying through `Switch` and other conditional statements.\n",
            "         * name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
            "              uniquified automatically.\n",
            "            dtype: If set, initial_value will be converted to the given type.\n",
            "              If `None`, either the datatype will be kept (if `initial_value` is\n",
            "              a Tensor), or `convert_to_tensor` will decide.\n",
            "         * constraint: A constraint function to be applied to the variable after\n",
            "              updates by some algorithms.\n",
            "         * synchronization: Indicates when a distributed a variable will be\n",
            "              aggregated. Accepted values are constants defined in the class\n",
            "              `tf.VariableSynchronization`. By default the synchronization is set to\n",
            "              `AUTO` and the current `DistributionStrategy` chooses\n",
            "              when to synchronize.\n",
            "         * aggregation: Indicates how a distributed variable will be aggregated.\n",
            "              Accepted values are constants defined in the class\n",
            "              `tf.VariableAggregation`.\n",
            "        \n",
            "        This set may grow over time, so it's important the signature of creators is as\n",
            "        mentioned above.\n",
            "        \n",
            "        Args:\n",
            "          variable_creator: the passed creator\n",
            "        \n",
            "        Yields:\n",
            "          A scope in which the creator is active\n",
            "    \n",
            "    vectorized_map(fn, elems)\n",
            "        Parallel map on the list of tensors unpacked from `elems` on dimension 0.\n",
            "        \n",
            "        \n",
            "        This method works similar to tf.map_fn but is optimized to run much faster,\n",
            "        possibly with a much larger memory footprint. The speedups are obtained by\n",
            "        vectorization (see https://arxiv.org/pdf/1903.04243.pdf). The idea behind\n",
            "        vectorization is to semantically launch all the invocations of `fn` in\n",
            "        parallel and fuse corresponding operations across all these invocations. This\n",
            "        fusion is done statically at graph generation time and the generated code is\n",
            "        often similar in performance to a manually fused version.\n",
            "        \n",
            "        Because `tf.vectorized_map` fully parallelizes the batch, this method will\n",
            "        generally be significantly faster than using `tf.map_fn`, especially in eager\n",
            "        mode. However this is an experimental feature and currently has a lot of\n",
            "        limitations:\n",
            "          - There should be no data dependency between the different semantic\n",
            "            invocations of `fn`, i.e. it should be safe to map the elements of the\n",
            "            inputs in any order.\n",
            "          - Stateful kernels may mostly not be supported since these often imply a\n",
            "            data dependency. We do support a limited set of such stateful kernels\n",
            "            though (like RandomFoo, Variable operations like reads, etc).\n",
            "          - `fn` has limited support for control flow operations. `tf.cond` in\n",
            "            particular is not supported.\n",
            "          - `fn` should return nested structure of Tensors or Operations. However\n",
            "            if an Operation is returned, it should have zero outputs.\n",
            "          - The shape and dtype of any intermediate or output tensors in the\n",
            "            computation of `fn` should not depend on the input to `fn`.\n",
            "        \n",
            "        Examples:\n",
            "        ```python\n",
            "        def outer_product(a):\n",
            "          return tf.tensordot(a, a, 0)\n",
            "        \n",
            "        batch_size = 100\n",
            "        a = tf.ones((batch_size, 32, 32))\n",
            "        c = tf.vectorized_map(outer_product, a)\n",
            "        assert c.shape == (batch_size, 32, 32, 32, 32)\n",
            "        ```\n",
            "        \n",
            "        ```python\n",
            "        # Computing per-example gradients\n",
            "        \n",
            "        batch_size = 10\n",
            "        num_features = 32\n",
            "        layer = tf.keras.layers.Dense(1)\n",
            "        \n",
            "        def model_fn(arg):\n",
            "          with tf.GradientTape() as g:\n",
            "            inp, label = arg\n",
            "            inp = tf.expand_dims(inp, 0)\n",
            "            label = tf.expand_dims(label, 0)\n",
            "            prediction = layer(inp)\n",
            "            loss = tf.nn.l2_loss(label - prediction)\n",
            "          return g.gradient(loss, (layer.kernel, layer.bias))\n",
            "        \n",
            "        inputs = tf.random.uniform([batch_size, num_features])\n",
            "        labels = tf.random.uniform([batch_size, 1])\n",
            "        per_example_gradients = tf.vectorized_map(model_fn, (inputs, labels))\n",
            "        assert per_example_gradients[0].shape == (batch_size, num_features, 1)\n",
            "        assert per_example_gradients[1].shape == (batch_size, 1)\n",
            "        ```\n",
            "        \n",
            "        Args:\n",
            "          fn: The callable to be performed. It accepts one argument, which will have\n",
            "            the same (possibly nested) structure as `elems`, and returns a possibly\n",
            "            nested structure of Tensors and Operations, which may be different than\n",
            "            the structure of `elems`.\n",
            "          elems: A tensor or (possibly nested) sequence of tensors, each of which will\n",
            "            be unpacked along their first dimension. The nested sequence of the\n",
            "            resulting slices will be mapped over by `fn`.\n",
            "        \n",
            "        Returns:\n",
            "          A tensor or (possibly nested) sequence of tensors. Each tensor packs the\n",
            "          results of applying fn to tensors unpacked from elems along the first\n",
            "          dimension, from first to last.\n",
            "    \n",
            "    where = where_v2(condition, x=None, y=None, name=None)\n",
            "        Return the elements where `condition` is `True` (multiplexing `x` and `y`).\n",
            "        \n",
            "        This operator has two modes: in one mode both `x` and `y` are provided, in\n",
            "        another mode neither are provided. `condition` is always expected to be a\n",
            "        `tf.Tensor` of type `bool`.\n",
            "        \n",
            "        #### Retrieving indices of `True` elements\n",
            "        \n",
            "        If `x` and `y` are not provided (both are None):\n",
            "        \n",
            "        `tf.where` will return the indices of `condition` that are `True`, in\n",
            "        the form of a 2-D tensor with shape (n, d).\n",
            "        (Where n is the number of matching indices in `condition`,\n",
            "        and d is the number of dimensions in `condition`).\n",
            "        \n",
            "        Indices are output in row-major order.\n",
            "        \n",
            "        >>> tf.where([True, False, False, True])\n",
            "        <tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n",
            "        array([[0],\n",
            "               [3]])>\n",
            "        \n",
            "        >>> tf.where([[True, False], [False, True]])\n",
            "        <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
            "        array([[0, 0],\n",
            "               [1, 1]])>\n",
            "        \n",
            "        >>> tf.where([[[True, False], [False, True], [True, True]]])\n",
            "        <tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n",
            "        array([[0, 0, 0],\n",
            "               [0, 1, 1],\n",
            "               [0, 2, 0],\n",
            "               [0, 2, 1]])>\n",
            "        \n",
            "        #### Multiplexing between `x` and `y`\n",
            "        \n",
            "        If `x` and `y` are provided (both have non-None values):\n",
            "        \n",
            "        `tf.where` will choose an output shape from the shapes of `condition`, `x`,\n",
            "        and `y` that all three shapes are\n",
            "        [broadcastable](https://docs.scipy.org/doc/numpy/reference/ufuncs.html) to.\n",
            "        \n",
            "        The `condition` tensor acts as a mask that chooses whether the corresponding\n",
            "        element / row in the output should be taken from `x`\n",
            "        (if the elemment in `condition is True) or `y` (if it is false).\n",
            "        \n",
            "        >>> tf.where([True, False, False, True], [1,2,3,4], [100,200,300,400])\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 200, 300,   4],\n",
            "        dtype=int32)>\n",
            "        >>> tf.where([True, False, False, True], [1,2,3,4], [100])\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],\n",
            "        dtype=int32)>\n",
            "        >>> tf.where([True, False, False, True], [1,2,3,4], 100)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],\n",
            "        dtype=int32)>\n",
            "        >>> tf.where([True, False, False, True], 1, 100)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   1],\n",
            "        dtype=int32)>\n",
            "        \n",
            "        >>> tf.where(True, [1,2,3,4], 100)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4],\n",
            "        dtype=int32)>\n",
            "        >>> tf.where(False, [1,2,3,4], 100)\n",
            "        <tf.Tensor: shape=(4,), dtype=int32, numpy=array([100, 100, 100, 100],\n",
            "        dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          condition: A `tf.Tensor` of type `bool`\n",
            "          x: If provided, a Tensor which is of the same type as `y`, and has a shape\n",
            "            broadcastable with `condition` and `y`.\n",
            "          y: If provided, a Tensor which is of the same type as `y`, and has a shape\n",
            "            broadcastable with `condition` and `x`.\n",
            "          name: A name of the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          If `x` and `y` are provided:\n",
            "            A `Tensor` with the same type as `x` and `y`, and shape that\n",
            "            is broadcast from `condition`, `x`, and `y`.\n",
            "          Otherwise, a `Tensor` with shape `(num_true, dim_size(condition))`.\n",
            "        \n",
            "        Raises:\n",
            "          ValueError: When exactly one of `x` or `y` is non-None, or the shapes\n",
            "            are not all broadcastable.\n",
            "    \n",
            "    while_loop = while_loop_v2(cond, body, loop_vars, shape_invariants=None, parallel_iterations=10, back_prop=True, swap_memory=False, maximum_iterations=None, name=None)\n",
            "        Repeat `body` while the condition `cond` is true. (deprecated argument values)\n",
            "        \n",
            "        Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(back_prop=False)`. They will be removed in a future version.\n",
            "        Instructions for updating:\n",
            "        back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "        Instead of:\n",
            "        results = tf.while_loop(c, b, vars, back_prop=False)\n",
            "        Use:\n",
            "        results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
            "        \n",
            "        `cond` is a callable returning a boolean scalar tensor. `body` is a callable\n",
            "        returning a (possibly nested) tuple, namedtuple or list of tensors of the same\n",
            "        arity (length and structure) and types as `loop_vars`. `loop_vars` is a\n",
            "        (possibly nested) tuple, namedtuple or list of tensors that is passed to both\n",
            "        `cond` and `body`. `cond` and `body` both take as many arguments as there are\n",
            "        `loop_vars`.\n",
            "        \n",
            "        In addition to regular Tensors or IndexedSlices, the body may accept and\n",
            "        return TensorArray objects.  The flows of the TensorArray objects will\n",
            "        be appropriately forwarded between loops and during gradient calculations.\n",
            "        \n",
            "        Note that `while_loop` calls `cond` and `body` *exactly once* (inside the\n",
            "        call to `while_loop`, and not at all during `Session.run()`). `while_loop`\n",
            "        stitches together the graph fragments created during the `cond` and `body`\n",
            "        calls with some additional graph nodes to create the graph flow that\n",
            "        repeats `body` until `cond` returns false.\n",
            "        \n",
            "        For correctness, `tf.while_loop()` strictly enforces shape invariants for\n",
            "        the loop variables. A shape invariant is a (possibly partial) shape that\n",
            "        is unchanged across the iterations of the loop. An error will be raised\n",
            "        if the shape of a loop variable after an iteration is determined to be more\n",
            "        general than or incompatible with its shape invariant. For example, a shape\n",
            "        of [11, None] is more general than a shape of [11, 17], and [11, 21] is not\n",
            "        compatible with [11, 17]. By default (if the argument `shape_invariants` is\n",
            "        not specified), it is assumed that the initial shape of each tensor in\n",
            "        `loop_vars` is the same in every iteration. The `shape_invariants` argument\n",
            "        allows the caller to specify a less specific shape invariant for each loop\n",
            "        variable, which is needed if the shape varies between iterations. The\n",
            "        `tf.Tensor.set_shape`\n",
            "        function may also be used in the `body` function to indicate that\n",
            "        the output loop variable has a particular shape. The shape invariant for\n",
            "        SparseTensor and IndexedSlices are treated specially as follows:\n",
            "        \n",
            "        a) If a loop variable is a SparseTensor, the shape invariant must be\n",
            "        TensorShape([r]) where r is the rank of the dense tensor represented\n",
            "        by the sparse tensor. It means the shapes of the three tensors of the\n",
            "        SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here\n",
            "        is the shape of the SparseTensor.dense_shape property. It must be the shape of\n",
            "        a vector.\n",
            "        \n",
            "        b) If a loop variable is an IndexedSlices, the shape invariant must be\n",
            "        a shape invariant of the values tensor of the IndexedSlices. It means\n",
            "        the shapes of the three tensors of the IndexedSlices are (shape, [shape[0]],\n",
            "        [shape.ndims]).\n",
            "        \n",
            "        `while_loop` implements non-strict semantics, enabling multiple iterations\n",
            "        to run in parallel. The maximum number of parallel iterations can be\n",
            "        controlled by `parallel_iterations`, which gives users some control over\n",
            "        memory consumption and execution order. For correct programs, `while_loop`\n",
            "        should return the same result for any parallel_iterations > 0.\n",
            "        \n",
            "        For training, TensorFlow stores the tensors that are produced in the\n",
            "        forward inference and are needed in back propagation. These tensors are a\n",
            "        main source of memory consumption and often cause OOM errors when training\n",
            "        on GPUs. When the flag swap_memory is true, we swap out these tensors from\n",
            "        GPU to CPU. This for example allows us to train RNN models with very long\n",
            "        sequences and large batches.\n",
            "        \n",
            "        Args:\n",
            "          cond: A callable that represents the termination condition of the loop.\n",
            "          body: A callable that represents the loop body.\n",
            "          loop_vars: A (possibly nested) tuple, namedtuple or list of numpy array,\n",
            "            `Tensor`, and `TensorArray` objects.\n",
            "          shape_invariants: The shape invariants for the loop variables.\n",
            "          parallel_iterations: The number of iterations allowed to run in parallel. It\n",
            "            must be a positive integer.\n",
            "          back_prop: (optional) Deprecated. False disables support for back\n",
            "            propagation. Prefer using `tf.stop_gradient` instead.\n",
            "          swap_memory: Whether GPU-CPU memory swap is enabled for this loop.\n",
            "          maximum_iterations: Optional maximum number of iterations of the while loop\n",
            "            to run.  If provided, the `cond` output is AND-ed with an additional\n",
            "            condition ensuring the number of iterations executed is no greater than\n",
            "            `maximum_iterations`.\n",
            "          name: Optional name prefix for the returned tensors.\n",
            "        \n",
            "        Returns:\n",
            "          The output tensors for the loop variables after the loop. The return value\n",
            "            has the same structure as `loop_vars`.\n",
            "        \n",
            "        Raises:\n",
            "          TypeError: if `cond` or `body` is not callable.\n",
            "          ValueError: if `loop_vars` is empty.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "        ```python\n",
            "        i = tf.constant(0)\n",
            "        c = lambda i: tf.less(i, 10)\n",
            "        b = lambda i: (tf.add(i, 1), )\n",
            "        r = tf.while_loop(c, b, [i])\n",
            "        ```\n",
            "        \n",
            "        Example with nesting and a namedtuple:\n",
            "        \n",
            "        ```python\n",
            "        import collections\n",
            "        Pair = collections.namedtuple('Pair', 'j, k')\n",
            "        ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))\n",
            "        c = lambda i, p: i < 10\n",
            "        b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))\n",
            "        ijk_final = tf.while_loop(c, b, ijk_0)\n",
            "        ```\n",
            "        \n",
            "        Example using shape_invariants:\n",
            "        \n",
            "        ```python\n",
            "        i0 = tf.constant(0)\n",
            "        m0 = tf.ones([2, 2])\n",
            "        c = lambda i, m: i < 10\n",
            "        b = lambda i, m: [i+1, tf.concat([m, m], axis=0)]\n",
            "        tf.while_loop(\n",
            "            c, b, loop_vars=[i0, m0],\n",
            "            shape_invariants=[i0.get_shape(), tf.TensorShape([None, 2])])\n",
            "        ```\n",
            "        \n",
            "        Example which demonstrates non-strict semantics: In the following\n",
            "        example, the final value of the counter `i` does not depend on `x`. So\n",
            "        the `while_loop` can increment the counter parallel to updates of `x`.\n",
            "        However, because the loop counter at one loop iteration depends\n",
            "        on the value at the previous iteration, the loop counter itself cannot\n",
            "        be incremented in parallel. Hence if we just want the final value of the\n",
            "        counter (which we print on the line `print(sess.run(i))`), then\n",
            "        `x` will never be incremented, but the counter will be updated on a\n",
            "        single thread. Conversely, if we want the value of the output (which we\n",
            "        print on the line `print(sess.run(out).shape)`), then the counter may be\n",
            "        incremented on its own thread, while `x` can be incremented in\n",
            "        parallel on a separate thread. In the extreme case, it is conceivable\n",
            "        that the thread incrementing the counter runs until completion before\n",
            "        `x` is incremented even a single time. The only thing that can never\n",
            "        happen is that the thread updating `x` can never get ahead of the\n",
            "        counter thread because the thread incrementing `x` depends on the value\n",
            "        of the counter.\n",
            "        \n",
            "        ```python\n",
            "        import tensorflow as tf\n",
            "        \n",
            "        n = 10000\n",
            "        x = tf.constant(list(range(n)))\n",
            "        c = lambda i, x: i < n\n",
            "        b = lambda i, x: (tf.compat.v1.Print(i + 1, [i]), tf.compat.v1.Print(x + 1,\n",
            "        [i], \"x:\"))\n",
            "        i, out = tf.while_loop(c, b, (0, x))\n",
            "        with tf.compat.v1.Session() as sess:\n",
            "            print(sess.run(i))  # prints [0] ... [9999]\n",
            "        \n",
            "            # The following line may increment the counter and x in parallel.\n",
            "            # The counter thread may get ahead of the other thread, but not the\n",
            "            # other way around. So you may see things like\n",
            "            # [9996] x:[9987]\n",
            "            # meaning that the counter thread is on iteration 9996,\n",
            "            # while the other thread is on iteration 9987\n",
            "            print(sess.run(out).shape)\n",
            "        ```\n",
            "    \n",
            "    zeros(shape, dtype=tf.float32, name=None)\n",
            "        Creates a tensor with all elements set to zero.\n",
            "        \n",
            "        This operation returns a tensor of type `dtype` with shape `shape` and\n",
            "        all elements set to zero.\n",
            "        \n",
            "        >>> tf.zeros([3, 4], tf.int32)\n",
            "        <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
            "        array([[0, 0, 0, 0],\n",
            "               [0, 0, 0, 0],\n",
            "               [0, 0, 0, 0]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          shape: A `list` of integers, a `tuple` of integers, or\n",
            "            a 1-D `Tensor` of type `int32`.\n",
            "          dtype: The DType of an element in the resulting `Tensor`.\n",
            "          name: Optional string. A name for the operation.\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with all elements set to zero.\n",
            "    \n",
            "    zeros_like = zeros_like_v2(input, dtype=None, name=None)\n",
            "        Creates a tensor with all elements set to zero.\n",
            "        \n",
            "        See also `tf.zeros`.\n",
            "        \n",
            "        Given a single tensor or array-like object (`input`), this operation returns\n",
            "        a tensor of the same type and shape as `input` with all elements set to zero.\n",
            "        Optionally, you can use `dtype` to specify a new type for the returned tensor.\n",
            "        \n",
            "        Examples:\n",
            "        \n",
            "          >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
            "          >>> tf.zeros_like(tensor)\n",
            "          <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "          array([[0, 0, 0],\n",
            "                 [0, 0, 0]], dtype=int32)>\n",
            "        \n",
            "          >>> tf.zeros_like(tensor, dtype=tf.float32)\n",
            "          <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
            "          array([[0., 0., 0.],\n",
            "                 [0., 0., 0.]], dtype=float32)>\n",
            "        \n",
            "          >>> tf.zeros_like([[1, 2, 3], [4, 5, 6]])\n",
            "          <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
            "          array([[0, 0, 0],\n",
            "                 [0, 0, 0]], dtype=int32)>\n",
            "        \n",
            "        Args:\n",
            "          input: A `Tensor` or array-like object.\n",
            "          dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,\n",
            "            `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,\n",
            "            `complex64`, `complex128`, `bool` or `string` (optional).\n",
            "          name: A name for the operation (optional).\n",
            "        \n",
            "        Returns:\n",
            "          A `Tensor` with all elements set to zero.\n",
            "\n",
            "DATA\n",
            "    __all__ = ['AggregationMethod', 'Assert', 'CriticalSection', 'DType', ...\n",
            "    __compiler_version__ = '7.4.0'\n",
            "    __cxx11_abi_flag__ = 0\n",
            "    __git_version__ = 'v2.2.0-0-g2b96f3662b'\n",
            "    __monolithic_build__ = 0\n",
            "    bfloat16 = tf.bfloat16\n",
            "    bool = tf.bool\n",
            "    complex128 = tf.complex128\n",
            "    complex64 = tf.complex64\n",
            "    double = tf.float64\n",
            "    float16 = tf.float16\n",
            "    float32 = tf.float32\n",
            "    float64 = tf.float64\n",
            "    half = tf.float16\n",
            "    int16 = tf.int16\n",
            "    int32 = tf.int32\n",
            "    int64 = tf.int64\n",
            "    int8 = tf.int8\n",
            "    newaxis = None\n",
            "    qint16 = tf.qint16\n",
            "    qint32 = tf.qint32\n",
            "    qint8 = tf.qint8\n",
            "    quint16 = tf.quint16\n",
            "    quint8 = tf.quint8\n",
            "    resource = tf.resource\n",
            "    string = tf.string\n",
            "    uint16 = tf.uint16\n",
            "    uint32 = tf.uint32\n",
            "    uint64 = tf.uint64\n",
            "    uint8 = tf.uint8\n",
            "    variant = tf.variant\n",
            "\n",
            "VERSION\n",
            "    2.2.0\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQehzGIK_d1T",
        "colab_type": "code",
        "outputId": "1e84c477-6dd5-4b7b-c4f7-18399f558767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzosBaOfNRIO",
        "colab_type": "code",
        "outputId": "82706c70-03e2-4851-8bc9-1b6c486e870e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3bFX7bd_jIk",
        "colab_type": "code",
        "outputId": "ed5f6ad4-b905-419a-8040-3bd38d00e932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#import the necessary packages\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "import pydot\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras import layers\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg \n",
        "\n",
        "from IPython.display import SVG\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxpxKtnT_wzt",
        "colab_type": "code",
        "outputId": "9875b95a-644c-476a-f17a-6186385679a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        }
      },
      "source": [
        "#load numerical data using Pandas\n",
        "!pip install -q xlrd\n",
        "!pip install pillow\n",
        "\n",
        "cols = ['author','creation_year','height(inch)','width(inch)','estimate_low($)','estimate_high','auction_year','artist_birth','hammer price', 'artist_rank','artist_points','num_bill','wealth_bill','canvas','paper','Acrylic','oil','Mixed media','image']\n",
        "df = pd.read_excel('result_2009.xlsx', usecols=cols)\n",
        "df = pd.get_dummies(df, prefix=['auth'], columns = ['author'])\n",
        "df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height(inch)</th>\n",
              "      <th>width(inch)</th>\n",
              "      <th>estimate_low($)</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>hammer price</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>image</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>artist_rank</th>\n",
              "      <th>artist_points</th>\n",
              "      <th>num_bill</th>\n",
              "      <th>wealth_bill</th>\n",
              "      <th>canvas</th>\n",
              "      <th>paper</th>\n",
              "      <th>Acrylic</th>\n",
              "      <th>Mixed media</th>\n",
              "      <th>oil</th>\n",
              "      <th>auth_Ai Weiwei</th>\n",
              "      <th>auth_Alberto Giacometti</th>\n",
              "      <th>auth_Alex Katz</th>\n",
              "      <th>auth_Alexander Calder</th>\n",
              "      <th>auth_Alighiero Boëtti</th>\n",
              "      <th>auth_Andy Warhol</th>\n",
              "      <th>auth_Anselm Kiefer</th>\n",
              "      <th>auth_Antoni Tàpies</th>\n",
              "      <th>auth_Arnulf Rainer</th>\n",
              "      <th>auth_Carl Andre</th>\n",
              "      <th>auth_Christian Boltanski</th>\n",
              "      <th>auth_Christian Marclay</th>\n",
              "      <th>auth_Cindy Sherman</th>\n",
              "      <th>auth_Claes Oldenburg</th>\n",
              "      <th>auth_Cy Twombly</th>\n",
              "      <th>auth_Damien Hirst</th>\n",
              "      <th>auth_Dan Graham</th>\n",
              "      <th>auth_Daniel Buren</th>\n",
              "      <th>auth_David Hockney</th>\n",
              "      <th>auth_Dieter Roth</th>\n",
              "      <th>auth_Douglas Gordon</th>\n",
              "      <th>auth_Ed Ruscha</th>\n",
              "      <th>...</th>\n",
              "      <th>auth_Joseph Beuys</th>\n",
              "      <th>auth_Kader Attia</th>\n",
              "      <th>auth_Kiki Smith</th>\n",
              "      <th>auth_Lawrence Weiner</th>\n",
              "      <th>auth_Louise Bourgeois</th>\n",
              "      <th>auth_Lucio Fontana</th>\n",
              "      <th>auth_Man Ray</th>\n",
              "      <th>auth_Marcel Broodthaers</th>\n",
              "      <th>auth_Marcel Duchamp</th>\n",
              "      <th>auth_Maria Lassnig</th>\n",
              "      <th>auth_Marina Abramovic</th>\n",
              "      <th>auth_Marlene Dumas</th>\n",
              "      <th>auth_Martin Kippenberger</th>\n",
              "      <th>auth_Max Ernst</th>\n",
              "      <th>auth_Mike Kelley</th>\n",
              "      <th>auth_Mona Hatoum</th>\n",
              "      <th>auth_Nam June Paik</th>\n",
              "      <th>auth_Olafur Eliasson</th>\n",
              "      <th>auth_Pablo Picasso</th>\n",
              "      <th>auth_Paul Klee</th>\n",
              "      <th>auth_Paul McCarthy</th>\n",
              "      <th>auth_Pierre Huyghe</th>\n",
              "      <th>auth_Richard Long</th>\n",
              "      <th>auth_Richard Prince</th>\n",
              "      <th>auth_Richard Serra</th>\n",
              "      <th>auth_Rirkrit Tiravanija</th>\n",
              "      <th>auth_Robert Mapplethorpe</th>\n",
              "      <th>auth_Robert Rauschenberg</th>\n",
              "      <th>auth_Rosemarie Trockel</th>\n",
              "      <th>auth_Roy Lichtenstein</th>\n",
              "      <th>auth_Sigmar Polke</th>\n",
              "      <th>auth_Sol LeWitt</th>\n",
              "      <th>auth_Tacita Dean</th>\n",
              "      <th>auth_Thomas Ruff</th>\n",
              "      <th>auth_Thomas Schütte</th>\n",
              "      <th>auth_Tony Cragg</th>\n",
              "      <th>auth_Valie Export</th>\n",
              "      <th>auth_William Kentridge</th>\n",
              "      <th>auth_Yayoi Kusama</th>\n",
              "      <th>auth_Yoko Ono</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1964.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>150000.0</td>\n",
              "      <td>200000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Flowers-1-0.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1964.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1150000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Flowers-1-1.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Self-Defense (Positive)-1-2...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Self-Defense (Negative)-1-3...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1983.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Untitled (Four)-1-4.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14402</th>\n",
              "      <td>1999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>149320.0</td>\n",
              "      <td>223980.0</td>\n",
              "      <td>313572.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-Dancer-3-13.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14403</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>218820.0</td>\n",
              "      <td>281340.0</td>\n",
              "      <td>484530.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-\"The Peeping Tom \"-3-14.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14404</th>\n",
              "      <td>1995.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>250000.0</td>\n",
              "      <td>350000.0</td>\n",
              "      <td>440000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Evil Eye-3-15.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14405</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Transparent Slip-3-16.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14406</th>\n",
              "      <td>1986.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>images/Pierre Huyghe-Sans titre double-face-1-...</td>\n",
              "      <td>1962.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>12992.24</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14407 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       creation_year  height(inch)  ...  auth_Yayoi Kusama  auth_Yoko Ono\n",
              "0             1964.0           5.0  ...                  0              0\n",
              "1             1964.0          22.0  ...                  0              0\n",
              "2             1985.0          20.0  ...                  0              0\n",
              "3             1985.0          20.0  ...                  0              0\n",
              "4             1983.0          20.0  ...                  0              0\n",
              "...              ...           ...  ...                ...            ...\n",
              "14402         1999.0           2.0  ...                  0              0\n",
              "14403         1994.0           2.0  ...                  0              0\n",
              "14404         1995.0          79.0  ...                  0              0\n",
              "14405         1996.0          49.0  ...                  0              0\n",
              "14406         1986.0          39.0  ...                  0              0\n",
              "\n",
              "[14407 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cRDwRTDeeQ1",
        "colab_type": "code",
        "outputId": "26bc7b35-dbf0-4b64-c3f7-cc8d8a4d07d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "#drop rows containing missing data\n",
        "ccols = ['creation_year','height(inch)','width(inch)','estimate_low($)','estimate_high','auction_year','artist_birth','hammer price', 'artist_rank','artist_points','num_bill','wealth_bill','canvas','paper','Acrylic','oil','Mixed media']\n",
        "df[ccols] = df[ccols].apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height(inch)</th>\n",
              "      <th>width(inch)</th>\n",
              "      <th>estimate_low($)</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>hammer price</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>image</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>artist_rank</th>\n",
              "      <th>artist_points</th>\n",
              "      <th>num_bill</th>\n",
              "      <th>wealth_bill</th>\n",
              "      <th>canvas</th>\n",
              "      <th>paper</th>\n",
              "      <th>Acrylic</th>\n",
              "      <th>Mixed media</th>\n",
              "      <th>oil</th>\n",
              "      <th>auth_Ai Weiwei</th>\n",
              "      <th>auth_Alberto Giacometti</th>\n",
              "      <th>auth_Alex Katz</th>\n",
              "      <th>auth_Alexander Calder</th>\n",
              "      <th>auth_Alighiero Boëtti</th>\n",
              "      <th>auth_Andy Warhol</th>\n",
              "      <th>auth_Anselm Kiefer</th>\n",
              "      <th>auth_Antoni Tàpies</th>\n",
              "      <th>auth_Arnulf Rainer</th>\n",
              "      <th>auth_Carl Andre</th>\n",
              "      <th>auth_Christian Boltanski</th>\n",
              "      <th>auth_Christian Marclay</th>\n",
              "      <th>auth_Cindy Sherman</th>\n",
              "      <th>auth_Claes Oldenburg</th>\n",
              "      <th>auth_Cy Twombly</th>\n",
              "      <th>auth_Damien Hirst</th>\n",
              "      <th>auth_Dan Graham</th>\n",
              "      <th>auth_Daniel Buren</th>\n",
              "      <th>auth_David Hockney</th>\n",
              "      <th>auth_Dieter Roth</th>\n",
              "      <th>auth_Douglas Gordon</th>\n",
              "      <th>auth_Ed Ruscha</th>\n",
              "      <th>...</th>\n",
              "      <th>auth_Joseph Beuys</th>\n",
              "      <th>auth_Kader Attia</th>\n",
              "      <th>auth_Kiki Smith</th>\n",
              "      <th>auth_Lawrence Weiner</th>\n",
              "      <th>auth_Louise Bourgeois</th>\n",
              "      <th>auth_Lucio Fontana</th>\n",
              "      <th>auth_Man Ray</th>\n",
              "      <th>auth_Marcel Broodthaers</th>\n",
              "      <th>auth_Marcel Duchamp</th>\n",
              "      <th>auth_Maria Lassnig</th>\n",
              "      <th>auth_Marina Abramovic</th>\n",
              "      <th>auth_Marlene Dumas</th>\n",
              "      <th>auth_Martin Kippenberger</th>\n",
              "      <th>auth_Max Ernst</th>\n",
              "      <th>auth_Mike Kelley</th>\n",
              "      <th>auth_Mona Hatoum</th>\n",
              "      <th>auth_Nam June Paik</th>\n",
              "      <th>auth_Olafur Eliasson</th>\n",
              "      <th>auth_Pablo Picasso</th>\n",
              "      <th>auth_Paul Klee</th>\n",
              "      <th>auth_Paul McCarthy</th>\n",
              "      <th>auth_Pierre Huyghe</th>\n",
              "      <th>auth_Richard Long</th>\n",
              "      <th>auth_Richard Prince</th>\n",
              "      <th>auth_Richard Serra</th>\n",
              "      <th>auth_Rirkrit Tiravanija</th>\n",
              "      <th>auth_Robert Mapplethorpe</th>\n",
              "      <th>auth_Robert Rauschenberg</th>\n",
              "      <th>auth_Rosemarie Trockel</th>\n",
              "      <th>auth_Roy Lichtenstein</th>\n",
              "      <th>auth_Sigmar Polke</th>\n",
              "      <th>auth_Sol LeWitt</th>\n",
              "      <th>auth_Tacita Dean</th>\n",
              "      <th>auth_Thomas Ruff</th>\n",
              "      <th>auth_Thomas Schütte</th>\n",
              "      <th>auth_Tony Cragg</th>\n",
              "      <th>auth_Valie Export</th>\n",
              "      <th>auth_William Kentridge</th>\n",
              "      <th>auth_Yayoi Kusama</th>\n",
              "      <th>auth_Yoko Ono</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1964.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1150000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Flowers-1-1.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Self-Defense (Negative)-1-3...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1983.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Untitled (Four)-1-4.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1976.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Torso-1-5.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>72000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Are You \"Different?\" (Posit...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14699</th>\n",
              "      <td>1999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>149320.0</td>\n",
              "      <td>223980.0</td>\n",
              "      <td>313572.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-Dancer-3-13.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14700</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>218820.0</td>\n",
              "      <td>281340.0</td>\n",
              "      <td>484530.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-\"The Peeping Tom \"-3-14.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14701</th>\n",
              "      <td>1995.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>250000.0</td>\n",
              "      <td>350000.0</td>\n",
              "      <td>440000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Evil Eye-3-15.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14702</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Transparent Slip-3-16.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14703</th>\n",
              "      <td>1986.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>images/Pierre Huyghe-Sans titre double-face-1-...</td>\n",
              "      <td>1962.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>12992.24</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8559 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       creation_year  height(inch)  ...  auth_Yayoi Kusama  auth_Yoko Ono\n",
              "1             1964.0          22.0  ...                  0              0\n",
              "3             1985.0          20.0  ...                  0              0\n",
              "4             1983.0          20.0  ...                  0              0\n",
              "5             1976.0           8.0  ...                  0              0\n",
              "6             1985.0           8.0  ...                  0              0\n",
              "...              ...           ...  ...                ...            ...\n",
              "14699         1999.0           2.0  ...                  0              0\n",
              "14700         1994.0           2.0  ...                  0              0\n",
              "14701         1995.0          79.0  ...                  0              0\n",
              "14702         1996.0          49.0  ...                  0              0\n",
              "14703         1986.0          39.0  ...                  0              0\n",
              "\n",
              "[8559 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIhK4kiewuOx",
        "colab_type": "code",
        "outputId": "782a32da-703d-417b-9bba-d979281df0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "# Pre-processing array to remove problematic rows. \n",
        "# images/François Morellet-Ligne droite-1-13.jpg\n",
        "\n",
        "#df[df['image'].str.contains('Morellet-Ligne droite-1-13')]\n",
        "df = df[~df['image'].str.contains(\"Morellet-Ligne droite-1-13\")]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height(inch)</th>\n",
              "      <th>width(inch)</th>\n",
              "      <th>estimate_low($)</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>hammer price</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>image</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>artist_rank</th>\n",
              "      <th>artist_points</th>\n",
              "      <th>num_bill</th>\n",
              "      <th>wealth_bill</th>\n",
              "      <th>canvas</th>\n",
              "      <th>paper</th>\n",
              "      <th>Acrylic</th>\n",
              "      <th>Mixed media</th>\n",
              "      <th>oil</th>\n",
              "      <th>auth_Ai Weiwei</th>\n",
              "      <th>auth_Alberto Giacometti</th>\n",
              "      <th>auth_Alex Katz</th>\n",
              "      <th>auth_Alexander Calder</th>\n",
              "      <th>auth_Alighiero Boëtti</th>\n",
              "      <th>auth_Andy Warhol</th>\n",
              "      <th>auth_Anselm Kiefer</th>\n",
              "      <th>auth_Antoni Tàpies</th>\n",
              "      <th>auth_Arnulf Rainer</th>\n",
              "      <th>auth_Carl Andre</th>\n",
              "      <th>auth_Christian Boltanski</th>\n",
              "      <th>auth_Christian Marclay</th>\n",
              "      <th>auth_Cindy Sherman</th>\n",
              "      <th>auth_Claes Oldenburg</th>\n",
              "      <th>auth_Cy Twombly</th>\n",
              "      <th>auth_Damien Hirst</th>\n",
              "      <th>auth_Dan Graham</th>\n",
              "      <th>auth_Daniel Buren</th>\n",
              "      <th>auth_David Hockney</th>\n",
              "      <th>auth_Dieter Roth</th>\n",
              "      <th>auth_Douglas Gordon</th>\n",
              "      <th>auth_Ed Ruscha</th>\n",
              "      <th>...</th>\n",
              "      <th>auth_Joseph Beuys</th>\n",
              "      <th>auth_Kader Attia</th>\n",
              "      <th>auth_Kiki Smith</th>\n",
              "      <th>auth_Lawrence Weiner</th>\n",
              "      <th>auth_Louise Bourgeois</th>\n",
              "      <th>auth_Lucio Fontana</th>\n",
              "      <th>auth_Man Ray</th>\n",
              "      <th>auth_Marcel Broodthaers</th>\n",
              "      <th>auth_Marcel Duchamp</th>\n",
              "      <th>auth_Maria Lassnig</th>\n",
              "      <th>auth_Marina Abramovic</th>\n",
              "      <th>auth_Marlene Dumas</th>\n",
              "      <th>auth_Martin Kippenberger</th>\n",
              "      <th>auth_Max Ernst</th>\n",
              "      <th>auth_Mike Kelley</th>\n",
              "      <th>auth_Mona Hatoum</th>\n",
              "      <th>auth_Nam June Paik</th>\n",
              "      <th>auth_Olafur Eliasson</th>\n",
              "      <th>auth_Pablo Picasso</th>\n",
              "      <th>auth_Paul Klee</th>\n",
              "      <th>auth_Paul McCarthy</th>\n",
              "      <th>auth_Pierre Huyghe</th>\n",
              "      <th>auth_Richard Long</th>\n",
              "      <th>auth_Richard Prince</th>\n",
              "      <th>auth_Richard Serra</th>\n",
              "      <th>auth_Rirkrit Tiravanija</th>\n",
              "      <th>auth_Robert Mapplethorpe</th>\n",
              "      <th>auth_Robert Rauschenberg</th>\n",
              "      <th>auth_Rosemarie Trockel</th>\n",
              "      <th>auth_Roy Lichtenstein</th>\n",
              "      <th>auth_Sigmar Polke</th>\n",
              "      <th>auth_Sol LeWitt</th>\n",
              "      <th>auth_Tacita Dean</th>\n",
              "      <th>auth_Thomas Ruff</th>\n",
              "      <th>auth_Thomas Schütte</th>\n",
              "      <th>auth_Tony Cragg</th>\n",
              "      <th>auth_Valie Export</th>\n",
              "      <th>auth_William Kentridge</th>\n",
              "      <th>auth_Yayoi Kusama</th>\n",
              "      <th>auth_Yoko Ono</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1964.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1150000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Flowers-1-1.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Self-Defense (Negative)-1-3...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1983.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Untitled (Four)-1-4.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1976.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Torso-1-5.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>72000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Are You \"Different?\" (Posit...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14699</th>\n",
              "      <td>1999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>149320.0</td>\n",
              "      <td>223980.0</td>\n",
              "      <td>313572.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-Dancer-3-13.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14700</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>218820.0</td>\n",
              "      <td>281340.0</td>\n",
              "      <td>484530.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-\"The Peeping Tom \"-3-14.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14701</th>\n",
              "      <td>1995.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>250000.0</td>\n",
              "      <td>350000.0</td>\n",
              "      <td>440000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Evil Eye-3-15.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14702</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Transparent Slip-3-16.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14703</th>\n",
              "      <td>1986.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>1077.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>images/Pierre Huyghe-Sans titre double-face-1-...</td>\n",
              "      <td>1962.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>12992.24</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8558 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       creation_year  height(inch)  ...  auth_Yayoi Kusama  auth_Yoko Ono\n",
              "1             1964.0          22.0  ...                  0              0\n",
              "3             1985.0          20.0  ...                  0              0\n",
              "4             1983.0          20.0  ...                  0              0\n",
              "5             1976.0           8.0  ...                  0              0\n",
              "6             1985.0           8.0  ...                  0              0\n",
              "...              ...           ...  ...                ...            ...\n",
              "14699         1999.0           2.0  ...                  0              0\n",
              "14700         1994.0           2.0  ...                  0              0\n",
              "14701         1995.0          79.0  ...                  0              0\n",
              "14702         1996.0          49.0  ...                  0              0\n",
              "14703         1986.0          39.0  ...                  0              0\n",
              "\n",
              "[8558 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRER55fhXrt_",
        "colab_type": "code",
        "outputId": "29291a26-dfe6-43cd-ba60-88235f9aeda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "#check distribution of hammer price\n",
        "df.hist(column='hammer price', bins=100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f79295c8668>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xy4E5YFWhSt",
        "colab_type": "code",
        "outputId": "82211fe5-8358-4be8-a4de-65062e2f2071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "#create log hammer price\n",
        "df['log hammer price'] = np.log10(df['hammer price'])\n",
        "df.hist(column='log hammer price', bins=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f7928f81630>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7zDpOLLyzUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop 'hammer price' column\n",
        "df = df.drop(columns='hammer price')\n",
        "df['image'].replace({'\\n':' '},inplace=True,regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATx94FXNWhaV",
        "colab_type": "code",
        "outputId": "e5be23b8-bb14-4e30-a372-d7ffa6fd931b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#bin log hammer price\n",
        "bins = [0,3,3.25,3.5, 3.75, 4,4.25,4.5,4.75, 5,5.25, 5.5,5.75,6,6.25,6.5,6.75,7,7.25,7.5,7.75,8]\n",
        "labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "df['binned'] = pd.cut(df['log hammer price'], bins=bins, labels=labels)\n",
        "print(df)\n",
        "df.shape\n",
        "len(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       creation_year  height(inch)  ...  log hammer price  binned\n",
            "1             1964.0          22.0  ...          6.060698      13\n",
            "3             1985.0          20.0  ...          4.698970       7\n",
            "4             1983.0          20.0  ...          4.845098       8\n",
            "5             1976.0           8.0  ...          4.845098       8\n",
            "6             1985.0           8.0  ...          4.857332       8\n",
            "...              ...           ...  ...               ...     ...\n",
            "14699         1999.0           2.0  ...          5.496337      10\n",
            "14700         1994.0           2.0  ...          5.685321      11\n",
            "14701         1995.0          79.0  ...          5.643453      11\n",
            "14702         1996.0          49.0  ...          4.778151       8\n",
            "14703         1986.0          39.0  ...          3.032216       1\n",
            "\n",
            "[8558 rows x 107 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8NLUKrGLIjK",
        "colab_type": "code",
        "outputId": "87b10dd1-b4d3-45cf-e6e1-515f8226dd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>creation_year</th>\n",
              "      <th>height(inch)</th>\n",
              "      <th>width(inch)</th>\n",
              "      <th>estimate_low($)</th>\n",
              "      <th>estimate_high</th>\n",
              "      <th>auction_year</th>\n",
              "      <th>image</th>\n",
              "      <th>artist_birth</th>\n",
              "      <th>artist_rank</th>\n",
              "      <th>artist_points</th>\n",
              "      <th>num_bill</th>\n",
              "      <th>wealth_bill</th>\n",
              "      <th>canvas</th>\n",
              "      <th>paper</th>\n",
              "      <th>Acrylic</th>\n",
              "      <th>Mixed media</th>\n",
              "      <th>oil</th>\n",
              "      <th>auth_Ai Weiwei</th>\n",
              "      <th>auth_Alberto Giacometti</th>\n",
              "      <th>auth_Alex Katz</th>\n",
              "      <th>auth_Alexander Calder</th>\n",
              "      <th>auth_Alighiero Boëtti</th>\n",
              "      <th>auth_Andy Warhol</th>\n",
              "      <th>auth_Anselm Kiefer</th>\n",
              "      <th>auth_Antoni Tàpies</th>\n",
              "      <th>auth_Arnulf Rainer</th>\n",
              "      <th>auth_Carl Andre</th>\n",
              "      <th>auth_Christian Boltanski</th>\n",
              "      <th>auth_Christian Marclay</th>\n",
              "      <th>auth_Cindy Sherman</th>\n",
              "      <th>auth_Claes Oldenburg</th>\n",
              "      <th>auth_Cy Twombly</th>\n",
              "      <th>auth_Damien Hirst</th>\n",
              "      <th>auth_Dan Graham</th>\n",
              "      <th>auth_Daniel Buren</th>\n",
              "      <th>auth_David Hockney</th>\n",
              "      <th>auth_Dieter Roth</th>\n",
              "      <th>auth_Douglas Gordon</th>\n",
              "      <th>auth_Ed Ruscha</th>\n",
              "      <th>auth_Erwin Wurm</th>\n",
              "      <th>...</th>\n",
              "      <th>auth_Kiki Smith</th>\n",
              "      <th>auth_Lawrence Weiner</th>\n",
              "      <th>auth_Louise Bourgeois</th>\n",
              "      <th>auth_Lucio Fontana</th>\n",
              "      <th>auth_Man Ray</th>\n",
              "      <th>auth_Marcel Broodthaers</th>\n",
              "      <th>auth_Marcel Duchamp</th>\n",
              "      <th>auth_Maria Lassnig</th>\n",
              "      <th>auth_Marina Abramovic</th>\n",
              "      <th>auth_Marlene Dumas</th>\n",
              "      <th>auth_Martin Kippenberger</th>\n",
              "      <th>auth_Max Ernst</th>\n",
              "      <th>auth_Mike Kelley</th>\n",
              "      <th>auth_Mona Hatoum</th>\n",
              "      <th>auth_Nam June Paik</th>\n",
              "      <th>auth_Olafur Eliasson</th>\n",
              "      <th>auth_Pablo Picasso</th>\n",
              "      <th>auth_Paul Klee</th>\n",
              "      <th>auth_Paul McCarthy</th>\n",
              "      <th>auth_Pierre Huyghe</th>\n",
              "      <th>auth_Richard Long</th>\n",
              "      <th>auth_Richard Prince</th>\n",
              "      <th>auth_Richard Serra</th>\n",
              "      <th>auth_Rirkrit Tiravanija</th>\n",
              "      <th>auth_Robert Mapplethorpe</th>\n",
              "      <th>auth_Robert Rauschenberg</th>\n",
              "      <th>auth_Rosemarie Trockel</th>\n",
              "      <th>auth_Roy Lichtenstein</th>\n",
              "      <th>auth_Sigmar Polke</th>\n",
              "      <th>auth_Sol LeWitt</th>\n",
              "      <th>auth_Tacita Dean</th>\n",
              "      <th>auth_Thomas Ruff</th>\n",
              "      <th>auth_Thomas Schütte</th>\n",
              "      <th>auth_Tony Cragg</th>\n",
              "      <th>auth_Valie Export</th>\n",
              "      <th>auth_William Kentridge</th>\n",
              "      <th>auth_Yayoi Kusama</th>\n",
              "      <th>auth_Yoko Ono</th>\n",
              "      <th>log hammer price</th>\n",
              "      <th>binned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1964.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Flowers-1-1.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.060698</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Self-Defense (Negative)-1-3...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.698970</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1983.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Untitled (Four)-1-4.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.845098</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1976.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Torso-1-5.jpg</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.845098</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1985.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>images/Andy Warhol-Are You \"Different?\" (Posit...</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>51496.69</td>\n",
              "      <td>2153.0</td>\n",
              "      <td>8.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.857332</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14699</th>\n",
              "      <td>1999.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>149320.0</td>\n",
              "      <td>223980.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-Dancer-3-13.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.496337</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14700</th>\n",
              "      <td>1994.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>218820.0</td>\n",
              "      <td>281340.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>images/Marlene Dumas-\"The Peeping Tom \"-3-14.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>14262.10</td>\n",
              "      <td>1011.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.685321</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14701</th>\n",
              "      <td>1995.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>250000.0</td>\n",
              "      <td>350000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Evil Eye-3-15.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.643453</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14702</th>\n",
              "      <td>1996.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>70000.0</td>\n",
              "      <td>100000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>images/Marlene Dumas-Transparent Slip-3-16.jpg</td>\n",
              "      <td>1953.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>14765.08</td>\n",
              "      <td>793.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.778151</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14703</th>\n",
              "      <td>1986.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>1700.0</td>\n",
              "      <td>2018.0</td>\n",
              "      <td>images/Pierre Huyghe-Sans titre double-face-1-...</td>\n",
              "      <td>1962.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>12992.24</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.032216</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8558 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       creation_year  height(inch)  ...  log hammer price  binned\n",
              "1             1964.0          22.0  ...          6.060698      13\n",
              "3             1985.0          20.0  ...          4.698970       7\n",
              "4             1983.0          20.0  ...          4.845098       8\n",
              "5             1976.0           8.0  ...          4.845098       8\n",
              "6             1985.0           8.0  ...          4.857332       8\n",
              "...              ...           ...  ...               ...     ...\n",
              "14699         1999.0           2.0  ...          5.496337      10\n",
              "14700         1994.0           2.0  ...          5.685321      11\n",
              "14701         1995.0          79.0  ...          5.643453      11\n",
              "14702         1996.0          49.0  ...          4.778151       8\n",
              "14703         1986.0          39.0  ...          3.032216       1\n",
              "\n",
              "[8558 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfu5j_zDs2ga",
        "colab_type": "code",
        "outputId": "12a3e3ce-605e-4a84-b1c4-a6573642ddb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#########################\n",
        "# 2018 and beyond X / Y #\n",
        "#########################\n",
        "\n",
        "dataset = df.values\n",
        "\n",
        "# Shuffle rows to make dev vs testing split easier later \n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Train set from < 2018 data\n",
        "# Dev and Test set from >= 2018 data\n",
        "\n",
        "df_train = df[df['auction_year'] < 2018.]\n",
        "df_devtest = df[df['auction_year'] >= 2018.]\n",
        "\n",
        "print(\"df_train\",df_train.shape)\n",
        "print(\"df_devtest\",df_devtest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_train (7105, 107)\n",
            "df_devtest (1453, 107)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNTqYPu3fEYc",
        "colab_type": "code",
        "outputId": "88572773-eb20-45e9-f9da-2329672eaa4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# Making training/testing/evaluation for image/numeric\n",
        "\n",
        "## this part for Images and Y value \n",
        "dataset_train = df_train.values\n",
        "X_image_train = dataset_train[:,6]\n",
        "Y_train = dataset_train[:,106]\n",
        "\n",
        "dataset_devtest = df_devtest.values\n",
        "X_image_devtest = dataset_devtest[:,6]\n",
        "Y_devtest = dataset_devtest[:,106]\n",
        "\n",
        "## this part for numerical data ##\n",
        "dataset_train=df_train.drop(columns=['image','log hammer price','binned'])\n",
        "dataset_devtest=df_devtest.drop(columns=['image','log hammer price','binned'])\n",
        "\n",
        "X_numeric_train = dataset_train.values\n",
        "X_numeric_devtest = dataset_devtest.values\n",
        "\n",
        "## split dev set and test set \n",
        "\n",
        "X_image_dev = X_image_devtest[:725]\n",
        "X_image_test = X_image_devtest[725:]\n",
        "Y_dev = Y_devtest[:725]\n",
        "Y_test = Y_devtest[725:]\n",
        "X_numeric_dev = X_numeric_devtest[:725]\n",
        "X_numeric_test = X_numeric_devtest[725:]\n",
        "\n",
        "## final check \n",
        "print (\"X_image_train\", X_image_train.shape)\n",
        "print (\"X_numeric_train\", X_numeric_train.shape)\n",
        "print (\"Y_train\", Y_train.shape)\n",
        "\n",
        "#print(type(X_image_tt))\n",
        "print (\"X_image_dev\", X_image_dev.shape)\n",
        "print (\"X_image_test\", X_image_test.shape)\n",
        "print (\"X_numeric_dev\", X_numeric_dev.shape)\n",
        "print (\"X_numeric_test\", X_numeric_test.shape)\n",
        "print (\"Y_dev\", Y_dev.shape)\n",
        "print (\"Y_test\", Y_test.shape)\n",
        "\n",
        "#print(type(Y_tt))\n",
        "X_numeric_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_image_train (7105,)\n",
            "X_numeric_train (7105, 104)\n",
            "Y_train (7105,)\n",
            "X_image_dev (725,)\n",
            "X_image_test (728,)\n",
            "X_numeric_dev (725, 104)\n",
            "X_numeric_test (728, 104)\n",
            "Y_dev (725,)\n",
            "Y_test (728,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.993e+03, 2.000e+00, 7.000e+00, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       [1.995e+03, 4.000e+00, 8.000e+00, ..., 0.000e+00, 1.000e+00,\n",
              "        0.000e+00],\n",
              "       [2.005e+03, 8.000e+00, 1.400e+01, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       ...,\n",
              "       [1.979e+03, 5.200e+01, 6.800e+01, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       [1.976e+03, 5.400e+01, 7.400e+01, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00],\n",
              "       [1.981e+03, 4.000e+01, 4.000e+01, ..., 0.000e+00, 0.000e+00,\n",
              "        0.000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sYYHv8aw12x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Y train, dev, test set in log hammer price\n",
        "Y_train_loghammerprice=df_train['log hammer price']\n",
        "Y_devtest_loghammerprice=df_devtest['log hammer price']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3qGDzjAxuOL",
        "colab_type": "code",
        "outputId": "b782e7e8-a1de-4ee2-d3ee-afad1f60eee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y_dev_loghammerprice = Y_devtest_loghammerprice[:725]\n",
        "Y_test_loghammerprice = Y_devtest_loghammerprice[725:]\n",
        "Y_train_loghammerprice"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11730    3.774517\n",
              "1831     4.698970\n",
              "4856     4.399206\n",
              "13725    4.130623\n",
              "13584    5.232559\n",
              "           ...   \n",
              "8001     5.490717\n",
              "7146     3.849051\n",
              "12529    5.612784\n",
              "14339    4.527230\n",
              "9183     4.026165\n",
              "Name: log hammer price, Length: 7105, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW71jXL5bEWR",
        "colab_type": "code",
        "outputId": "aff224db-6b3b-4953-9637-419f5883b077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## skip if using the whole dataset ##\n",
        "#image_number_start = 3000\n",
        "#image_number_end = 3400\n",
        "\n",
        "#load_image = X_image_train[image_number_start:image_number_end]\n",
        "#load_numeric = X_numeric_train[image_number_start:image_number_end]\n",
        "#load_bin = Y_train[image_number_start:image_number_end]\n",
        "\n",
        "#load_image_test = X_image_test[image_number_start:image_number_end]\n",
        "#load_numeric_test = X_numeric_test[image_number_start:image_number_end]\n",
        "#load_bin_test = Y_test[image_number_start:image_number_end]\n",
        "\n",
        "  ## load_image\n",
        "  #problem_image = 'images/Andy Warhol-Heaven and Hell Are Just One Breath Away (Positive and Negati                 ...-28-20.jpg' #X_image[510]\n",
        "  #print(problem_image)\n",
        "\n",
        "  # Output Images \n",
        " # print(\"test image\")\n",
        "#imgg = mpimg.imread(load_image[142]) \n",
        " # plt.imshow(imgg) \n",
        "\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfVWeJI8vyay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "## CNN ##########################\n",
        "#################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohwUM9zw6Wii",
        "colab_type": "code",
        "outputId": "f234cb3d-61f7-4b67-a73e-232ec2ff9807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'2013_Korean_Tax_Organizer(Outbound)(Kr).doc'\n",
            "'2018-3-23 2.WIRE ESCROW INFORMATION(D).JPG'\n",
            " CAM00001.jpg\n",
            " CAM00002.jpg\n",
            " CAM00003.jpg\n",
            " CAM00006.jpg\n",
            " CAM00009.jpg\n",
            " CAM00010.jpg\n",
            " CAM00011.jpg\n",
            " CAM00012.jpg\n",
            " CAM00013.jpg\n",
            " CAM00014.jpg\n",
            " CAM00015.jpg\n",
            " CAM00016.jpg\n",
            " CAM00017.jpg\n",
            " CAM00019.jpg\n",
            " CAM00021.jpg\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "'CV_AnitaJwa copy.doc'\n",
            "'FINAL SURVEY.pdf'\n",
            "'[GoHenry] Bass Lake - Yosemite.gmap'\n",
            "'[GoHenry] Bayland National Park.gmap'\n",
            "'[GoHenry] Big Sur.gmap'\n",
            "'[GoHenry] Durante'\\''s Tavern.gmap'\n",
            "'[GoHenry] Golden Gate Bridge View.gmap'\n",
            "'[GoHenry] Great Places in SF.gmap'\n",
            "'[GoHenry] Lighthouse Hostel.gmap'\n",
            "'[GoHenry] Muir Woods .gmap'\n",
            "'[GoHenry] Night view of SF city.gmap'\n",
            "'[GoHenry] Sausalito.gmap'\n",
            " Henry2009_all.doc\n",
            "'How Google Makes Money.gslides'\n",
            "'< I always liked to do the following >.gdoc'\n",
            " image3.jpeg\n",
            " \u001b[01;34mimages\u001b[0m/\n",
            "'법률저널 광고.jpg'\n",
            "'Korea Trip 1.gdoc'\n",
            "'Law Offices of E.J. Hong - Checklist_01.gdoc'\n",
            "'Law Offices of E.J. Hong - Checklist_01.pdf'\n",
            " \u001b[01;34mlogs\u001b[0m/\n",
            " message_zdm.html\n",
            " Oct.zip\n",
            " result_2009.xlsx\n",
            " result_2017_2018.xlsx\n",
            " result.xlsx\n",
            "'Stanford Graduate School of Busi.gdoc'\n",
            "'Thoughts on 7 1 2010.gdoc'\n",
            " UmbrellaRenew2018-2019.pdf\n",
            "'Untitled spreadsheet.gsheet'\n",
            "'Wen Meeting - This Friday Lunch.gdoc'\n",
            " X_image_devarray.npy\n",
            " X_image_testarray.npy\n",
            " X_image_trainarray.npy\n",
            " X_numeric_dev.npy\n",
            " X_numeric_test.npy\n",
            " X_numeric_train.npy\n",
            " Y_dev_loghammerprice.npy\n",
            " Y_test_loghammerprice.npy\n",
            " Y_train_loghammerprice.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iamDkN3Kh44I",
        "colab_type": "code",
        "outputId": "94790b32-f7ae-41d0-ed23-520675fcf880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "################################\n",
        "## Batch pre processing block ##\n",
        "## You do it only once - #######\n",
        "################################\n",
        "\"\"\"\n",
        "import cv2\n",
        "import glob\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import save_img\n",
        "\n",
        "filenames = [img for img in glob.glob(\"images/*.jpg\")]\n",
        "\n",
        "images = []\n",
        "\n",
        "for img in filenames:\n",
        "    print(\"images/reduced2\"+img)\n",
        "    n = cv2.imread(img)\n",
        "    n2 = cv2.resize(n,(299,299))\n",
        "    status = cv2.imwrite(\"images/reduced\"+img , n2)\n",
        "    print(\"Image writtedn to file-system : \", status)\n",
        "    \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport cv2\\nimport glob\\nfrom PIL import Image\\nfrom skimage.transform import resize\\nfrom keras.preprocessing import image\\nfrom keras.preprocessing.image import save_img\\n\\nfilenames = [img for img in glob.glob(\"images/*.jpg\")]\\n\\nimages = []\\n\\nfor img in filenames:\\n    print(\"images/reduced2\"+img)\\n    n = cv2.imread(img)\\n    n2 = cv2.resize(n,(299,299))\\n    status = cv2.imwrite(\"images/reduced\"+img , n2)\\n    print(\"Image writtedn to file-system : \", status)\\n    \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rG_IKBnakxV",
        "colab_type": "code",
        "outputId": "b5a7ea4d-bbdd-481c-d8a2-36a871029d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "# Test section\n",
        "\n",
        "#from PIL import Image\n",
        "#from skimage.transform import resize\n",
        "#from keras.preprocessing import image\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import save_img\n",
        "\n",
        "X_image_trainarray = []\n",
        "count = 0\n",
        "\n",
        "for f in X_image_train:\n",
        "    print(count, len(X_image_train), f)\n",
        "    m = cv2.imread(\"images/reduced\"+f)\n",
        "    m = image.img_to_array(m)\n",
        "    X_image_trainarray.append(m)\n",
        "    count += 1\n",
        "\n",
        "\n",
        "#x_Image_array = np.array(x_Image_array)\n",
        "#x_Image_array.shape\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Test section\\n\\n#from PIL import Image\\n#from skimage.transform import resize\\n#from keras.preprocessing import image\\n\\nimport cv2\\nimport glob\\nfrom PIL import Image\\nfrom skimage.transform import resize\\nfrom keras.preprocessing import image\\nfrom keras.preprocessing.image import save_img\\n\\nX_image_trainarray = []\\ncount = 0\\n\\nfor f in X_image_train:\\n    print(count, len(X_image_train), f)\\n    m = cv2.imread(\"images/reduced\"+f)\\n    m = image.img_to_array(m)\\n    X_image_trainarray.append(m)\\n    count += 1\\n\\n\\n#x_Image_array = np.array(x_Image_array)\\n#x_Image_array.shape\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arBaELy6gBNo",
        "colab_type": "code",
        "outputId": "fdcfd470-7d07-448c-b567-9d3e60406882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "X_image_devarray = []\n",
        "count = 0\n",
        "\n",
        "for f in X_image_dev:\n",
        "    print(count, len(X_image_dev), f)\n",
        "    m = cv2.imread(\"images/reduced\"+f)\n",
        "    m = image.img_to_array(m)\n",
        "    X_image_devarray.append(m)\n",
        "    count += 1\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_image_devarray = []\\ncount = 0\\n\\nfor f in X_image_dev:\\n    print(count, len(X_image_dev), f)\\n    m = cv2.imread(\"images/reduced\"+f)\\n    m = image.img_to_array(m)\\n    X_image_devarray.append(m)\\n    count += 1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NskhVBNMgA8j",
        "colab_type": "code",
        "outputId": "01f79668-3a76-4a66-d0ed-4e8368267449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "X_image_testarray = []\n",
        "count = 0\n",
        "\n",
        "for f in X_image_test:\n",
        "    print(count, len(X_image_test), f)\n",
        "    m = cv2.imread(\"images/reduced\"+f)\n",
        "    m = image.img_to_array(m)\n",
        "    X_image_testarray.append(m)\n",
        "    count += 1\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_image_testarray = []\\ncount = 0\\n\\nfor f in X_image_test:\\n    print(count, len(X_image_test), f)\\n    m = cv2.imread(\"images/reduced\"+f)\\n    m = image.img_to_array(m)\\n    X_image_testarray.append(m)\\n    count += 1\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwmZbSbyz09e",
        "colab_type": "code",
        "outputId": "947c1b0b-73fa-429e-f964-0949aca2f3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "# convert to array\n",
        "X_image_trainarray = np.asarray(X_image_trainarray)\n",
        "X_image_devarray = np.asarray(X_image_devarray)\n",
        "X_image_testarray = np.asarray(X_image_testarray)\n",
        "\n",
        "# save as .npy files\n",
        "np.save('X_image_trainarray_299.npy', X_image_trainarray)\n",
        "np.save('X_image_devarray_299.npy', X_image_devarray)\n",
        "np.save('X_image_testarray_299.npy', X_image_testarray)\n",
        "\n",
        "np.save('X_numeric_test.npy', X_numeric_test)\n",
        "np.save('X_numeric_train.npy', X_numeric_train)\n",
        "np.save('X_numeric_dev.npy', X_numeric_dev)\n",
        "\n",
        "np.save('Y_train_loghammerprice.npy', Y_train_loghammerprice)\n",
        "np.save('Y_dev_loghammerprice.npy', Y_dev_loghammerprice)\n",
        "np.save('Y_test_loghammerprice.npy', Y_test_loghammerprice)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# convert to array\\nX_image_trainarray = np.asarray(X_image_trainarray)\\nX_image_devarray = np.asarray(X_image_devarray)\\nX_image_testarray = np.asarray(X_image_testarray)\\n\\n# save as .npy files\\nnp.save('X_image_trainarray_299.npy', X_image_trainarray)\\nnp.save('X_image_devarray_299.npy', X_image_devarray)\\nnp.save('X_image_testarray_299.npy', X_image_testarray)\\n\\nnp.save('X_numeric_test.npy', X_numeric_test)\\nnp.save('X_numeric_train.npy', X_numeric_train)\\nnp.save('X_numeric_dev.npy', X_numeric_dev)\\n\\nnp.save('Y_train_loghammerprice.npy', Y_train_loghammerprice)\\nnp.save('Y_dev_loghammerprice.npy', Y_dev_loghammerprice)\\nnp.save('Y_test_loghammerprice.npy', Y_test_loghammerprice)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A3QZoudRUaH",
        "colab_type": "code",
        "outputId": "3ae09805-cad1-4388-9679-b7d2f1570252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "print(\"X_image_trainarray_299\", X_image_trainarray.shape)\n",
        "print(\"X_image_devarray_299\", X_image_devarray.shape)\n",
        "print(\"X_image_testarray_299\", X_image_testarray.shape)\n",
        "print(\"X_numeric_test\", X_numeric_test.shape)\n",
        "print(\"X_numeric_train\", X_numeric_train.shape)\n",
        "print(\"X_numeric_dev\", X_numeric_dev.shape)\n",
        "print(\"Y_train_loghammerprice\", Y_train_loghammerprice.shape)\n",
        "print(\"Y_dev_loghammerprice\", Y_dev_loghammerprice.shape)\n",
        "print(\"Y_test_loghammerprice\", Y_test_loghammerprice.shape)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"X_image_trainarray_299\", X_image_trainarray.shape)\\nprint(\"X_image_devarray_299\", X_image_devarray.shape)\\nprint(\"X_image_testarray_299\", X_image_testarray.shape)\\nprint(\"X_numeric_test\", X_numeric_test.shape)\\nprint(\"X_numeric_train\", X_numeric_train.shape)\\nprint(\"X_numeric_dev\", X_numeric_dev.shape)\\nprint(\"Y_train_loghammerprice\", Y_train_loghammerprice.shape)\\nprint(\"Y_dev_loghammerprice\", Y_dev_loghammerprice.shape)\\nprint(\"Y_test_loghammerprice\", Y_test_loghammerprice.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-HphSPUeapP",
        "colab_type": "code",
        "outputId": "43d9d89c-243c-4d29-e3a1-ff89a2443723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# one-hot encode the log hammer price <==== NEED TO BE FIXED!!!!\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "encoder.fit(Y_train)\n",
        "encoded_Y_train = encoder.transform(Y_train)\n",
        "dummy_Y_train = np_utils.to_categorical(encoded_Y_train) \n",
        "\n",
        "encoder.fit(Y_dev)\n",
        "encoded_Y_dev = encoder.transform(Y_dev)\n",
        "dummy_Y_dev = np_utils.to_categorical(encoded_Y_dev) \n",
        "\n",
        "encoder.fit(Y_test)\n",
        "encoded_Y_test = encoder.transform(Y_test)\n",
        "dummy_Y_test = np_utils.to_categorical(encoded_Y_test) \n",
        "\n",
        "print(dummy_Y_train)\n",
        "print(dummy_Y_train.shape)\n",
        "print(dummy_Y_dev.shape)\n",
        "print(dummy_Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(7105, 21)\n",
            "(725, 19)\n",
            "(728, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL4_Tpd6Ozkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset from files\n",
        "\n",
        "X_image_trainarray = np.load('X_image_trainarray_299.npy')\n",
        "X_image_devarray = np.load('X_image_devarray_299.npy')\n",
        "X_image_testarray = np.load('X_image_testarray_299.npy')\n",
        "X_numeric_train = np.load('X_numeric_train.npy')\n",
        "X_numeric_dev = np.load('X_numeric_dev.npy')\n",
        "X_numeric_test = np.load('X_numeric_test.npy')\n",
        "Y_train_loghammerprice = np.load('Y_train_loghammerprice.npy')\n",
        "Y_dev_loghammerprice = np.load('Y_dev_loghammerprice.npy')\n",
        "Y_test_loghammerprice = np.load('Y_test_loghammerprice.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE9LZV5EeL3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset from files\n",
        "\n",
        "X_image_trainarray = np.load('X_image_trainarray_299.npy')\n",
        "X_image_devarray = np.load('X_image_devarray_299.npy')\n",
        "X_image_testarray = np.load('X_image_testarray_299.npy')\n",
        "\n",
        "Y_train_loghammerprice = np.load('Y_train_loghammerprice.npy')\n",
        "Y_dev_loghammerprice = np.load('Y_dev_loghammerprice.npy')\n",
        "Y_test_loghammerprice = np.load('Y_test_loghammerprice.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8W6yk_4zcse",
        "colab_type": "code",
        "outputId": "9d47ac8d-0417-462e-9922-5d5a781ea819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_image_trainarray[30,:,:,:].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(299, 299, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "112rb5g7zgwb",
        "colab_type": "code",
        "outputId": "48d80368-0ecb-4590-d71b-4ef4d78f0451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(X_image_trainarray[30,:,:,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWeZ1jeWzk7c",
        "colab_type": "code",
        "outputId": "9dccd0f4-1673-4583-87da-99be9cd71d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "img1 = X_image_trainarray[24,:,:,:]\n",
        "img = image.array_to_img(img1)\n",
        "img\n",
        "\n",
        "#from keras.preprocessing import image\n",
        "#from keras.preprocessing import array_to_img\n",
        "#from PIL import Image\n",
        "\n",
        "#plt.imshow(img)\n",
        "#Y_test_loghammerprice[2]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=299x299 at 0x7FDC3A312550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBcvJ28ARLYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Subsample the total sample space to accelerate Image CNN TensorBoard search. ##\n",
        "\n",
        "subsample_factor=4\n",
        "X_train_sub = np.floor(len(X_image_trainarray)/subsample_factor)\n",
        "X_dev_sub = np.floor(len(X_image_devarray)/subsample_factor)\n",
        "X_test_sub = np.floor(len(X_image_testarray)/subsample_factor)\n",
        "\n",
        "# This is actual number of entries after sub-sampling \n",
        "X_train_sub = X_train_sub.astype(int)\n",
        "X_dev_sub = X_dev_sub.astype(int)\n",
        "X_test_sub = X_test_sub.astype(int)\n",
        "\n",
        "# This gets the indices to make sure the same entries are subsampled between X and Y \n",
        "X_train_sub_idx = np.random.choice(np.arange(len(X_image_trainarray)), X_train_sub, replace=False)\n",
        "X_dev_sub_idx = np.random.choice(np.arange(len(X_image_devarray)), X_dev_sub, replace=False)\n",
        "X_test_sub_idx = np.random.choice(np.arange(len(X_image_testarray)), X_test_sub, replace=False)\n",
        "\n",
        "# Apply the indices \n",
        "X_image_trainarray_sub = X_image_trainarray[X_train_sub_idx,:,:,:]\n",
        "X_image_devarray_sub = X_image_devarray[X_dev_sub_idx,:,:,:]\n",
        "X_image_testarray_sub = X_image_testarray[X_test_sub_idx,:,:,:]\n",
        "Y_train_loghammerprice_sub = Y_train_loghammerprice[X_train_sub_idx]\n",
        "Y_dev_loghammerprice_sub = Y_dev_loghammerprice[X_dev_sub_idx]\n",
        "Y_test_loghammerprice_sub = Y_test_loghammerprice[X_test_sub_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJcD4R5sUX6-",
        "colab_type": "code",
        "outputId": "8c9b70a6-73cc-4b90-8854-a787c02cd8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "img1 = X_image_testarray_sub[24,:,:,:]\n",
        "img = image.array_to_img(img1, scale=False)\n",
        "img\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=299x299 at 0x7FDBA47426A0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ciDDKeELfzE",
        "colab_type": "code",
        "outputId": "6ced1677-06da-4033-86bf-df205a9d7b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(\"X_image_trainarray\", X_image_trainarray.shape)\n",
        "print(\"X_image_devarray\", X_image_devarray.shape)\n",
        "print(\"X_image_testarray\", X_image_testarray.shape)\n",
        "\n",
        "print(\"X_image_trainarray_sub\", X_image_trainarray_sub.shape)\n",
        "print(\"X_image_devarray_sub\", X_image_devarray_sub.shape)\n",
        "print(\"X_image_testarray_sub\", X_image_testarray_sub.shape)\n",
        "\n",
        "print(\"X_numeric_train\", X_numeric_train.shape)\n",
        "print(\"X_numeric_test\", X_numeric_test.shape)\n",
        "print(\"X_numeric_dev\", X_numeric_dev.shape)\n",
        "\n",
        "print(\"Y_train_loghammerprice\", Y_train_loghammerprice.shape)\n",
        "print(\"Y_dev_loghammerprice\", Y_dev_loghammerprice.shape)\n",
        "print(\"Y_test_loghammerprice\", Y_test_loghammerprice.shape)\n",
        "\n",
        "print(\"Y_train_loghammerprice_sub\", Y_train_loghammerprice_sub.shape)\n",
        "print(\"Y_dev_loghammerprice_sub\", Y_dev_loghammerprice_sub.shape)\n",
        "print(\"Y_test_loghammerprice_sub\", Y_test_loghammerprice_sub.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_image_trainarray (7105, 299, 299, 3)\n",
            "X_image_devarray (725, 299, 299, 3)\n",
            "X_image_testarray (728, 299, 299, 3)\n",
            "X_image_trainarray_sub (1776, 299, 299, 3)\n",
            "X_image_devarray_sub (181, 299, 299, 3)\n",
            "X_image_testarray_sub (182, 299, 299, 3)\n",
            "X_numeric_train (7105, 104)\n",
            "X_numeric_test (728, 104)\n",
            "X_numeric_dev (725, 104)\n",
            "Y_train_loghammerprice (7105,)\n",
            "Y_dev_loghammerprice (725,)\n",
            "Y_test_loghammerprice (728,)\n",
            "Y_train_loghammerprice_sub (1776,)\n",
            "Y_dev_loghammerprice_sub (181,)\n",
            "Y_test_loghammerprice_sub (182,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qr5Wm7t2upa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#standardize image arrays\n",
        "X_image_trainarray_sub /=255\n",
        "X_image_devarray_sub /=255\n",
        "X_image_testarray_sub /=255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq6nEGSX0iIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_log2_hammerprice_sub = np.log(np.power(10, Y_train_loghammerprice_sub))\n",
        "Y_dev_log2_hammerprice_sub = np.log(np.power(10, Y_dev_loghammerprice_sub))\n",
        "Y_test_log2_hammerprice_sub = np.log(np.power(10, Y_test_loghammerprice_sub))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bsfierwIW8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_loghammerprice_sub_rounded = np.round(Y_train_loghammerprice_sub*2)\n",
        "Y_dev_loghammerprice_sub_rounded = np.round(Y_dev_loghammerprice_sub*2)\n",
        "Y_test_loghammerprice_sub_rounded = np.round(Y_test_loghammerprice_sub*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcigoM9GKmxp",
        "colab_type": "code",
        "outputId": "35cfddb9-10ee-48f4-a6bc-6cc7c18fe3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# one-hot encode the log hammer price\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y_train_loghammerprice_sub_rounded)\n",
        "encoded_Y_train_loghammerprice_sub_rounded = encoder.transform(Y_train_loghammerprice_sub_rounded)\n",
        "dummy_Y_train_loghammerprice_sub_rounded = np_utils.to_categorical(encoded_Y_train_loghammerprice_sub_rounded) \n",
        "print(dummy_Y_train_loghammerprice_sub_rounded)\n",
        "print(dummy_Y_train_loghammerprice_sub_rounded.shape)\n",
        "print(np.min(Y_train_loghammerprice_sub_rounded))\n",
        "print(np.max(Y_train_loghammerprice_sub_rounded))\n",
        "dummy_Y_train_loghammerprice_sub_rounded.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "(1776, 11)\n",
            "5.0\n",
            "15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-7reBBK4Ko",
        "colab_type": "code",
        "outputId": "be96872f-0567-4088-8982-01c90b96a303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# one-hot encode the log hammer price\n",
        "#encoder = LabelEncoder()\n",
        "#encoder.fit(Y_train_loghammerprice_sub_rounded)\n",
        "encoded_Y_dev_loghammerprice_sub_rounded = encoder.transform(Y_dev_loghammerprice_sub_rounded)\n",
        "dummy_Y_dev_loghammerprice_sub_rounded = np_utils.to_categorical(encoded_Y_dev_loghammerprice_sub_rounded) \n",
        "print(dummy_Y_dev_loghammerprice_sub_rounded)\n",
        "print(dummy_Y_dev_loghammerprice_sub_rounded.shape)\n",
        "print(np.min(Y_dev_loghammerprice_sub_rounded))\n",
        "print(np.max(Y_dev_loghammerprice_sub_rounded))\n",
        "dummy_Y_dev_loghammerprice_sub_rounded.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]]\n",
            "(181, 11)\n",
            "5.0\n",
            "15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sILrHsF2LCZV",
        "colab_type": "code",
        "outputId": "6800f3ae-c400-4407-eb0b-9020f24b096f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# one-hot encode the log hammer price\n",
        "#encoder = LabelEncoder()\n",
        "#encoder.fit(Y_train_loghammerprice_sub_rounded)\n",
        "encoded_Y_test_loghammerprice_sub_rounded = encoder.transform(Y_test_loghammerprice_sub_rounded)\n",
        "dummy_Y_test_loghammerprice_sub_rounded = np_utils.to_categorical(encoded_Y_test_loghammerprice_sub_rounded) \n",
        "print(dummy_Y_test_loghammerprice_sub_rounded)\n",
        "print(dummy_Y_test_loghammerprice_sub_rounded.shape)\n",
        "print(np.min(Y_test_loghammerprice_sub_rounded))\n",
        "print(np.max(Y_test_loghammerprice_sub_rounded))\n",
        "dummy_Y_test_loghammerprice_sub_rounded.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]]\n",
            "(182, 11)\n",
            "6.0\n",
            "15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH2LdeJvLM_r",
        "colab_type": "code",
        "outputId": "22771d28-b653-43a1-91fa-15d82480d623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(dummy_Y_train_loghammerprice_sub_rounded.shape, dummy_Y_dev_loghammerprice_sub_rounded.shape, dummy_Y_test_loghammerprice_sub_rounded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1776, 11) (181, 11) (182, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-8hz0aV6zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGBjdZXn0XMm",
        "colab_type": "code",
        "outputId": "5a0ca0cb-e553-4766-dbcf-7674ee272f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "img1 = X_image_testarray_sub[24,:,:,:]\n",
        "img = image.array_to_img(img1, scale=False)\n",
        "img\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAErCAIAAAAJxjLjAAABYklEQVR4nO3dwQqAIAwAUAf9/y/byYhKOjbbeyedImMI3lxrABXE1wkAQAqLvoiLpp3X9nUCZfUxiDGNSbzN7/2x2k/Bx82XDa/HxuTwgu7lLV4QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD+Rp9AAAAAAAAAAAAAAAAAAAAAAAAAIB0fpQEAAAAAAAAAAAAAAAAAAAAAAACsy59yAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACkp+kWAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABSxAyGGDQuwNaS2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=299x299 at 0x7FDBA4742FD0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3u2RGnQN_8",
        "colab_type": "code",
        "outputId": "11a186db-438b-4a68-bfd3-c8b88a9d279f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "# Split X_numeric into numerical and categorical data. \n",
        "X_numeric_train_1 = X_numeric_train[:,:11]\n",
        "X_numeric_train_2 = X_numeric_train[:,11:]\n",
        "\n",
        "X_numeric_dev_1 = X_numeric_dev[:,:11]\n",
        "X_numeric_dev_2 = X_numeric_dev[:,11:]\n",
        "\n",
        "X_numeric_test_1 = X_numeric_test[:,:11]\n",
        "X_numeric_test_2 = X_numeric_test[:,11:]\n",
        "\n",
        "#standardize numerical data using mean and standard deviation of training samples\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_numeric_train_1)\n",
        "X_numeric_train_1_scaled = scaler.transform(X_numeric_train_1)\n",
        "X_numeric_dev_1_scaled = scaler.transform(X_numeric_dev_1)\n",
        "X_numeric_test_1_scaled = scaler.transform(X_numeric_test_1)\n",
        "\n",
        "# construct our train/dev/test set by concatenating the categorical features with the continuous features\n",
        "X_numeric_train_scaled = np.hstack([X_numeric_train_1_scaled, X_numeric_train_2])\n",
        "X_numeric_dev_scaled = np.hstack([X_numeric_dev_1_scaled, X_numeric_dev_2])\n",
        "X_numeric_test_scaled = np.hstack([X_numeric_test_1_scaled, X_numeric_test_2])\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Split X_numeric into numerical and categorical data. \\nX_numeric_train_1 = X_numeric_train[:,:11]\\nX_numeric_train_2 = X_numeric_train[:,11:]\\n\\nX_numeric_dev_1 = X_numeric_dev[:,:11]\\nX_numeric_dev_2 = X_numeric_dev[:,11:]\\n\\nX_numeric_test_1 = X_numeric_test[:,:11]\\nX_numeric_test_2 = X_numeric_test[:,11:]\\n\\n#standardize numerical data using mean and standard deviation of training samples\\nscaler = StandardScaler()\\n\\nscaler.fit(X_numeric_train_1)\\nX_numeric_train_1_scaled = scaler.transform(X_numeric_train_1)\\nX_numeric_dev_1_scaled = scaler.transform(X_numeric_dev_1)\\nX_numeric_test_1_scaled = scaler.transform(X_numeric_test_1)\\n\\n# construct our train/dev/test set by concatenating the categorical features with the continuous features\\nX_numeric_train_scaled = np.hstack([X_numeric_train_1_scaled, X_numeric_train_2])\\nX_numeric_dev_scaled = np.hstack([X_numeric_dev_1_scaled, X_numeric_dev_2])\\nX_numeric_test_scaled = np.hstack([X_numeric_test_1_scaled, X_numeric_test_2])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS64K4pFL7lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KMzdqh5L7rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VZ1uKxhL7xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "\n",
        "#from keras.callbacks import TensorBoard\n",
        "#from tensorboard.plugins.hparams import api as hp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0hWQoCGL712",
        "colab_type": "code",
        "outputId": "1b2c179b-3d12-414c-ae3a-ceb5b5dbad90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32]))\n",
        "HP_BATCHN = hp.HParam('batch_number', hp.Discrete([32]))\n",
        "HP_LL = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2]))\n",
        "HP_KERNEL = hp.HParam('kernel_size', hp.Discrete([5]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "#with tf.summary.FileWriter('logs/hparam_tuning'):\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_KERNEL, HP_OPTIMIZER, HP_LL, HP_BATCHN],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nHP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32]))\\nHP_BATCHN = hp.HParam('batch_number', hp.Discrete([32]))\\nHP_LL = hp.HParam('learning_rate', hp.Discrete([0.001]))\\nHP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2]))\\nHP_KERNEL = hp.HParam('kernel_size', hp.Discrete([5]))\\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\\n\\n\\nMETRIC_ACCURACY = 'accuracy'\\n\\n#with tf.summary.FileWriter('logs/hparam_tuning'):\\nwith tf.summary.create_file_writer('logs/hparam_tuning').as_default():\\n  hp.hparams_config(\\n    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_KERNEL, HP_OPTIMIZER, HP_LL, HP_BATCHN],\\n    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\\n  )\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvVGcMgZ1LFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "log_dir=\"logs/fit/\"+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "### When training Keras models, you can use callbacks instead of writing these directly:\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir.format(time()))\n",
        "#hp_callback = hp.KerasCallback(log_dir, hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHWssMeQ0n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# root mean squared error (rmse) for regression\n",
        "def rmse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "# mean squared error (mse) for regression\n",
        "def mse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# coefficient of determination (R^2) for regression\n",
        "def r_square(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
        "\n",
        "def r_square_loss(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return 1 - ( 1 - SS_res/(SS_tot + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtDEZdZ71Q5s",
        "colab_type": "code",
        "outputId": "a34881d9-898b-45b1-d531-a11f92853054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "np.power(10,Y_test_loghammerprice_sub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.970670e+05, 2.172500e+04, 1.165630e+05, 2.000000e+04,\n",
              "       2.800000e+04, 2.714200e+04, 3.533960e+05, 4.240700e+04,\n",
              "       8.200000e+05, 1.788600e+04, 3.068100e+04, 6.914000e+03,\n",
              "       2.000000e+05, 6.140000e+03, 8.354720e+05, 1.600000e+05,\n",
              "       1.600000e+05, 8.960620e+05, 3.300000e+05, 4.190004e+06,\n",
              "       3.178420e+05, 1.354600e+04, 2.500000e+03, 8.156130e+05,\n",
              "       8.947250e+05, 4.427000e+03, 1.800000e+05, 5.238650e+05,\n",
              "       6.548300e+04, 3.057300e+04, 1.699420e+05, 1.900000e+05,\n",
              "       2.166150e+05, 3.800000e+05, 1.402060e+06, 6.500000e+04,\n",
              "       1.332800e+05, 1.624670e+05, 2.000000e+05, 5.828000e+03,\n",
              "       1.058845e+06, 2.633500e+04, 2.901190e+05, 3.621560e+05,\n",
              "       3.100000e+05, 8.500000e+03, 1.200000e+06, 3.547210e+05,\n",
              "       3.143191e+06, 4.452850e+05, 3.415830e+05, 2.124120e+05,\n",
              "       2.600000e+06, 8.066000e+03, 4.308800e+04, 2.348000e+03,\n",
              "       5.000000e+05, 5.600000e+05, 1.000000e+05, 1.326259e+06,\n",
              "       1.182823e+06, 1.009600e+04, 4.976710e+05, 7.500000e+04,\n",
              "       5.616400e+04, 3.500000e+05, 1.077000e+03, 9.196400e+04,\n",
              "       2.637750e+05, 8.643770e+05, 2.802530e+05, 3.170390e+05,\n",
              "       1.549200e+04, 7.800000e+05, 9.559400e+04, 7.285620e+05,\n",
              "       4.435300e+04, 1.437900e+04, 9.264800e+04, 2.124120e+05,\n",
              "       1.741100e+05, 1.657560e+05, 3.577252e+06, 8.574000e+03,\n",
              "       9.500000e+04, 6.720400e+04, 1.019891e+06, 4.074200e+04,\n",
              "       8.000000e+04, 6.224420e+05, 5.068360e+05])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_NzTZFDL75g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "\n",
        "#def train_test_model(hparams):\n",
        "input_shape = (299,299,3)\n",
        "pool_size = 3\n",
        "\n",
        "#  final_node_size = dummy_load_bin.shape[1]\n",
        "\n",
        "Image_model = tf.keras.models.Sequential([\n",
        "    #tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32*2, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32*2, (5, 5), activation=tf.keras.layers.LeakyReLU(alpha=0.3), padding='same', kernel_initializer='he_normal', input_shape=input_shape),\n",
        "    tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'),\n",
        "    tf.keras.layers.MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(11, activation='softmax'),\n",
        "  ])\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJCr3pLd2-BJ",
        "colab_type": "code",
        "outputId": "a8e830a8-fa7e-4e1b-aa60-382e5be3ef15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Image_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_70 (Conv2D)           (None, 299, 299, 32)      2432      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 299, 299, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_70 (MaxPooling (None, 150, 150, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 150, 150, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 150, 150, 32)      25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 150, 150, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 75, 75, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 75, 75, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 38, 38, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 38, 38, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 19, 19, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 19, 19, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 10, 10, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_75 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 5, 5, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 5, 5, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_76 (MaxPooling (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 3, 3, 32)          25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 3, 3, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_77 (MaxPooling (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 2, 2, 32)          25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 2, 2, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_78 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 1, 1, 64)          51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 1, 1, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_79 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 312,235\n",
            "Trainable params: 311,467\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_oEWVIu2XeH",
        "colab_type": "code",
        "outputId": "e376f9ef-972e-46c6-cdcc-9564c9f59765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "sgd = optimizers.SGD(lr=0.04, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "Image_model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "  \n",
        "  #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "print(Image_model.summary())\n",
        "\n",
        "history = Image_model.fit(\n",
        "      X_image_trainarray_sub, \n",
        "      dummy_Y_train_loghammerprice_sub_rounded, \n",
        "      batch_size=32, \n",
        "      epochs=50, \n",
        "      callbacks=[tensorboard_callback], \n",
        "      validation_data = (X_image_devarray_sub, dummy_Y_dev_loghammerprice_sub_rounded)) \n",
        "  \n",
        "_, result = Image_model.evaluate(X_image_testarray_sub, dummy_Y_test_loghammerprice_sub_rounded) \n",
        "  \n",
        "  #return result, history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_70 (Conv2D)           (None, 299, 299, 32)      2432      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 299, 299, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_70 (MaxPooling (None, 150, 150, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 150, 150, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_71 (Conv2D)           (None, 150, 150, 32)      25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 150, 150, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_72 (Conv2D)           (None, 75, 75, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 75, 75, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_73 (Conv2D)           (None, 38, 38, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 38, 38, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_74 (Conv2D)           (None, 19, 19, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 19, 19, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 10, 10, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_75 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 5, 5, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 5, 5, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_76 (MaxPooling (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 3, 3, 32)          25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 3, 3, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_77 (MaxPooling (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 2, 2, 32)          25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 2, 2, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_78 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 1, 1, 64)          51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 1, 1, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_79 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 11)                715       \n",
            "=================================================================\n",
            "Total params: 312,235\n",
            "Trainable params: 311,467\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "56/56 [==============================] - 6s 103ms/step - loss: 2.7755 - val_loss: 2.2838\n",
            "Epoch 2/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 2.5067 - val_loss: 2.1989\n",
            "Epoch 3/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.3746 - val_loss: 2.1479\n",
            "Epoch 4/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.2861 - val_loss: 2.1246\n",
            "Epoch 5/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.2232 - val_loss: 2.0996\n",
            "Epoch 6/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.1949 - val_loss: 2.0362\n",
            "Epoch 7/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 2.1614 - val_loss: 2.0031\n",
            "Epoch 8/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 2.1327 - val_loss: 1.9794\n",
            "Epoch 9/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 2.1019 - val_loss: 1.9596\n",
            "Epoch 10/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.0969 - val_loss: 1.9522\n",
            "Epoch 11/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 2.0650 - val_loss: 1.9426\n",
            "Epoch 12/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.0593 - val_loss: 1.9464\n",
            "Epoch 13/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0515 - val_loss: 1.9335\n",
            "Epoch 14/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0559 - val_loss: 1.9278\n",
            "Epoch 15/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0335 - val_loss: 1.9306\n",
            "Epoch 16/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0394 - val_loss: 1.9305\n",
            "Epoch 17/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0292 - val_loss: 1.9189\n",
            "Epoch 18/50\n",
            "56/56 [==============================] - 5s 85ms/step - loss: 2.0182 - val_loss: 1.9358\n",
            "Epoch 19/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 2.0235 - val_loss: 1.9159\n",
            "Epoch 20/50\n",
            "56/56 [==============================] - 5s 95ms/step - loss: 1.9943 - val_loss: 1.9145\n",
            "Epoch 21/50\n",
            "56/56 [==============================] - 5s 90ms/step - loss: 1.9978 - val_loss: 1.9109\n",
            "Epoch 22/50\n",
            "56/56 [==============================] - 5s 92ms/step - loss: 1.9994 - val_loss: 1.9181\n",
            "Epoch 23/50\n",
            "56/56 [==============================] - 5s 92ms/step - loss: 1.9862 - val_loss: 1.9184\n",
            "Epoch 24/50\n",
            "56/56 [==============================] - 5s 92ms/step - loss: 1.9931 - val_loss: 1.9245\n",
            "Epoch 25/50\n",
            "56/56 [==============================] - 5s 91ms/step - loss: 1.9669 - val_loss: 1.9252\n",
            "Epoch 26/50\n",
            "56/56 [==============================] - 5s 90ms/step - loss: 1.9865 - val_loss: 1.9144\n",
            "Epoch 27/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9876 - val_loss: 1.9040\n",
            "Epoch 28/50\n",
            "56/56 [==============================] - 5s 90ms/step - loss: 1.9852 - val_loss: 1.8972\n",
            "Epoch 29/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9610 - val_loss: 1.8993\n",
            "Epoch 30/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9736 - val_loss: 1.9166\n",
            "Epoch 31/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9592 - val_loss: 1.9077\n",
            "Epoch 32/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9564 - val_loss: 1.9142\n",
            "Epoch 33/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9594 - val_loss: 1.9008\n",
            "Epoch 34/50\n",
            "56/56 [==============================] - 5s 90ms/step - loss: 1.9773 - val_loss: 1.9025\n",
            "Epoch 35/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9592 - val_loss: 1.9059\n",
            "Epoch 36/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9395 - val_loss: 1.9207\n",
            "Epoch 37/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9614 - val_loss: 1.9299\n",
            "Epoch 38/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9489 - val_loss: 1.9137\n",
            "Epoch 39/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9452 - val_loss: 1.9445\n",
            "Epoch 40/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9401 - val_loss: 1.9070\n",
            "Epoch 41/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9368 - val_loss: 1.9134\n",
            "Epoch 42/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9346 - val_loss: 1.9088\n",
            "Epoch 43/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9404 - val_loss: 1.9115\n",
            "Epoch 44/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9103 - val_loss: 1.8982\n",
            "Epoch 45/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9200 - val_loss: 1.8962\n",
            "Epoch 46/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9430 - val_loss: 1.9401\n",
            "Epoch 47/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9342 - val_loss: 1.8838\n",
            "Epoch 48/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9147 - val_loss: 1.8865\n",
            "Epoch 49/50\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 1.9162 - val_loss: 1.9126\n",
            "Epoch 50/50\n",
            "56/56 [==============================] - 5s 89ms/step - loss: 1.9373 - val_loss: 1.9202\n",
            "6/6 [==============================] - 0s 44ms/step - loss: 1.8576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-302-195f3302e1d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       validation_data = (X_image_devarray_sub, dummy_Y_dev_loghammerprice_sub_rounded)) \n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_image_testarray_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_Y_test_loghammerprice_sub_rounded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#return result, history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYAKyjeAL7vL",
        "colab_type": "code",
        "outputId": "58cda98e-f939-49db-d5bb-e7e170bb6b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "  \"\"\"\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy, history = train_test_model(hparams)\n",
        "    print(\"accuracy\", accuracy)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
        "\n",
        "\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef run(run_dir, hparams):\\n  with tf.summary.create_file_writer(run_dir).as_default():\\n    hp.hparams(hparams)  # record the values used in this trial\\n    accuracy, history = train_test_model(hparams)\\n    print(\"accuracy\", accuracy)\\n    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\\n\\n\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYNoPXeeL7pO",
        "colab_type": "code",
        "outputId": "f5c5851e-7620-4a87-ec9a-5e27bc7f83c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    for kernel_size in HP_KERNEL.domain.values:\n",
        "      for optimizer in HP_OPTIMIZER.domain.values:\n",
        "        for learning_rate in HP_LL.domain.values:\n",
        "          for batch_number in HP_BATCHN.domain.values:\n",
        "            hparams = {\n",
        "                HP_NUM_UNITS: num_units,\n",
        "                HP_DROPOUT: dropout_rate,\n",
        "                HP_KERNEL: kernel_size,\n",
        "                HP_OPTIMIZER: optimizer,\n",
        "                HP_LL: learning_rate,\n",
        "                HP_BATCHN: batch_number,\n",
        "            }\n",
        "            run_name = \"run-%d\" % session_num\n",
        "            print('--- Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            run('logs/hparam_tuning/' + run_name, hparams)\n",
        "            session_num += 1\n",
        "\n",
        "            \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsession_num = 0\\n\\nfor num_units in HP_NUM_UNITS.domain.values:\\n  for dropout_rate in HP_DROPOUT.domain.values:\\n    for kernel_size in HP_KERNEL.domain.values:\\n      for optimizer in HP_OPTIMIZER.domain.values:\\n        for learning_rate in HP_LL.domain.values:\\n          for batch_number in HP_BATCHN.domain.values:\\n            hparams = {\\n                HP_NUM_UNITS: num_units,\\n                HP_DROPOUT: dropout_rate,\\n                HP_KERNEL: kernel_size,\\n                HP_OPTIMIZER: optimizer,\\n                HP_LL: learning_rate,\\n                HP_BATCHN: batch_number,\\n            }\\n            run_name = \"run-%d\" % session_num\\n            print(\\'--- Starting trial: %s\\' % run_name)\\n            print({h.name: hparams[h] for h in hparams})\\n            run(\\'logs/hparam_tuning/\\' + run_name, hparams)\\n            session_num += 1\\n\\n            '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6IhxKGT0feS",
        "colab_type": "code",
        "outputId": "85ae039d-c677-4d26-8cda-c7cb85fc0660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "\n",
        "# plot training curve for R^2 (beware of scale, starts very low negative)\n",
        "#plt.plot(history.history['val_r_square'])\n",
        "#plt.plot(history.history['r_square'])\n",
        "#plt.title('model R^2')\n",
        "#plt.ylabel('R^2')\n",
        "#plt.xlabel('epoch')\n",
        "#plt.legend(['train', 'test'], loc='upper left')\n",
        "#plt.show()\n",
        "           \n",
        "# plot training curve for rmse\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaq3hmuGMOEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sMyIyXK7pYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs/hparam_tuning/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpa6DdCvTCeP",
        "colab_type": "code",
        "outputId": "c0c5e736-6fcc-4faf-8bca-b1beecd83012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "### Text NET ##########\n",
        "\"\"\"\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import regularizers\n",
        "\n",
        "def numerical_model(dim, regress=False):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, activation='relu', input_dim = dim, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(64, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(Dense(32, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "  \n",
        "\t# check to see if the regression node should be added\n",
        "  if regress:\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "\t# return our model\n",
        "  return model\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom keras.layers import BatchNormalization\\nfrom keras import regularizers\\n\\ndef numerical_model(dim, regress=False):\\n  model = Sequential()\\n  model.add(Dense(128, activation=\\'relu\\', input_dim = dim, kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(128, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(64, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = \\'zeros\\', gamma_initializer = \\'ones\\', moving_mean_initializer=\\'zeros\\', moving_variance_initializer=\\'ones\\'))\\n  model.add(Dense(32, activation=\\'relu\\', kernel_initializer=\\'normal\\', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\\n  \\n\\t# check to see if the regression node should be added\\n  if regress:\\n    model.add(Dense(1, activation=\"linear\"))\\n \\n\\t# return our model\\n  return model\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLsCWLgo92HE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMG_SHAPE = (224, 224, 3)\n",
        "#res_net = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SHAPE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPMBqsfc_4VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#res_net.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxMKIdl8AA-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "#output_layer = tf.keras.layers.Dense(1, activation=tf.nn.relu)\n",
        "#tl_model = tf.keras.Sequential([\n",
        "#  res_net,\n",
        "#  global_average_layer,\n",
        "#  output_layer\n",
        "#])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZbwSac6AZ6f",
        "colab_type": "code",
        "outputId": "57dfcdfd-254d-4f0e-a555-d78cfd419799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#tl_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 23,589,761\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbQV4m2lAnVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "tl_model.compile(\n",
        "      learning_rate = 0.002,\n",
        "      optimizer = 'adam',\n",
        "      loss=[mse], \n",
        "      metrics=[r_square],\n",
        "      )\n",
        "  \n",
        "  #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "print(tl_model.summary())\n",
        "\n",
        "tl_history = tl_model.fit(\n",
        "      X_image_trainarray_sub, \n",
        "      Y_train_log2_hammerprice_sub, \n",
        "      batch_size=32, \n",
        "      epochs=50, \n",
        "      callbacks=[tensorboard_callback], \n",
        "      validation_data = (X_image_devarray_sub, Y_dev_log2_hammerprice_sub)) \n",
        "  \n",
        "_, tl_result = tl_model.evaluate(X_image_testarray_sub, Y_test_log2_hammerprice_sub) \n",
        "  \n",
        "  #return result, history\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn_zGcoxVr9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "# use pretrained model - Resnet50\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "def cnn_model(regress=False):\n",
        "\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  # let's add a fully-connected layer\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(1024, activation = 'relu')(x)\n",
        "\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "      x = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # construct the CNN\n",
        "  model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "  # return the CNN\n",
        "  return model\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dehwNMe_24h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ1ueBY07w4E",
        "colab_type": "code",
        "outputId": "ba752489-f139-4b3a-ab3a-d33c1ede44c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "cnn = Image_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-5b2d92cfa075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'inputs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVltsi5VfDXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the numerical and CNN models\n",
        "numeric = numerical_model(104, regress=False)\n",
        "cnn = cnn_model(regress=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOzbYZfuiNRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import concatenate\n",
        "\n",
        "# create the input to our final set of layers as the *output* of both the numeric and CNN\n",
        "combinedInput = concatenate([numeric.output, cnn.output])\n",
        "\n",
        "print(combinedInput.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt_fGTufYiKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(1024, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001))(combinedInput)\n",
        "x = BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
        "x = Dense(512, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001))(x)\n",
        "x = BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
        "x = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
        "\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "model = Model(inputs=[numeric.input, cnn.input], outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRI478hHjcde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# root mean squared error (rmse) for regression\n",
        "def rmse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "# mean squared error (mse) for regression\n",
        "def mse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# coefficient of determination (R^2) for regression\n",
        "def r_square(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
        "\n",
        "def r_square_loss(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return 1 - ( 1 - SS_res/(SS_tot + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ji3ZByEai2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# compile the model using mean absolute percentage error as our loss,\n",
        "# implying that we seek to minimize the absolute percentage difference\n",
        "# between our price *predictions* and the *actual prices*\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt, metrics=[\"mean_absolute_percentage_error\", r_square])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-LBnJbIb65R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "result = model.fit(\n",
        "\t[X_numeric_train_scaled, X_image_trainarray], Y_train_loghammerprice,\n",
        "\tvalidation_data=([X_numeric_dev_scaled, X_image_devarray], Y_dev_loghammerprice),\n",
        "\tepochs=5, batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhUb2dROmeM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict([X_numeric_test_scaled, X_image_testarray])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4b8DqPAysnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = y_pred.flatten() - Y_test_loghammerprice\n",
        "percentDiff = (diff / Y_test_loghammerprice) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        " \n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        " \n",
        "# finally, show some statistics on our model\n",
        "print(\"[INFO] mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8cYLWp2mrlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot training curve for R^2 (beware of scale, starts very low negative)\n",
        "plt.plot(result.history['val_r_square'])\n",
        "plt.plot(result.history['r_square'])\n",
        "plt.title('model R^2')\n",
        "plt.ylabel('R^2')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "           \n",
        "# plot training curve for rmse\n",
        "plt.plot(result.history['rmse'])\n",
        "plt.plot(result.history['val_rmse'])\n",
        "plt.title('rmse')\n",
        "plt.ylabel('rmse')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# print the linear regression and display datapoints\n",
        "from sklearn.linear_model import LinearRegression  \n",
        "regressor = LinearRegression()  \n",
        "regressor.fit(Y_test_loghammerprice.reshape(-1,1), y_pred)  \n",
        "y_fit = regressor.predict(y_pred) \n",
        "\n",
        "reg_intercept = round(regressor.intercept_[0],4)\n",
        "reg_coef = round(regressor.coef_.flatten()[0],4)\n",
        "reg_label = \"y = \" + str(reg_intercept) + \"*x +\" + str(reg_coef)\n",
        "\n",
        "plt.scatter(Y_test_loghammerprice, y_pred, color='blue', label= 'data')\n",
        "plt.plot(y_pred, y_fit, color='red', linewidth=2, label = 'Linear regression\\n'+reg_label) \n",
        "plt.title('Linear Regression')\n",
        "plt.legend()\n",
        "plt.xlabel('observed')\n",
        "plt.ylabel('predicted')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuKojMKpb-F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model\n",
        "model.evaluate(x = [X_numeric_test_scaled, X_image_testarray], y = Y_test_loghammerprice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9QfVGgLbZ-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split training set and test set (actually, it is \"Validation set\")\n",
        "#x_Image_array_resized_train, x_Image_array_resized_test, load_bin_train, load_bin_test = train_test_split(x_Image_array, dummy_load_bin, test_size=0.25)\n",
        "\n",
        "#x_train = np.multiply(x_Image_array_resized_train, 1/255)\n",
        "#x_test = np.multiply(x_Image_array_resized_test, 1/255)\n",
        "#y_train = load_bin_train\n",
        "#y_test = load_bin_test\n",
        "\n",
        "#print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuyKLtUKedqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spHH2zajM6yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_C1AuQPdwN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorboard.plugins.hparams import api as hp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOhuSPFEM3PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2]))\n",
        "#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "HP_KERNEL = hp.HParam('kernel_size', hp.Discrete([5, 7]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.FileWriter('logs/hparam_tuning'):\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_KERNEL],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZtVUSp7v8ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_test_model(hparams):\n",
        "  input_shape = (224,224,3)\n",
        "  hparams[HP_KERNEL] = 7\n",
        "  hparams[HP_NUM_UNITS] = 32\n",
        "  pool_size = 3\n",
        "  final_node_size = dummy_load_bin.shape[1]\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same', input_shape=input_shape))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  \n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "\n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  #model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "    \n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  #model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "\n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "    \n",
        "  model.add(Conv2D(32, (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  #model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "\n",
        "  #model.add(Conv2D(hparams[HP_NUM_UNITS], (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  #model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  #model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "\n",
        "  #model.add(Conv2D(hparams[HP_NUM_UNITS], (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  #model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  #model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  #model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "\n",
        "  model.add(Conv2D(hparams[HP_NUM_UNITS], (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "\n",
        "  model.add(Conv2D(hparams[HP_NUM_UNITS], (hparams[HP_KERNEL], hparams[HP_KERNEL]), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "  model.add(MaxPooling2D((pool_size, pool_size), strides=(2, 2), padding='same'))\n",
        "  model.add(Dropout(hparams[HP_DROPOUT]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(final_node_size, activation='softmax'))\n",
        "    \n",
        "  model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train, batch_size=32, epochs=10) #, callbacks=[hp.KerasCallback('logs/hparam_tuning/', hparams)])\n",
        "  _, accuracy = model.evaluate(x_test, y_test) \n",
        "\n",
        "  return accuracy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BUksRHfRJAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.FileWriter(run_dir):\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy)#, step=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RdV3z0ahQFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "### When training Keras models, you can use callbacks instead of writing these directly:\n",
        "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir.format(time()))\n",
        "#hp_callback = hp.KerasCallback(log_dir, hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fxuo2EDR1Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    for kernel_size in HP_KERNEL.domain.values:\n",
        "      hparams = {\n",
        "          HP_NUM_UNITS: num_units,\n",
        "          HP_DROPOUT: dropout_rate,\n",
        "          HP_KERNEL: kernel_size,\n",
        "      }\n",
        "      run_name = \"run-%d\" % session_num\n",
        "      print('--- Starting trial: %s' % run_name)\n",
        "      print({h.name: hparams[h] for h in hparams})\n",
        "      run('logs/hparam_tuning/' + run_name, hparams)\n",
        "      session_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwBjXY0QRrME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#history = model.fit(x_train, y_train, batch_size=32, epochs=1, verbose=1, validation_data=(x_test, y_test), callbacks=[tensorboard_callback, hp_callback])\n",
        "#history = model.fit(x_train, y_train, batch_size=32, epochs=50, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH6UtJWQ1XYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This evaluation is not actually done right. Need to get the real evaluation dataset (2018 and beyond)\n",
        "#score = model.evaluate(x_test, y_test, verbose=1)\n",
        "#print('Test loss:', score[0])\n",
        "#print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0xOGgEniBT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs/fit\n",
        "%tensorboard --logdir logs/hparam_tuning/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpq8SlEEU3x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud7X-O1azZr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "## TEXT NET #####################\n",
        "#################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLToTfdH0UfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Jqjqr0KoOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split training set and validation set\n",
        "#load_text = load_text.values\n",
        "X_text_train, X_text_test, Y_text_train, Y_text_test = train_test_split(load_text, dummy_load_bin, test_size=0.1)\n",
        "print(X_text_train.shape, Y_text_train.shape,X_text_test.shape, Y_text_test.shape)\n",
        "X_text_test[:,10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp8C0vMQlbPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split columns to StandardScalar the table more effectively. \n",
        "\n",
        "X_text_train_1 = X_text_train[:,:11]\n",
        "X_text_train_2 = X_text_train[:,11:]\n",
        "\n",
        "print(\"X_text_train_1\", X_text_train_1.shape)\n",
        "print(\"X_text_train_2\", X_text_train_2.shape)\n",
        "\n",
        "X_text_test_1 = X_text_test[:, :11]\n",
        "X_text_test_2 = X_text_test[:,11:]\n",
        "\n",
        "print(\"X_text_test_1\", X_text_test_1.shape)\n",
        "print(\"X_text_test_2\", X_text_test_2.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsH8c0v6cqwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #standardize X_train and X_test using mean and standard deviation of training samples\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(X_text_train_1)\n",
        "X_text_train_1_scaled = scaler.transform(X_text_train_1)\n",
        "X_text_test_1_scaled = scaler.transform(X_text_test_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atanz44V2UiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# construct our training and testing data points by concatenating the categorical features with the continuous features\n",
        "X_text_train_scaled = np.hstack([X_text_train_1_scaled, X_text_train_2])\n",
        "X_text_test_scaled = np.hstack([X_text_test_1_scaled, X_text_test_2])\n",
        "#X_dev_scaled = np.hstack([X_dev_scaled, X_dev_2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix9EfM-b2kw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_node_size = dummy_load_bin.shape[1]\n",
        "\n",
        "#Create model\n",
        "model = Sequential()\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import regularizers\n",
        "\n",
        "#Add more layers\n",
        "model.add(Dense(128, activation='relu', input_shape = (104,), kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "model.add(BatchNormalization(axis=-1, momentum = 0.99, epsilon = 0.001, center = True, scale = True, beta_initializer = 'zeros', gamma_initializer = 'ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n",
        "model.add(Dense(32, activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(final_node_size, activation='softmax', kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.0001)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc7LBgvMYWK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# complie the model using adam optimizer and categorical crossentropy \n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEvZF0Is26k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4gmvXa23rJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOeH7iVj3yf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "#hist = model.fit(X_text_train_scaled, Y_text_train, batch_size=64, epochs=100, shuffle=True, callbacks=[tensorboard_callback])\n",
        "hist = model.fit(X_text_train_scaled, Y_text_train, batch_size=64, epochs=100,validation_data= (X_text_test_scaled, Y_text_test), shuffle=True, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNR_ChetYg_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(x = X_text_test_scaled, y = Y_text_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOjjuB4cY-8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc = 'upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#plot the accuracy\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc = 'lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2kX3ymqZFMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7co4iDhl4Tth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}